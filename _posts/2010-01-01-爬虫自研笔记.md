---
layout:     post
title:      爬虫自研笔记
subtitle:   潜龙，勿用
date:       2010-01-01
author:     Rainsblue.chan
header-img: ![3](../../../wallpaper/3.jpg)
catalog:   true
tags:
    - 经验
---
# 爬虫自研笔记

## 前言

​		用于学习爬虫的笔记，共勉。目录详见右侧。

​		笔记全手打，代码内容经过个人测试，有部分内容因python版本支持不同，有一些更替。

## 模块一	爬虫基础原理

### 01.必知必会，掌握HTTP基本原理

#### URI、URL、URN

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101322760.png" alt="image-20230621202914020" style="zoom:50%;" />

```URI和URL
例如：
https://github.com/favicon.ico既是一个URL，也是一个URI。
即有这样的一个图标资源，用URL/URI来唯一指定了它的访问方式，
这其中包括了访问协议HTTPS、访问路径（即github的根目录）和资源名称favicon.ico
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101322685.png" alt="image-20230621203232056" style="zoom:50%;" />

URL（统一资源定位符）是URI（统一资源标识符）的一个子集，URI还有一个子类叫做URN（统一资源名称，**它只负责命名资源而不制定如何定位资源**）。所以这个URL的作用是资源的**定位**，这是标识的一个细化，可以说是**特别的标识**。**只要一个资源有定位符，它则一定是URI的一种**。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101322859.png" alt="image-20230621203547812" style="zoom:50%;" />

我们看这幅图和这个例子，URN命名了一本书作为资源，它可以唯一标识这本书，**但是没有指定到哪里定位这本书**。

现在一般URN非常少，URL也可称作为URI（**网页链接**）。

![image-20230621204812916](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101322801.png)

ctrl+u查看源代码，都是超文本。

#### HTTP和HTTPS概念

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101322695.png" alt="image-20230621204937712" style="zoom:50%;" />

URL开头的这个http或者https就是访问资源所需要的协议类型。这两股协议是最常见的。我们来了解一下它们的含义。

![image-20230621205304617](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323227.png)

```HTTP概念
HTTP（Hyper Text Transfer Protocol）
中文名叫做超文本传输协议，用于从网络传输超文本数据到本地浏览器的传送协议，能保证高效而准确地传送超文本文档。
由来是万维网协会和IETF（互联网工作小组）共同指定的规范。
目前广泛使用的HTTP1.1版本。
```

![image-20230621205623679](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323682.png)

```HTTPS概念
多了一个套接层，SSL。大势所趋。
它的主要作用可以分为两种：
1.建立一个信息安全通道，来保证数据传输的安全
2.确认网站的真实性，凡是使用了HTTPS的网站，都可以通过点击浏览器地址的锁头标志来查看网站认证之后的真实信息，也可以通过CA机构颁发的安全签章来查询。
```

![image-20230621205952581](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323320.png)

#### HTTP请求过程

![image-20230621210134303](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323606.png)

输入URL，回车，实质是浏览器向网站服务器发送一个请求request，网站服务器接受到这个请求进行处理和解析，返回对应的response，传回给浏览器。响应里面会有网页源代码，浏览器会对这些超文本进行一个解析，就比如标签直接读掉，类似这种。

##### 开发者工具network选项卡使用

![image-20230621211021477](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323563.png)

![image-20230621211716430](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323317.png)

我们打开百度，进入开发者工具，查看网络选项卡。这里以我自己的截图举例子，我们看见网络下面有几个块，名称，状态，类型（type，指我们所请求的文件类型），启动器，大小，响应时间和可视化瀑布流。

我们这里以名称为“www.baidu.com"举例，它的状态为200，就是正常，类型为document，代表我们这里请求的是一个**“html文档”**，size如果是cache就是缓存的概念，说明曾经打开过这个网站？不是很理解，但是不太重要。

这里type这个标签还是很重要的吧。

![image-20230621212051878](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323852.png)

点开它，能够详细看到它的一些信息。

##### General部分

Request URL是指请求的网页，Request Method是指请求方法，Status Code为状态码，Remote Address，远程服务器地址端口，Referer Policy是判别策略。

响应头和请求头，请求头中带有许多请求信息，例如**浏览器标识**，**cookie**，**Host**等信息。这是请求的一部分。

![image-20230621225725985](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323068.png)

服务器会**根据请求头内的信息判断请求是否合法**，进而做出对应的响应。Response Headers就是响应的一部分。

![image-20230621225923041](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323821.png)

当中包含了服务器的类型，文档的类型，日期，这里GMT就是格林威治时间，我们在东八区，所以可知我现在是2023年6月21号晚上九点十六分在百度进行了一次请求。Server这里是百度自己的服务器，我搜了一下。

![image-20230621230258564](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323079.png)

还有一篇相关问答，[说是用来代替阿帕奇的](http://www.aiyiweb.com/linux/22997)。链接附在这段字下面了。

浏览器接收到响应会解析响应内容并显示在网页上。

#### 请求（GET和POST）

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101323493.png" alt="image-20230621230613872" style="zoom:50%;" />

![image-20230621231325437](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324297.png)

做web题的时候也常用这种方式，就是一个？跟着所需参数=某个数值。经常就是?c=system('tac fla?.txt');这种形式，就是**GET传参**

![image-20230621231340881](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324829.png)

这点也有一点点体会，我记得post传参是以表单的形式，不是在url中修改，一般都是hackbar直接post。我们看一下区别。

![image-20230621232259056](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324453.png)

就两个区别，一个是get传参写在url里面，post是写在请求体里的，另外一个是字节限制，get最多1024bytes，post则没有限制。

一般来说，登陆时包含了登陆密码，其中涉及到了**敏感信息**，使用GET的方式，密码就会暴露在url里面，造成密码的泄露，所以最好以post的方式发送，较大也会用post。

还有一些其他的请求方式，总结为下表。

![image-20230621232805049](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324601.png)

请求的网址，即**统一资源定位符URL**，可以唯一确定我们想请求的资源。请求头说明了附加信息，比较重要的信息有**cookie、referer、user-agent**

#### （重要）请求头分析（cookie、referer、user-agent）

![image-20230621232916967](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324331.png)

![image-20230621233228239](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324370.png)

![image-20230621233542244](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324914.png)

![image-20230621233736681](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324723.png)

![image-20230621234028796](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324996.png)

```分析ing
Accept：请求报头域，用于指定客户端可接受哪些类型的信息
这里我们看，text/html代表html格式，也有图像类image/apng代表apng图片，application/xhtml+xml代表xhtml+xml类型。当中其实是Content-Type，对照表网址为http://tool.oschina.net/commons,
Accept-Language：指定客户端可接受的语言类型
这里支持中文英文。
Accept-Encoding：指定客户端可接受的内容编码
gzip，deflate，br
Host：用于指定请求资源的主机IP和端口号，其内容为请求URL的原始服务器或网关的位置，从HTTP1.1版本开始，请求必须包含此内容
这里看见Host就是baidu。
Cookie：很重要，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据，它的主要功能是维持当前访问会话。如果你讲一个登陆了的网站的cookie删除，那么自动你就从网页中掉出去。上面那张图解释的很清楚了。
```

![image-20230621234055032](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324692.png)

![image-20230621234128871](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324235.png)

![image-20230621234331459](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324913.png)

所以Referer是标识请求来源，UA则是标识客户使用的操作系统及版本，比如你可以使得一个普通浏览器换成微信浏览器的标识，这样你就可以伪装成微信浏览器。

在写爬虫时，大部分时间都需要请求头。

#### 请求体

![image-20230621234655724](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324720.png)

登录github输入完账号密码，这样回车之后就会以表单形式提交给服务器，此时需要注意，**request headers制定content-type为application/x-www-form-urlencoded**，才会以表单的数据形式提交。以url形式编码。

也可以将content-type设置为json，来提交json的内容。或者设置multipart/form-data来上传文件。

![image-20230621235325396](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324474.png)

在制作爬虫时，如果需要使用post，必须正确了解content-type。、

#### 响应体

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324636.png" alt="image-20230621235456734" style="zoom:50%;" />

有各种状态码，一般200就是正常返回数据。其他遇到问题再搜。

![image-20230621235602881](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324965.png)

![image-20230621225923041](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324248.png)

响应编码为gzip，文档类型返回为text/html，html文档，用的utf-8编码，Server为百度自研1.1，这里有三个set-cookies，我估计是有不同的地方会用到，比如搜索栏，比如登录。

![image-20230622000611300](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324448.png)

响应体就是爬虫最后要解析的东西。

请求网页返回html源码，而图片则是它的二进制。

### 02.夯实根基，Web 网页基础

#### 网页的组成三大件：HTML、JavaScript、CSS

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324180.png" alt="image-20230622213016300" style="zoom:67%;" />

HTML相当于骨架、CSS是渲染格式，所以是皮肤、JavaScript是脚本，执行操作，所以相当于人体运动的肌肉，三者完整结合在一起才能形成网页。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324090.png" alt="image-20230624134740705" style="zoom:50%;" />

HTML定义网页的内容和结构，CSS描述网页的布局，JavaScript定义网页的行为。

##### HTML（Hyper Text Markup Language，超文本标记语言）

HTML是用来**描述网页**的一种语言

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324714.png" alt="image-20230624113333616" style="zoom:67%;" />

不同类型的元素通过不同类型标签来表示，如上图所示。

在网页当中选择开发者模式，elements选项卡中展示的就是HTML源码。下图为示例。

这里就是用div嵌套做一个个模块。

![image-20230624113600325](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324507.png)

![image-20230624122903222](scrawer/image-20230624122903222.png)

相互嵌套组合，形成了网页架构。但只有html的元素的网页并不美观，为了让网页更加美观，所以出现了CSS。

##### CSS（Cascading Style Sheets，层叠样式表）

![image-20230624123518409](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324504.png)

层叠是**排版**，而样式更像是word文档中调整文字格式大小，颜色等等的一个操作。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324880.png" alt="image-20230624130033048" style="zoom:50%;" />

```css
#head_wrapper.s-ps-islite.s-p-top{		
# 大括号前面是一个css选择器，这个选择器的作用首先是选中id为head_wrapper且class为s-ps-islite节点，然后再选中其中为s-p-top的节点
大括号其中的就是一条条样式规则
postion指定了这个元素的布局方式为绝对布局（absolute）
bottom指定了元素的下边距为40像素
width决定了宽度占百分之百
height决定元素的高度
```

等于就是我们把位置、宽度、高度统一写成这样的形式，然后用大括号括起来，接着在开头补充一个css选择器，这就代表**css选择器对于选中的元素生效了**。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324012.png" alt="image-20230624133009812" style="zoom: 50%;" />

我们看上面举过的实例。这里就是引用了testcss.css作为自己网页的一个层叠样式表。

![image-20230624133133012](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324096.png)

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324173.png" alt="image-20230624133358466" style="zoom:50%;" />

我们可以相对来修改一下它，比如图片之类的，就能够更好地来理解。

![image-20230624133553857](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324038.png)

这里我将第一块的图片给换了，就长这样。

现在HTML和CSS只是静态信息，为了使得网页具有交互效果，我们需要JS。

##### JavaScript（简称JS，一种脚本语言）

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101324136.png" alt="image-20230624134027750" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101325269.png" alt="image-20230624134101213" style="zoom:50%;" />

就如上图所示，在HTML中通过script，也就是JavaScript的script标签引入。script本身的意思就是脚本，所以这就是脚本标签。

#### 网页的结构

![image-20230624135541944](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101325722.png)

```
这里比较重要的是要注意div标签。它定义了网页中的区块，id是container，这是一个非常常用的属性，且id在网页中的内容是唯一的。我们可以通过它来获取这个区块。
这个区块内又有一个div标签，这里定义一个class为wrapper，这也是非常常用的属性，经常与css配合使用来设定样式。
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101325945.png" alt="image-20230624135250017" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310101325750.png" alt="image-20230624140659775" style="zoom:67%;" />

新建一个文本文件，然后应该在浏览器上的呈现就会是这样。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211771.png" alt="image-20230624135949771" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211031.png" alt="image-20230624140017080" style="zoom:50%;" />

这里我们就有这样一个例子，作者没用id的方式，而是直接用class，说明内容量不多吧，然后直接引用的css中的div.第一块来作为一个样式的修改。

#### 节点树及节点间的关系（DOM）

在HTML中，**所有标签定义的内容都是节点**，它们构成了一个**HTML DOM树**。

DOM是W3C（万维网联盟）的标准，其英文全称为Document Object Model，即文档对象模型。

![image-20230624140942015](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211222.png)

DOM是一个接口，用于程序、脚本动态访问，更新文档内容、结构和样式。

W3C也根据对象分为3个不同的部分，核心DOM（**针对任何结构化文档，泛用性广**），XML DOM（只针对XML文档），HTML DOM（只针对HTML）。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211195.png" alt="image-20230624141550927" style="zoom:67%;" />

根据W3C的HTML DOM标准，HTML文档中的所有内容都是节点：

- 整个文档是一个文档节点
- 每个HTML元素是元素节点
- HTML元素内的文本是文本节点
- 每个HTML属性是属性节点
- 注释是注释节点

##### HTML节点树

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211622.png" alt="image-20230624141905083" style="zoom:50%;" />

```HTML节点树
通过HTML DOM
树中的所有节点均可通过JavaScript访问
所有HTML节点元素均可被修改
也可以被创建或删除
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211549.png" alt="image-20230624142239775" style="zoom:50%;" />

##### 定位节点（css选择器）

> 一般我们会用css选择器来定位节点，比如<div id="container">

这个就表示为 **#container**，其中  #  开头代表选择id，其后紧跟id的名称。

如果要选择class为wrapper的节点，可以使用  **.wrapper**

这里  .  开头代表选择class，其后紧跟class的名称，也可使用根据标签名筛选，如想选择二级标题，直接用h2即可。

![image-20230624142658900](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211430.png)

支持**嵌套选择**，各个选择器加上空格分隔开代表嵌套。如#container .wrapper p（中间有两个空格）

- 代表先选择id为container的节点
- 然后再选中其**内部**为wrapper的节点
- 然后再进一步选中其内部的p节点

如果不加空格，则代表**并列关系**，如div#container .wrapper p.text（但这个例子应该是没有并列...）

- 代表先选择id为container的div节点
- 然后选中其内部的class为wrapper的节点
- 再进一步选中其内部的class为text的p节点

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211052.png" alt="image-20230624143801110" style="zoom:67%;" />

这里能看见并列关系用的是**逗号，** 

还有一种常用的选择器是**XPath**，之后会提到。

### 03.原理探究，了解爬虫的基本原理

#### 爬虫概述

爬虫就是获取网页并提取和保存信息的自动化程序。蜘蛛通过节点连线，获取下一个节点，这样所有的节点都可以被爬取到。

##### 获取网页

爬虫首先要做的就是**获取网页**，就是获取网页的源代码。

**源代码里包含网页的部分有用信息**，只要把源代码获取下来，就可以从中提取想要的信息。

前面讲了**请求**和**响应**的概念。向服务器发送一个请求，返回的**响应体**便是网页的源代码。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211480.png" alt="image-20230626091852403" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211178.png" alt="image-20230626091918093" style="zoom:50%;" />

我们可以利用这些库非常方便地完成**HTTP请求操作**。

请求和响应都可以用类库提供的数据结构来表示。得到响应之后，只需要解析数据结构中的body部分即可，即得到**网页源代码**。这样我们就可以用程序来实现获取网页内容的过程了。

##### 提取信息

获取网页源代码后，接下来就是分析网页源代码，从中提取想要的数据。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211315.png" alt="image-20230626092416653" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211519.png" alt="image-20230626092432288" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211150.png" alt="image-20230626092446589" style="zoom:50%;" />

```
两种方法：
最通用的方法是采用正则表达式提取，这是万能方案，但是构造容易出错。
第二种方法基于网页结构的规则，所以有一些根据网页节点属性、css选择器或XPath来提取网页信息的库，
比如Beautiful Soup、pyquery、lxml等等，它们可以高效提取节点属性、文本值等等。
```

##### 保存数据

提取信息后，一般会将提取到的数据保存到某处以便后续使用。

保存形式多种多样，如可以简单保存为**TXT文本或者JSON文本**。

也可以保存到数据库，如**MySQL和MongoDB**等。

还可以保存到远程服务器，如**借助SFTP进行操作**等。

##### 自动化程序

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211467.png" alt="image-20230626092923370" style="zoom:50%;" />

自动化程序的出现就是为了大量采集数据。

#### 能抓怎样的数据

```
在网页中能看到各种各样的信息，最常见的便是常规网页，它们对应着HTML代码，而最常抓取的便是HTML源代码。

有些网页返回的不是HTML代码，而是一个JSON字符串。而且这种数据提取会更加方便。

网页中还会有二进制文件，比如图片，视频和音频，利用爬虫可以将他们捕捉下来改成对应文件名。
```

#### JavaScript渲染页面

有时用urllib或requests抓取网页时，**得到的源代码实际和浏览器中看到的不一样**。

这是非常常见的情况，现在网页越来越多的采用**Ajax、前端模块化工具**来构建，

整个网页可能都是有JavaScript渲染出来的，即通过这个脚本重新写入HTML代码。

**原始的HTML代码可能就是一个空壳。**

例子如下图。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102211968.png" alt="image-20230626094244345" style="zoom:50%;" />

这里我们看见在html代码中body里只有一个节点为container。后面呢引入了这个app.js的脚本。

如果这个时候我们用urllib或者requests来做一个html的请求，那么得到的只会是这一段HTML的代码。

因此使用基本HTTP请求库得到的源代码可能和浏览器中的页面源代码不太一样，可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染。

### 04.基础探究，Session与Cookies

我们在网页浏览时经常会遇到登录的情况，有些网页只有登录之后才能够访问，而登录之后可以连续很多次的访问网站，但是有时候过一段时间就需要重新登录。还有一些网站，打开浏览器就自动登录了，而且很长时间都不会失效。

这种情况设计**Session和Cookies**的相关知识。

#### 静态网页和动态网页

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212352.png" alt="image-20230626095228058" style="zoom:50%;" />

静态网页一般会放在公网服务器上，一般配置有nginx或者apache。

但是存在很大缺陷，比如**可维护性差，不能根据URL灵活多变地显示内容**等。比如输入一个name参数，就没有办法找。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212996.png" alt="image-20230626095431401" style="zoom: 67%;" />

动态网页可以动态解析URL中参数的变化，**关联数据库**并动态呈现不同的页面内容，非常灵活多变。

现在遇到的大多数网站都是动态网站，不再是一个简单的HTML。

可能由JSP、PHP、Python等语言编写，其功能比静态网页强大和丰富太多，

动态网站还可以实现用户登录和注册的功能。

这种功能需要一个**特殊的凭证**，而这就是**Session和Cookies**的作用。

而在了解它们之前，我们必须了解HTTP的一个特点，叫做无状态。

#### 无状态HTTP

HTTP特点之一——**无状态**

是指**HTTP协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态**。

HTTP有点像我，记性很差。

当我们向服务器发送请求后，服务器解析这个请求，然后返回对你的响应，服务器负责完成这个过程，而且这个过程是完全独立的，**服务器不会记录前后状态的变化，也就是缺少记录。**

这就意味着，**如果后续需要处理前面的信息，则必须重传**，这也导致需要额外传递一些前面的重复请求，才能获取后续响应。然而这种效果肯定不是我们想要的，为了保持前面的状态，我们还得全部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。

```
例如，一个人需要登录网站，他做了一次这个操作，当他再需要进入网站，那么他还要做一次这个操作。（如果没有session和cookies）
```

于是两个保持HTTP连接状态的技术就出现力！就是我们的Session和Cookies。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210360.png" alt="image-20230626101348601" style="zoom:80%;" />

![image-20230626102557122](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210464.png)

在服务端：网站的服务器用来保存用户的Session信息，我们可以简单称之为**会话信息**。

在客户端：可以理解为浏览器端，浏览器在下次访问网页时会自动附带上它发送给服务器，**服务器通过识别Cookies并鉴定出是哪个用户，然后再判断用户是否是登录状态，进而返回对应的响应**。这种响应的表现形式就是你的网页，比如你在网页端登录了bilibili，那么这个时候的响应就是你的右上角有个头像。

有了cookies，浏览器在下次访问网页时，会自动附带上它，把他发送给服务器，我们可以理解为cookies里面保存了用户的凭证，因此下次登录就不需要再次输入用户名密码等信息，重新登陆。

因此在爬虫中，有时候处理一些需要登录才能访问的页面，我们一般**会将登录成功之后获取的这个cookies放在请求头中直接请求**，而不必重新模拟登录。

好了，详细地了解了它们的概念之后，我们剖析它们的原理。

#### Session（会话）

其本身的含义是指**有始有终的一系列动作/消息**。

比如打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个session，有点像反序列化中，我们的魔术手套，**不改变数据结构，但是只改变属性**，动作相同的证明，正式那个反序列化值。这里的session值，**就是这些动作的证明**。

在Web中，Session对象用来存储特定用户Session所需的属性及配置信息。这样当用户跳转页面，session变量中所存储的信息不会丢失。下面是百度的，讲的也差不离。

[session的概念](https://baike.baidu.com/item/session/479100?fr=aladdin)，在计算机中，尤其是在网络应用中，称为**“会话控制”**。**Session对象存储特定用户会话所需的属性及配置信息。**这样，当用户在应用程序的Web页之间跳转时，存储在Session对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。**（这样就不会被无状态，或说是无记忆的HTML影响）**当用户请求来自应用程序的 Web页时，如果该用户还没有会话，**则Web服务器将自动创建一个 Session对象。**当会话过期或被放弃后，服务器将终止该会话。Session 对象最常见的一个用法就是**存储用户的首选项**。例如，如果用户指明不喜欢查看图形，就可以将该信息存储在Session对象中。有关使用Session 对象的详细信息，请参阅“ASP应用程序”部分的“管理会话”。注意会话状态仅在支持cookie的浏览器中保留。

#### Cookies

指某些网站**为了辨别用户身份、进行Session跟踪而存储在用户本地终端上的数据**。

我们接下来讲一下这个过程：

##### **当客户端第一次请求服务器时**

服务器会返回一个响应头中带有**Set-Cookie字段**的响应给客户端

用来标记是哪一个用户，客户端浏览器会把Cookies保存起来

比如我这里搜索赛尔号，就是向百度的BWS服务器发送一个请求，这个post被解析之后会返回一个带有set-cookie字段的response，见下图，这用于标记我。

![image-20230626112827958](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210785.png)

##### **当浏览器下一次再请求该网站时**

浏览器会把此Cookies放到请求头一起提交给服务器

Cookies携带了Session ID信息，服务器检查该Cookies即可找到对应的Session是什么

然后判断Session来以此辨认用户状态

或者我们就说bilibili得了

![image-20230626113416909](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212185.png)

这是一样的，Set-Cookie会在这里做一个标记。这里有我的Session ID信息，然后就能交给服务器判断。

##### **在成功登录某个网站时**

服务器会告诉客户端设置哪些Cookies信息

在后续访问页面时客户端会把Cookies发送给服务器，服务器再找到对应的Session加以判断

如果Session中的某些设置登录状态的变量是有效的

就证明用户处于登录状态

**此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。**

反之，Cookies无效则返回上一页面。

##### Cookies位置

位于**开发者选项卡中application里的Storage**。

![image-20230626143637884](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212784.png)

这其中有很多条目，每一个条目都可以称作是一个**Cookie**。

它有以下几个属性：

- **Name**，即该Cookie的名称，这里要注意，**Cookie一旦创建，Name就不可能更改。**
- **Value**，即该Cookie的值，**如果值为Unicode字符，需要为字符编码**，**如果值为二进制属性，则需要使用base64编码**。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212940.png" alt="image-20230626144155771" style="zoom:80%;" />

- **Expires/Max-Age**，第一个i-cookie（有点没听清）生成的时间，单位为秒。常和Expires一起使用，计算cookie有效时间。**Max-Age如果为正数，则value，cookie在（max-age秒）之后失效。如果Max-Age为负数，则关闭浏览器时cookie即失效。**（浏览器也不会以任何形式保存该cookie）

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212853.png" alt="image-20230626145029762" style="zoom:50%;" />

- **Path**，即该cookie的使用路径，如果设置为Path路径，将只有路径设置为path的路径能够访问该cookie。如果设置为根路径，则本域名下所有的页面都可以访问该cookie。
- **Domain**，即可以访问该cookie的域名。例如设置为.zhihu.com，则所有以zhihu.com为结尾的都可以访问该cookie。
- **Size**,cookie大小。
- **HTTP**，即**此cookie的HTTP-only属性**。若此属性为True，则只有在HTTP-header中会带有此cookie的信息。**而不能通过document.cookie来访问此cookie。

##### 会话Cookie和持久Cookie

会话Cookie（Session-Cookie）就是把Cookie放在浏览器内存里，浏览器在关闭之后该Cookie即失效（**短时使用**）

持久Cookie则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态

**严格来说**，没有会话Cookie和持久Cookie之分，只是由Cookie的Max Age或Expires字段决定了过期的时间。Max Age为正就是cookie在Max Age秒之后失效，为负就是关闭浏览器后失效。**持久Cookie就是相应把Session有效时间调长一些**。

#### 登录Session控制的实现图

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212592.png" alt="image-20230626143113839" style="zoom:50%;" />

登录会话控制由两者共同协作，服务器端是Session来记录会话信息和属性，客户端是Cookies来记录用户信息。以此来填补HTML的无状态（无记忆）。

#### 常见误区

##### **在谈论Session机制时**

并不是将浏览器关闭，Session就消失，**而是会保留在服务器上，除非程序通知服务器删除一个session**。

**比如程序一般都是在我们做注销操作时才去删除Session**。我们做个实例。

- **注销前**

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212428.png" alt="image-20230626151156571" style="zoom:67%;" />

- **注销后**

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212475.png" alt="image-20230626151245663" style="zoom: 67%;" />

我们看见这个Session因为注销被删除了，关闭浏览器后Session也没有删除。

##### **当关闭浏览器时**

但是浏览器不会在关闭之前主动通知服务器它将要关闭，所以**服务器根本不会有机会知道浏览器已经关闭**。

之所以有这种错觉，是因为大部分网站都用的是会话Cookie。（**Cookie随关随注销**）当再次连接到服务器，没了cookie自然找不到session，不过，如果硬盘存储了cookie或是改写了请求头，那么仍然能恢复连接状态，找到session-id。

##### **由于关闭浏览器不会导致Session被删除**

这就需要服务器为Session设置一个失效时间，当距离客户端上一次使用Session的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把Session删除以节省存储空间。

### 05.多路加速，了解多线程基本原理

我们知道，在一台计算机中可以同时打开许多软件，比如同时打字听音乐。

这涉及到**多线程和多进程**。

为了提高爬虫效率，同样也需要这方面知识。我们这里主要是看如何在python中实现这种过程。

#### 多线程的含义

进程可以理解为一个**可以独立运行的程序单位**。比如开一个notepad，就是开启一个notepad进程。

一个进程中**可以同时处理很多事情**，比如浏览器可以在多个选项卡中打开多个页面，可以同时运行，互不干扰。

为什么能同时运行这么多任务？**任务对应着线程的执行。**

进程是线程的集合，是由一个或多个线程构成的，线程则是**操作系统进行运算调度的最小单位，是进程中的一个最小运行单元**。

比如上面的浏览器进程，播放音乐就是一个线程。

多线程就是**一个进程中同时执行多个线程**。

#### 并发和并行

![image-20230627073319815](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212556.png)

并发只有一个核，同一时刻只能有一条指令，看上去多个线程同时执行，实际上是快速切换；

并行需要多处理器，多线程同一时刻执行。

#### 多线程适用场景

在一个程序进程中，有些操作是比较**耗时**或者**需要等待**的，如：

- 等待数据库的查询结果的返回
- 等待网页结果的响应

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212277.png" alt="image-20230627073649527" style="zoom:80%;" />

使用单线程，处理器必须要等待这些操作完成才能继续执行，这个时候处理器被闲置；

使用多线程，处理器就可以在某个线程等待时执行其他线程，从而从整体上提高执行效率。

**网络爬虫**就是一个典型的例子，爬虫在向服务器发送请求之后，有一段时间必须要等待服务器的响应返回，这种任务就属于**IO密集型任务（IO-bound）（CPU相较为闲置）**。但也不是所有的任务都是IO-bound，还有**CPU密集型（CPU-bound）**，就是任务的运行一直需要处理器参与，这个时候IO就被闲置。**如果这个时候开启多线程，那么从一个cpu-bound切换到另一个cpu-bound，处理器依旧不会停下来，始终会忙于计算。（这样并不会节省总体的时间，因为所需要处理的任务量的总量是不变的）**

##### 总结

如果任务不全是CPU-bound，那么可以使用多线程来提高程序整体执行效率**（就是把CPU等待时间全给它变成执行）**，尤其对于**网络爬虫这种IO-bound任务**来说，使用多线程会**大大提高程序整体的爬取效率**。

#### Python实现多线程

##### threading、join、守护线程

在Python中，实现多线程的模块叫做**threading**，是Python自带的模块。下面开始演示。

```Python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: threadtest
import threading
import time

def target(second):
    print(f'Threading {threading.current_thread().name} is running')
    print(f'Threading {threading.current_thread().name} sleep {second}s')
    time.sleep(second)
    print(f'Threading {threading.current_thread().name} is ended')

print(f'Threading {threading.current_thread().name} is running')
for i in [1,5]:
    t = threading.Thread(target=target, args=[i])
    t.start()
    t.join()
print(f'Threading {threading.current_thread().name} is ended')
```

首先，我们可以使用thread类来创建一个线程。创建时需要**指定target参数为运行的方法名称**，如果被调用的方法需要插入额外的参数，则可以通过**thread-args**来指定。

在这里，我们首先声明了一个方法，叫做**target**。它接收一个参数叫做**second**，通过方法的实现可以发现，这个方法就是实现了一个**time.sleep**休眠操作，second参数就是**休眠秒数**。提前后print一些内容。其中线程的名字，我们可以通过**threading.current_thread().name**获取。如果是主线程，起值就是main thread，如果是子线程，起值就是thread-*，这个就是代表线程自己的名称。

然后我们通过thread类新建了两个线程，target参数就是我们定义的方法名，args，以列表的方式来呈递，两次循环中，这里i分别是1和5，这样两个线程休眠时间分别为1秒和5秒。声明完成之后，就可以通过start开始运行。

![image-20230627083218902](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212287.png)

主线程首先运行结束，然后是两个子线程接连运行结束，分别间隔1秒和4秒。这说明主线程并没有等待子线程并结束运行，而是直接退出了，有些不符合常理。

如果我们想要主线程等待子线程运行完毕再退出，可以让每个子线程都调用下面的**join方法**。 

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212006.png" alt="image-20230627084345524" style="zoom: 50%;" />

另外，也可以通过继承thread类的方式，需要在方法中写入run。

如果一个线程被设置为守护线程，那么就说明这个线程是不重要的。这就意味着如果主线程结束了，守护线程还没有运行完，那么它将被**强制结束**。在Python中，我们可以通过**setDaemon**方法来设置守护线程。

```Python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: threadtest
import threading
import time

def target(second):
    print(f'Threading {threading.current_thread().name} is running')
    print(f'Threading {threading.current_thread().name} sleep {second}s')
    time.sleep(second)
    print(f'Threading {threading.current_thread().name} is ended')

print(f'Threading {threading.current_thread().name} is running')
t1 = threading.Thread(target=target, args=[2])
t1.start()
t2 = threading.Thread(target=target, args=[5])
t2.setDaemon(True)	#3.7可以这么写，如果是3.10，用t.daemon=true
t2.start()
print(f'Threading {threading.current_thread().name} is ended')
```

![image-20230627085341040](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212102.png)

这里我们没有看见Thread2退出的信息，说明随着MainThread的退出而退出了。这里并没有调用join方法，如果调用，仍然会等待，无论是否为守护线程。

图为加了join后

![image-20230627085713584](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212147.png)

##### 互斥锁

在一个进程中的多个线程是共享资源的，比如在一个进程中有一个全局变量count用于计数，现在我们声明多个线程，每个线程运行值都给count加1，让我们看看效果如何。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: threadtest
import threading
import time

count = 0

class MyThread(threading.Thread): # 定义一个创建线程
    def __init__(self):
        threading.Thread.__init__(self)

    def run(self):
        global count
        temp = count + 1
        time.sleep(0.001)
        count = temp

threads =[] # 创建一个空列表用于记录？大概
for _ in range(1000): # 不使用迭代变量，也就是同样内容执行1000次
    thread = MyThread()
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join() # 等待thread进程结束
print(f'Final count: {count}') # f-string，统计count次数
```

结果如下

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212289.png" alt="image-20230627091624864" style="zoom:50%;" />

在这里我们声明了1000个线程，各休眠一小段时间，然后对count赋予新的值。那这样**按照常理，count值应该是1000**。但是count居然是8。原因是因为**count是共享的，每个线程都可以在执行count = temp这段代码时拿到count的值。但是这些线程可以是并发或者并行执行的。这就导致了不同的线程可能拿到的是同一个count值，加1操作也就没有生效，最后导致结果偏小。**不同的电脑可能在这里结果不一样，但是结果大致都是偏小的。

所以，**如果多个线程同时对某个数据进行读取或者修改，就会出现不可预料的结果**。为了避免这种情况，我们需要对多个线程进行**同步**，要实现同步，我们可以对需要操作的数据进行**加锁保护**。这里就需要threading.Lock（）了。

##### 加锁保护

某个线程在对数据进行操作前，需要先**加锁**，这样其他的线程发现被加锁了之后，就无法继续向下执行，会一直等待锁被释放，只有加锁的线程把锁释放了，其他的线程才能继续加锁并对数据做修改，修改完了再释放锁，**这样可以确保同一时间只有一个线程操作数据，多个线程不会再同时读取和修改同一个数据**，最后的运行结果就是对的了。我觉得这有点类似于并发。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: threadtest
import threading
import time

count = 0

class MyThread(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)

    def run(self):
        global count
        lock.acquire() # 获取count前先加锁
        temp = count + 1
        time.sleep(0.001)
        count = temp
        lock.release() # run完之后再释放锁

lock = threading.Lock()
threads =[] # 创建一个空列表用于记录？大概
for _ in range(1000): # 不使用迭代变量，也就是同样内容执行1000次
    thread = MyThread()
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join() # 等待thread进程结束
print(f'Final count: {count}') # f-string，统计count次数
```

在run方法中，在获取count前先加锁，等线程run完之后再释放锁。这样多个线程就不会同时修改操作同一个count值了。

![image-20230627094927086](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212343.png)

#### Python多线程的问题（GIL）

**GIL**，全称为Global Interpreter Lock，译为**全局解释器锁**。其最初设计是为了数据安全而考虑的。

由于python中GIL的限制，**无论是多核和单核，同一时刻只能运行一个线程**，这直接**导致python多线程无法发挥多核并行的优势**。这里一定要注意一点，GIL限制的是**线程**，不是进程。

在Python多线程下，每个线程的执行方式如下：

- 获取GIL
- 执行对应线程的代码
- 释放GIL

##### Python多线程下同一时刻只能运行一个线程的核心原因

我们可以把GIL想象成一张通信证，并且一个Python进程中只有一个GIL。拿不到通信证的线程就不允许执行，所以多线程下只能一个个来的原因，就在于此。

不过对于爬虫这种IO-bound来说，这个影响并不大（**原因是因为爬虫发送一个HTML请求后大多数时间都是在等待，cpu在这段时间内闲置**），而对于Cpu-bound，由于GIL的存在，多线程的效率可能会比单线程还低。

### 06.多路加速，了解多进程基本原理

上面讲的多线程，然后还提到Python中的多线程是不能很好的发挥多核的优势（**并行**）

#### 多进程的含义

进程（Process）是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是系统进行和资源分配和调度的一个独立单位，**多进程就是启用多个进程同时运行**。由于进程是线程的集合，所以一个进程运行就是**大于等于线程数量运行**。

#### Python多进程的优势

上一回说到，**由于进程中GIL的存在，Python中的多线程并不能很好地发挥多核优势，一个进程中的多个线程，同一时刻只能有一个线程运行。**这是线程的劣势。

然而**对于进程**，对于多进程来说，每个进程都有属于自己的GIL，**所以在多核处理器下，多进程的运行是不会受GIL的影响的，多进程能更好地发挥多核的优势**。（即，受GIL影响，多线程只能并发，而多进程仍旧并行，依旧坚挺）

当然，对于爬虫这种IO-bound来说，多线程和多进程实际差别并不大。而对于Cpu-bound任务，python的多进程会比多线程有成倍的提升。

##### 小总结

Python的多进程整体比多线程更有优势，在条件允许情况下能用多进程就尽量用多进程。

由于进程是系统进行资源分配和调度的一个**独立单位**，所以各个进程之间的数据是**无法共享的**。上面的线程例子也可以很好证明，**不加锁就有可能公用一个count变量，然后那个数值就并不会等于线程量而是小于它。**

#### 多进程的实现（multiprocessing）

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212071.png" alt="image-20230627175301941" style="zoom:50%;" />

这是一个库，提供很多功能。

##### 直接使用Process类

在multiprocessing中，每个进程都用一个Process类来表示

API调用：

```api
先解释一下何为API调用：
文章链接：https://weibo.com/ttarticle/p/show?id=2309404717603309355717
应用程序编程接口 (API) 是一组规则，使软件程序能够将数据传输到另一个软件程序。API 使开发人员能够避免冗余工作；与构建和重建已经存在的应用程序功能不同，开发人员可以通过按照 API 要求格式化请求，将现有功能整合到他们的新应用程序中。
API 是一种“接口”，意思是一种事物与另一种事物进行交互的方式。作为一个真实的例子，ATM 有一个界面——一个屏幕和几个按钮——允许客户与他们的银行互动并请求服务，比如取钱。同样，API 是一个软件如何与另一个程序交互以获得所需服务的方式。
想象一下，詹妮弗建立了一个网站，帮助通勤者在上班前检查公路交通。Jennifer 可能会花费大量时间和金钱来建立一个复杂的高速公路跟踪系统，以将这些信息提供给她网站的用户。但是这些能力已经存在，因为外部各方已经创建了这样的系统。Jennifer 的网站没有以这种方式重新发明轮子，而是使用由外部高速公路跟踪服务提供的 API。现在 Jennifer 可以专注于构建网站的其他方面。也就是用已经做好的模块。

什么是 API 调用？
API 调用，也称为 API 请求，是指向触发 API 使用的 API 的消息。回顾该示例，Jennifer 构建她的网站时，它会在加载时自动生成对高速公路跟踪服务的 API 调用。响应从该服务返回到网站，并使其能够显示最新的高速公路交通信息。
API 调用必须按照 API 的要求进行格式化才能工作。API 的要求称为其“架构”。该模式还描述了提供给每个请求的响应类型。
假设一位通勤者使用 Jennifer 的网站检查 192 号高速公路上的交通情况。该网站发送一个 API 调用来提供此信息 — 一条消息，内容为“192 号高速公路”。高速公路跟踪服务的 API 服务器收到此消息并回复 192 号高速公路上的行驶时间。
```

**Process([group [,target [,name [,args [,kwargs]]]]])**

- **target**表示调用对象，你可以传入方法的名字
- **args**表示被调用对象的位置参数元组

比如target是函数func，他有两个参数m，n，那么args就传入[m, n]即可

- **kwargs**表示调用对象的字典，如kwargs = {'name':Jack, 'age':18}
- **name**是别名，相当于给这个进程取一个名字
- **group**是分组

接下来是一个实例，一个简单的创建。

```Python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: 06processtest
import multiprocessing

def process(index):
    print(f'Process: {index}')

if __name__== '__main__':
    for i in range(5):
        p = multiprocessing.Process(target=process, args=(i,)) 
        # 注意这里的args必须是一个元组，如果只有一个参数，那么就要在元组的第一个元素后面加一个逗号
        # 如果没有这个逗号，那么这个和单个元素本身是没有区别的，这会导致参数的传递有问题
        p.start()
```

![image-20230627181844420](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212434.png)

结果就是这样，我们看一下它讲了什么。很简单，就是使用multiprocessing这个类，通过start即可启动，然后就会出现五个子进程，每个子进程都是调用了process方法。Process的index参数通过args传入，分别是0到4。

各个进程的数据不会共享，当前cpu核数不同的话，还会分配不同的核来执行进程，真正做到并行。

```python
几个比较有用的方法：
通过cpu_count方法来获取当前机器CPU的核心数量
通过active_children方法获取当前还在运行的所有进程
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: 06processtest
import multiprocessing
import time

def process(index):
    time.sleep(index)
    print(f'Process: {index}')

if __name__== '__main__':
    for i in range(5):
        p = multiprocessing.Process(target=process, args=[i])
        p.start()
    print(f'CPU number: {multiprocessing.cpu_count()}')
    for p in multiprocessing.active_children():
        print(f'Child process: {p.name} id: {p.pid}')
    print('Process Ended')
```

结果如下，发现我电脑有16个核。不同机器结果可能不同。通过p.name和p.pid可以很方便地得到进程信息。

![image-20230628072253225](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102212831.png)

以上我们就通过**直接使用Process类**完成多进程创建和一些基本信息的获取。

##### 继承Process类

创建进程的方式不止一种，也可以像线程Thread一样来通过继承的方式创建一个进程类，进程的基本操作在子类的run方法中实现即可。我们通过一个实例来看。

```Python
# Author: Rainsblue.chan
# Create: 2023/6/28
# FileName: 06继承
from multiprocessing import Process
import time

class MyProcess(Process):
    def __init__(self, loop): # 接受一个参数loop作为循环次数
        Process.__init__(self)
        self.loop = loop

    def run(self):
        for count in range(self.loop):
            time.sleep(1)
            print(f'Pid: {self.pid} LoopCount: {count}')

if __name__ == '__main__':
    for i in range(2,5): # i打印2，3，4
        p = MyProcess(i)
        p.start()
```

![image-20230628220546179](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102213225.png)

不同的机器，不同的时刻，运行时间可能不同。

##### 守护进程（类似守护线程，当主进程结束后，守护进程直接关闭）

如果一个进程被设置为守护进程，当父进程结束后，子进程会自动被终止，可以通过**设置daemon属性**来控制是否为守护进程。下面还是例子。

```Python
# Author: Rainsblue.chan
# Create: 2023/6/28
# FileName: 06继承
from multiprocessing import Process
import time

class MyProcess(Process):
    def __init__(self, loop):
        Process.__init__(self)
        self.loop = loop

    def run(self):
        for count in range(self.loop):
            time.sleep(1)
            print(f'Pid: {self.pid} LoopCount: {count}')

if __name__ == '__main__':
    for i in range(2,5):
        p = MyProcess(i)
        p.daemon = True
        p.start()

print('Main Process ended')
```

![image-20230628221625584](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102213288.png)

可以防止无控制的执行子进程。但是这里主进程直接运行结束了，因为给每一个子进程都设置了守护进程。

所以这里同样要进程等待，**join方法**。

```Python
if __name__ == '__main__':
    for i in range(2,5):
        p = MyProcess(i)
        p.daemon = True
        p.start()
        p.join()
```

结果如下

```无
Main Process ended
Pid: 35344 LoopCount: 0
Pid: 35344 LoopCount: 1
Main Process ended
Pid: 4492 LoopCount: 0
Pid: 4492 LoopCount: 1
Pid: 4492 LoopCount: 2
Main Process ended
Pid: 44224 LoopCount: 0
Pid: 44224 LoopCount: 1
Pid: 44224 LoopCount: 2
Pid: 44224 LoopCount: 3
Main Process ended
```

在调用start方法之后，父进程就在等待所有子进程运行完毕，join方法是无限等待，所以子进程没有完成，父进程就结束不了。所以可以加一些限制时间，这就是**父进程的最长等待时间**，当这个时间用完，父进程强制退出。

```Python
if __name__ == '__main__':
    for i in range(2,5):
        p = MyProcess(i)
        p.daemon = True
        p.start()
        p.join(1)
```

结果一下子就少了很多。

```None
Main Process ended
Pid: 18204 LoopCount: 0
Main Process ended
Pid: 18204 LoopCount: 1Pid: 30540 LoopCount: 0

Main Process ended
Main Process ended
```

运行一秒就被强制返回了。

##### 终止进程

终止进程当然不止守护进程一种方法，还可以通过**terminate方法**来终止某个子进程，还可以通过**is_alive方法**判断进程是否还在运行。

接下来是实例

```Python
# Author: Rainsblue.chan
# Create: 2023/6/28
# FileName: 06继承
import multiprocessing
import time

def process(): # 休眠
    print('Starting')
    time.sleep(5)
    print('Finished')

if __name__ == '__main__':
    p = multiprocessing.Process(target=process)
    print('Before:', p, p.is_alive())

    p.start()
    print('During:', p, p.is_alive)

    p.terminate()
    print('Terminate:', p, p.is_alive())

    p.join()
    print('Joined:', p, p.is_alive())
```

结果如下；

```
Before: <Process name='Process-1' parent=29032 initial> False
During: <Process name='Process-1' pid=32040 parent=29032 started> <bound method BaseProcess.is_alive of <Process name='Process-1' pid=32040 parent=29032 started>>
Terminate: <Process name='Process-1' pid=32040 parent=29032 started> True
Joined: <Process name='Process-1' pid=32040 parent=29032 stopped exitcode=-SIGTERM> False
```

##### 进程互斥锁

我们上面执行的一些结果，经常会出现没有换行的结果，这是因为**并行**的结果。

```
如果能保证多个进程运行期间的任一时间，
只能一个进程输出，其他进程等待，
等刚才那个进程输出完毕之后，
另一个进程再进行输出
就不会出现输出没有换行的现象
```

这种方案实际就是实现了**进程互斥**。多个进程同时抢占临界区，即输出资源，我们可以通过lock实现，就是加锁。

我们先实现一个不加锁的。

```Python
# Author: Rainsblue.chan
# Create: 2023/6/28
# FileName: 06继承
from multiprocessing import Process, Lock
import time

class MyProcess(Process):
    def __init__(self, loop, lock):
        Process.__init__(self)
        self.loop = loop
        self.lock = lock

    def run(self):
        for count in range(self.loop):
            time.sleep(0.1)
            # self.lock.acquire()
            print(f'Pid: {self.pid} LoopCount: {count}')
            # self.lock.release()

if __name__ == '__main__':
    lock = Lock()
    for i in range(10, 15):
        p = MyProcess(i, lock)
        p.start()
```

我们看一下结果

```
Pid: 30504 LoopCount: 0
Pid: 35192 LoopCount: 0
Pid: 39400 LoopCount: 0
Pid: 36484 LoopCount: 0
Pid: 35300 LoopCount: 0
Pid: 36484 LoopCount: 1
Pid: 39400 LoopCount: 1Pid: 35192 LoopCount: 1

Pid: 35300 LoopCount: 1
Pid: 30504 LoopCount: 1
Pid: 35300 LoopCount: 2Pid: 36484 LoopCount: 2
Pid: 30504 LoopCount: 2
Pid: 35192 LoopCount: 2Pid: 39400 LoopCount: 2


Pid: 35300 LoopCount: 3Pid: 39400 LoopCount: 3Pid: 35192 LoopCount: 3Pid: 36484 LoopCount: 3Pid: 30504 LoopCount: 3




Pid: 30504 LoopCount: 4Pid: 39400 LoopCount: 4
Pid: 35192 LoopCount: 4Pid: 35300 LoopCount: 4


Pid: 36484 LoopCount: 4
Pid: 35300 LoopCount: 5Pid: 36484 LoopCount: 5
Pid: 30504 LoopCount: 5
Pid: 35192 LoopCount: 5Pid: 39400 LoopCount: 5


Pid: 36484 LoopCount: 6Pid: 30504 LoopCount: 6Pid: 35192 LoopCount: 6
Pid: 39400 LoopCount: 6


Pid: 35300 LoopCount: 6
Pid: 39400 LoopCount: 7Pid: 35300 LoopCount: 7Pid: 35192 LoopCount: 7Pid: 30504 LoopCount: 7Pid: 36484 LoopCount: 7




Pid: 36484 LoopCount: 8Pid: 30504 LoopCount: 8Pid: 35192 LoopCount: 8Pid: 35300 LoopCount: 8Pid: 39400 LoopCount: 8




Pid: 35192 LoopCount: 9
Pid: 36484 LoopCount: 9Pid: 35300 LoopCount: 9
Pid: 30504 LoopCount: 9
Pid: 39400 LoopCount: 9

Pid: 39400 LoopCount: 10Pid: 35192 LoopCount: 10Pid: 30504 LoopCount: 10Pid: 35300 LoopCount: 10



Pid: 35300 LoopCount: 11
Pid: 35192 LoopCount: 11Pid: 30504 LoopCount: 11

Pid: 30504 LoopCount: 12Pid: 35192 LoopCount: 12

Pid: 30504 LoopCount: 13
```

我们看结果当中出现的内容会有很多同行问题，现在我们加锁，把注释取消。

```
Pid: 27840 LoopCount: 0
Pid: 30592 LoopCount: 0
Pid: 30952 LoopCount: 0
Pid: 15508 LoopCount: 0
Pid: 40872 LoopCount: 0
Pid: 27840 LoopCount: 1
Pid: 40872 LoopCount: 1
Pid: 30952 LoopCount: 1
Pid: 15508 LoopCount: 1
Pid: 30592 LoopCount: 1
Pid: 27840 LoopCount: 2
Pid: 15508 LoopCount: 2
Pid: 30592 LoopCount: 2
Pid: 30952 LoopCount: 2
Pid: 40872 LoopCount: 2
Pid: 27840 LoopCount: 3
Pid: 30952 LoopCount: 3
Pid: 15508 LoopCount: 3
Pid: 30592 LoopCount: 3
Pid: 40872 LoopCount: 3
Pid: 27840 LoopCount: 4
Pid: 40872 LoopCount: 4
Pid: 30592 LoopCount: 4
Pid: 15508 LoopCount: 4
Pid: 30952 LoopCount: 4
Pid: 27840 LoopCount: 5
Pid: 15508 LoopCount: 5
Pid: 30592 LoopCount: 5
Pid: 30952 LoopCount: 5
Pid: 40872 LoopCount: 5
Pid: 27840 LoopCount: 6
Pid: 40872 LoopCount: 6
Pid: 30592 LoopCount: 6
Pid: 30952 LoopCount: 6
Pid: 15508 LoopCount: 6
Pid: 27840 LoopCount: 7
Pid: 30952 LoopCount: 7
Pid: 15508 LoopCount: 7
Pid: 30592 LoopCount: 7
Pid: 40872 LoopCount: 7
Pid: 27840 LoopCount: 8
Pid: 40872 LoopCount: 8
Pid: 30592 LoopCount: 8
Pid: 30952 LoopCount: 8
Pid: 15508 LoopCount: 8
Pid: 27840 LoopCount: 9
Pid: 30952 LoopCount: 9
Pid: 40872 LoopCount: 9
Pid: 30592 LoopCount: 9
Pid: 15508 LoopCount: 9
Pid: 15508 LoopCount: 10
Pid: 30592 LoopCount: 10
Pid: 30952 LoopCount: 10
Pid: 40872 LoopCount: 10
Pid: 30952 LoopCount: 11
Pid: 40872 LoopCount: 11
Pid: 15508 LoopCount: 11
Pid: 15508 LoopCount: 12
Pid: 40872 LoopCount: 12
Pid: 15508 LoopCount: 13
```

现在看见输出结果就正常了，所以**进程互斥锁可以使得只有一个进程能在同一时刻访问共享资源**。

##### 信号量

信号量是进程同步过程中一个比较重要的角色

可以**控制临界资源的数量**

实现多个进程同时访问共享资源，限制进程的并发量

**可以用multiprocessing库中的Semaphore来实现信号量**。

经典的生产者-消费者问题

```python
# Author: Rainsblue.chan
# Create: 2023/6/28
# FileName: 06继承
from multiprocessing import Process, Semaphore, Lock, Queue
import time

buffer = Queue(10)
empty = Semaphore(2)
full = Semaphore(0)
lock = Lock()

class Consumer(Process):
    def run(self):
        global buffer, empty, full, lock
        while True:
            full.acquire()
            lock.acquire()
            buffer.get()
            print('Consumer pop an element')
            time.sleep(1)
            lock.release()
            empty.release()

class Producer(Process):
    def run(self):
        global buffer, empty, full, lock
        while True:
            empty.acquire() # 缓冲区空闲区大小减一，占有位置
            lock.acquire()  # 加锁
            buffer.put(1)
            print('Producer append an element')
            time.sleep(1)
            lock.release() # 释放锁
            full.release() # 缓冲区占用区大小加一

if __name__ == '__main__':
    p = Producer()
    c = Consumer()
    p.daemon = c.daemon = True
    p.start()
    c.start()
    p.join()
    c.join()
    print('Main Process Ended')
```

定义了一个生产者一个消费者，buffer中利用**multiprocessing.Queue()**定义了一个10的队列，然后定义了两个信号量**Semaphore**，并代表缓冲区的空余数（empty）和缓冲区的占有数（full），

生产者Producer使用**acquire方法**，来占有一个缓冲区位置，然后缓冲区空闲区大小减1，接下来进行加锁。然后释放锁，

代表占有的缓冲区位置加一。这就是很简单，**要工作，空闲区少去一个人，要休息，工作区位置空出来一个人**。消费者则相反。

Producer不断放入物品，Consumer不断地取出，这就是一个循环。

##### 队列（Queue）

让进程共享数据，这里的队列指的是multiprocessing里面的Queue。

##### 管道（Pipe）

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102213795.png" alt="image-20230629113049737" style="zoom:50%;" />

##### 进程池

假如有10000个任务，每个任务需要启动一个进程来执行，并且一个进程运行完毕之后要紧接着启动下一个进程，同时还需要控制进程的并发数量，不能并发太高。不然CPU处理不过来（如果同时运行的进程能维持在一个最高恒定值当然利用率是最高的）

进程池即multiprocessing中的Pool，可以提供指定数量的进程供用户调用，当有新的请求提交到pool中时，如果池还没有满，就会创建一个新的进程用来执行该请求，如果池中进程数已达到规定最大值，那么该请求就会等待直到池中有进程结束，才会创建新的进程来执行它。

```python
# Author: Rainsblue.chan
# Create: 2023/6/28
# FileName: 06继承
from multiprocessing import Pool
import time

def function(index):
    print(f'Start process: {index}')
    time.sleep(3)
    print(f'End process {index}',)

if __name__ == '__main__':
    pool = Pool(processes=3)
    for i in range(4):
        pool.apply_async(function, args=(i,))
    print('Main Proess started')
    pool.close()
    pool.join()
    print('Main Process Ended')
```

这里我们声明了一个大小为3的进程值， 通过**processes参数来指定**，如果不指定，它会根据内核数来分配。接着我们使用**apply_async方法**添加进程。apply_async是multiprocessing模块中的一个方法，用于在进程池中异步地执行一个函数或者方法。它的主要作用是将函数的执行和结果的获取分离开来，从而提高程序的执行效率。

结果如下：

```Python
Main Proess started
Start process: 0
Start process: 1
Start process: 2
End process 2End process 1

Start process: 3
End process 0
End process 3
Main Process Ended
```

进程值大小为3，所以可以看到3个进程运行，第4个值在等待。在有进程运行完毕之后，第四个进程马上跟着运行，出现了如上的运行结果。最后**用close方法关闭进程池**，使其不再接受新的任务。再调用**join方法**使得主进程等待子进程运行完毕之后，再退出。

###### 更好用的map方法

map方法的使用，第一个参数就是要启动的进程对应的执行方法，第二个参数是一个可迭代对象，其中的每个元素会被传递给这个执行方法。

**举个例子**，比如我们现在有个list，里面有很多url，另外我们也定义了很多方法，用来抓取每个url的内容解析，那么我们可以在map的第一个参数传入这个方法名，第二个参数传入这个url数组。现在我们看个实例。

```python
# Author: Rainsblue.chan
# Create: 2023/6/29
# FileName: 06Pool&map
from multiprocessing import Pool
import urllib.request
import urllib.error

def scrape(url): # 爬取
    try:
        urllib.request.urlopen(url)
        print(f'URL {url} Scraped')
    except (urllib.error.HTTPError, urllib.error.URLError):
        print(f'URL {url} not Scraped')

if __name__ == '__main__':
    pool = Pool(processes=3)
    urls = [
        'https://www.baidu.com',
        'http://www.meituan.com/',
        'http://blog.csdn.net/',
        'http://xxxyxxx.net',
    ]
    pool.map(scrape, urls)
    pool.close()
```

![image-20230629133343286](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210581.png)

这种方式非常适用于多进程的执行。下期就会开始正式开搞。

## 模块二	爬虫基本库的使用

### 07.入门首选，Requests库的基本运用

#### 前言

学习爬虫，最基础的便是模拟浏览器向服务器发送请求

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210138.png" alt="image-20230629135535716" style="zoom:50%;" />

Python的强大之处就在于提供功能丰富的类库，来帮助我们完成这些请求。

利用Python现有的库可以非常方便地实现网络请求的模拟，常见的库有**urllib、requests**等。

拿requests举例，有了它，我们只需关心请求的链接是什么，需要传的参数是什么，以及如何设置cookie的参数就好，不用深入到底层通信。

#### 实例引入

用python写爬虫的第一步就是模拟发起一个请求，把网页的源代码获取下来，类似于get类的http请求。浏览器得到源代码后，把它渲染出来就可以看见网页的内容。requests就有一个get方法。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
r = requests.get('http://httpbin.org/')
print(r.text) # 这边如果只是输入r，放回的是一个响应码，比如200
```

返回如下，是一段html源代码。

```html
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>httpbin.org</title>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Source+Code+Pro:300,600|Titillium+Web:400,600,700"
        rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="/flasgger_static/swagger-ui.css">
    <link rel="icon" type="image/png" href="/static/favicon.ico" sizes="64x64 32x32 16x16" />
    <style>
        html {
            box-sizing: border-box;
            overflow: -moz-scrollbars-vertical;
            overflow-y: scroll;
        }

        *,
        *:before,
        *:after {
            box-sizing: inherit;
        }

        body {
            margin: 0;
            background: #fafafa;
        }
    </style>
</head>

<body>
    <a href="https://github.com/requests/httpbin" class="github-corner" aria-label="View source on Github">
        <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;"
            aria-hidden="true">
            <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
            <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
                fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
            <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
                fill="currentColor" class="octo-body"></path>
        </svg>
    </a>
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="position:absolute;width:0;height:0">
        <defs>
            <symbol viewBox="0 0 20 20" id="unlocked">
                <path d="M15.8 8H14V5.6C14 2.703 12.665 1 10 1 7.334 1 6 2.703 6 5.6V6h2v-.801C8 3.754 8.797 3 10 3c1.203 0 2 .754 2 2.199V8H4c-.553 0-1 .646-1 1.199V17c0 .549.428 1.139.951 1.307l1.197.387C5.672 18.861 6.55 19 7.1 19h5.8c.549 0 1.428-.139 1.951-.307l1.196-.387c.524-.167.953-.757.953-1.306V9.199C17 8.646 16.352 8 15.8 8z"></path>
            </symbol>

            <symbol viewBox="0 0 20 20" id="locked">
                <path d="M15.8 8H14V5.6C14 2.703 12.665 1 10 1 7.334 1 6 2.703 6 5.6V8H4c-.553 0-1 .646-1 1.199V17c0 .549.428 1.139.951 1.307l1.197.387C5.672 18.861 6.55 19 7.1 19h5.8c.549 0 1.428-.139 1.951-.307l1.196-.387c.524-.167.953-.757.953-1.306V9.199C17 8.646 16.352 8 15.8 8zM12 8H8V5.199C8 3.754 8.797 3 10 3c1.203 0 2 .754 2 2.199V8z"
                />
            </symbol>

            <symbol viewBox="0 0 20 20" id="close">
                <path d="M14.348 14.849c-.469.469-1.229.469-1.697 0L10 11.819l-2.651 3.029c-.469.469-1.229.469-1.697 0-.469-.469-.469-1.229 0-1.697l2.758-3.15-2.759-3.152c-.469-.469-.469-1.228 0-1.697.469-.469 1.228-.469 1.697 0L10 8.183l2.651-3.031c.469-.469 1.228-.469 1.697 0 .469.469.469 1.229 0 1.697l-2.758 3.152 2.758 3.15c.469.469.469 1.229 0 1.698z"
                />
            </symbol>

            <symbol viewBox="0 0 20 20" id="large-arrow">
                <path d="M13.25 10L6.109 2.58c-.268-.27-.268-.707 0-.979.268-.27.701-.27.969 0l7.83 7.908c.268.271.268.709 0 .979l-7.83 7.908c-.268.271-.701.27-.969 0-.268-.269-.268-.707 0-.979L13.25 10z"
                />
            </symbol>

            <symbol viewBox="0 0 20 20" id="large-arrow-down">
                <path d="M17.418 6.109c.272-.268.709-.268.979 0s.271.701 0 .969l-7.908 7.83c-.27.268-.707.268-.979 0l-7.908-7.83c-.27-.268-.27-.701 0-.969.271-.268.709-.268.979 0L10 13.25l7.418-7.141z"
                />
            </symbol>


            <symbol viewBox="0 0 24 24" id="jump-to">
                <path d="M19 7v4H5.83l3.58-3.59L8 6l-6 6 6 6 1.41-1.41L5.83 13H21V7z" />
            </symbol>

            <symbol viewBox="0 0 24 24" id="expand">
                <path d="M10 18h4v-2h-4v2zM3 6v2h18V6H3zm3 7h12v-2H6v2z" />
            </symbol>

        </defs>
    </svg>


    <div id="swagger-ui">
        <div data-reactroot="" class="swagger-ui">
            <div>
                <div class="information-container wrapper">
                    <section class="block col-12">
                        <div class="info">
                            <hgroup class="main">
                                <h2 class="title">httpbin.org
                                    <small>
                                        <pre class="version">0.9.2</pre>
                                    </small>
                                </h2>
                                <pre class="base-url">[ Base URL: httpbin.org/ ]</pre>
                            </hgroup>
                            <div class="description">
                                <div class="markdown">
                                    <p>A simple HTTP Request &amp; Response Service.
                                        <br>
                                        <br>
                                        <b>Run locally: </b>
                                        <code>$ docker run -p 80:80 kennethreitz/httpbin</code>
                                    </p>
                                </div>
                            </div>
                            <div>
                                <div>
                                    <a href="https://kennethreitz.org" target="_blank">the developer - Website</a>
                                </div>
                                <a href="mailto:me@kennethreitz.org">Send email to the developer</a>
                            </div>
                        </div>
                        <!-- ADDS THE LOADER SPINNER -->
                        <div class="loading-container">
                            <div class="loading"></div>
                        </div>

                    </section>
                </div>
            </div>
        </div>
    </div>


    <div class='swagger-ui'>
        <div class="wrapper">
            <section class="clear">
                <span style="float: right;">
                    [Powered by
                    <a target="_blank" href="https://github.com/rochacbruno/flasgger">Flasgger</a>]
                    <br>
                </span>
            </section>
        </div>
    </div>



    <script src="/flasgger_static/swagger-ui-bundle.js"> </script>
    <script src="/flasgger_static/swagger-ui-standalone-preset.js"> </script>
    <script src='/flasgger_static/lib/jquery.min.js' type='text/javascript'></script>
    <script>

        window.onload = function () {
            

            fetch("/spec.json")
                .then(function (response) {
                    response.json()
                        .then(function (json) {
                            var current_protocol = window.location.protocol.slice(0, -1);
                            if (json.schemes[0] != current_protocol) {
                                // Switches scheme to the current in use
                                var other_protocol = json.schemes[0];
                                json.schemes[0] = current_protocol;
                                json.schemes[1] = other_protocol;

                            }
                            json.host = window.location.host;  // sets the current host

                            const ui = SwaggerUIBundle({
                                spec: json,
                                validatorUrl: null,
                                dom_id: '#swagger-ui',
                                deepLinking: true,
                                jsonEditor: true,
                                docExpansion: "none",
                                apisSorter: "alpha",
                                //operationsSorter: "alpha",
                                presets: [
                                    SwaggerUIBundle.presets.apis,
                                    // yay ES6 modules ↘
                                    Array.isArray(SwaggerUIStandalonePreset) ? SwaggerUIStandalonePreset : SwaggerUIStandalonePreset.default
                                ],
                                plugins: [
                                    SwaggerUIBundle.plugins.DownloadUrl
                                ],
            
            // layout: "StandaloneLayout"  // uncomment to enable the green top header
        })

        window.ui = ui

        // uncomment to rename the top brand if layout is enabled
        // $(".topbar-wrapper .link span").replaceWith("<span>httpbin</span>");
        })
    })
}
    </script>  <div class='swagger-ui'>
    <div class="wrapper">
        <section class="block col-12 block-desktop col-12-desktop">
            <div>

                <h2>Other Utilities</h2>

                <ul>
                    <li>
                        <a href="/forms/post">HTML form</a> that posts to /post /forms/post</li>
                </ul>

                <br />
                <br />
            </div>
        </section>
    </div>
</div>
</body>

</html>
```

我们只要把里面想要的内容爬取出来，就完成了。

#### GET请求

如果客户端发起的是GET请求的话，**该网站会判断并返回响应的请求信息**，包括Headers、IP等。

示例如下，我们在后缀加了get。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
r = requests.get('http://httpbin.org/get')
print(r.text)
```

运行结果如下，httpbin.org返回了**响应的请求信息**，包括Headers和IP。

```css
{
  "args": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", # 编码
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.31.0", # 用户请求头，这里就是我的requests库版本
    "X-Amzn-Trace-Id": "Root=1-649d2191-128870be64a80b4224d522ad"
  }, 
  "origin": "120.204.152.106", # IP信息
  "url": "http://httpbin.org/get"
}
```

如果想要添加两个参数，其中name是germey，age是25，URL就可以写出下列内容。

`http://httpbin.org/get?name=germey&age=25`

```css
{
  "args": {
    "age": "25", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.31.0", 
    "X-Amzn-Trace-Id": "Root=1-649d3f63-1872130e410d423b2aaff0b7"
  }, 
  "origin": "120.204.152.106", 
  "url": "http://httpbin.org/get?name=germey&age=25"
}
```

在这里，我们可以发现返回了两个参数，其中一个是年龄参数为25，另外一个名字参数为germey。但是手动拼接不太人性化，所以会用到**params参数**。

##### paras参数用法

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
data = {
    'name' : 'rainsbluechan', # 一定要注意别忘了逗号
    'age' : 20
}
r = requests.get('http://httpbin.org/get', params=data) # params引参
print(r.text)
```

这里通过字典的方式传递给params参数，不需要自己再构造url。

##### json格式

如果想直接解析返回结果，得到一个json格式的数据的话，可以直接调用json方法

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
r = requests.get('http://httpbin.org/get')
print(type(r.text))
print(r.json())
print(type(r.json()))
```

结果

```json
<class 'str'>
{'args': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.31.0', 'X-Amzn-Trace-Id': 'Root=1-649d4222-2631438a7eca1d83079c0356'}, 'origin': '120.204.152.106', 'url': 'http://httpbin.org/get'}
<class 'dict'>
```

调用json方法解析可以使一段字符串转换成一个字典。但如果不是json格式，则会抛出jsondecodeError等异常。

##### 抓取网页

由于课程实例已经过期，所以我打算自己找一个。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
import re

headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
r = requests.get('https://movie.douban.com/top250', headers=headers) # 伪装本机浏览器标识头 
pattern = re.compile('<span class="title">(.*?)</span>', re.S)
titles = re.findall(pattern, r.text)
print(r.text)
```

首先一个问题，就是如果你不伪装一个电脑的请求头，对于有反爬机制的网站会返给你一个418，那是个咖啡壶冷笑话，I’m teapot。我们可以看一下，这是经典的豆瓣250，俗称爬虫靶场。

![image-20230629170359741](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210133.png)

所以我们需要做的事情就是去伪装一个浏览器标头，位置在这里。

![image-20230629170614534](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210765.png)

放进去之后，我们用正则表达式进行处理，对第一页的题目进行一个过滤，这里不太追求细节，所以直接看结果。

```dict
['肖申克的救赎', '&nbsp;/&nbsp;The Shawshank Redemption', '霸王别姬', '阿甘正传', '&nbsp;/&nbsp;Forrest Gump', '泰坦尼克号', '&nbsp;/&nbsp;Titanic', '这个杀手不太冷', '&nbsp;/&nbsp;Léon', '千与千寻', '&nbsp;/&nbsp;千と千尋の神隠し', '美丽人生', '&nbsp;/&nbsp;La vita è bella', '辛德勒的名单', '&nbsp;/&nbsp;Schindler&#39;s List', '星际穿越', '&nbsp;/&nbsp;Interstellar', '盗梦空间', '&nbsp;/&nbsp;Inception', '楚门的世界', '&nbsp;/&nbsp;The Truman Show', '忠犬八公的故事', '&nbsp;/&nbsp;Hachi: A Dog&#39;s Tale', '海上钢琴师', '&nbsp;/&nbsp;La leggenda del pianista sull&#39;oceano', '三傻大闹宝莱坞', '&nbsp;/&nbsp;3 Idiots', '放牛班的春天', '&nbsp;/&nbsp;Les choristes', '机器人总动员', '&nbsp;/&nbsp;WALL·E', '无间道', '&nbsp;/&nbsp;無間道', '疯狂动物城', '&nbsp;/&nbsp;Zootopia', '控方证人', '&nbsp;/&nbsp;Witness for the Prosecution', '大话西游之大圣娶亲', '&nbsp;/&nbsp;西遊記大結局之仙履奇緣', '熔炉', '&nbsp;/&nbsp;도가니', '教父', '&nbsp;/&nbsp;The Godfather', '触不可及', '&nbsp;/&nbsp;Intouchables', '当幸福来敲门', '&nbsp;/&nbsp;The Pursuit of Happyness', '怦然心动', '&nbsp;/&nbsp;Flipped']
```

就是一个字典。我们用**re.compile**来匹配所有的标题，然后用**re.findall**来引入做好的pattern参数去处理r.text，最后呈现的就是上面的字典。关于正则表达式，下一讲会有非常详细的介绍。

上面是属于文字类，如果想要抓取图片，音频等二进制文件，应该怎么办呢？

##### 抓取二进制文件

###### r.content

以抓movie.douban.com站点图片为例

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
import re

headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
r = requests.get('https://movie.douban.com/favicon.ico',headers=headers)
print(r.text)
print(r.content)
```

返回结果如下

```binary
          

                                                                                               (       @                         w Jw çw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw äw Jw æw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw çw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿ‘À‰ÿÿÿÿÿÿÿÿÿøûøÿ|
ÿw ÿw ÿw ÿw ÿ|
ÿøûøÿÿÿÿÿÿÿÿÿ‘À‰ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÕçÒÿÿÿÿÿÿÿÿÿ¾Ú¹ÿw ÿw ÿw ÿw ÿw ÿw ÿ¾Ú¹ÿÿÿÿÿÿÿÿÿÕçÒÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿ+†ÿÿÿÿÿÿÿÿÿÿÿÿÿz³pÿw ÿw ÿw ÿw ÿw ÿw ÿz³pÿÿÿÿÿÿÿÿÿÿÿÿÿ+†ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿn¬dÿÿÿÿÿÿÿÿÿÿÿÿÿ5Œ'ÿw ÿw ÿw ÿw ÿw ÿw ÿ5Œ'ÿÿÿÿÿÿÿÿÿÿÿÿÿn¬dÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿ½Ù¸ÿÿÿÿÿÿÿÿÿäðâÿ/ˆ ÿ/ˆ ÿ/ˆ ÿ/ˆ ÿ/ˆ ÿ/ˆ ÿ/ˆ ÿ/ˆ ÿäðâÿÿÿÿÿÿÿÿÿ½Ù¸ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿÛêØÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿÛêØÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿµÕ°ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿµÕ°ÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿµÕ°ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿµÕ°ÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿµÕ°ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿµÕ°ÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿµÕ°ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿµÕ°ÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿÛêØÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿˆ»€ÿÛêØÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿÓæÐÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÓæÐÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿâîàÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿâîàÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿâîàÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿâîàÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿâîàÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿâîàÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw æw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw æw Iw æw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw ÿw æw I                                                                                                                                
b'\x00\x00\x01\x00\x02\x00\x10\x10\x00\x00\x01\x00\x08\x00h\x05\x00\x00&\x00\x00\x00  \x00\x00\x01\x00 \x00\xa8\x10\x00\x00\x8e\x05\x00\x00(\x00\x00\x00\x10\x00\x00\x00 \x00\x00\x00\x01\x00\x08\x00\x00\x00\x00\x00\x00\x01\x00\x00\x13\x0b\x00\x00\x13\x0b\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x11w\x00\x00\x1at\x0f\x00\x1av\x10\x00/\x84"\x00:\x88.\x00<\x8c/\x00C\x8d6\x00E\x918\x00G\x90;\x00S\x99G\x00S\x9aH\x00I\x97I\x00K\x99I\x00P\x9aI\x00P\x9bI\x00T\x9eN\x00b\xa3W\x00r\xa9h\x00\x86\xb6~\x00\x93\xbf\x8b\x00\x9a\xc3\x93\x00\xac\xcc\xa6\x00\xd7\xe8\xd1\x00\xde\xec\xdc\x00\xe0\xed\xde\x00\xe7\xf2\xe3\x00\xf7\xfa\xf5\x00\xfc\xfd\xfc\x00\xfd\xfe\xfc\x00\xfe\xfe\xfe\x00\xff\xff\xfe\x00\xff\xff\xff\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x07\x00\x10\x15\x15\x15\x15\x15\x15\x15\x15\x15\x15\x15\x15\x10\x00\x00\x14\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x14\x00\x00\x00\x00\x00\x13\x1f\x11\x01\x01\x11\x1f\x13\x00\x00\x02\x02\x00\x00\x02\x02\x18\x1b\x01\x01\x01\x01\x1f\x18\x00\x02\x02\x02\x00\x00\x02\t\x1f\x1c\x04\x06\x06\x04\x1f\x1a\t\x02\x00\x02\x00\x00\x08\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x08\x00\x02\x00\x00\x08\x1f\x17\x05\x05\x05\x05\x05\x05\x17\x1f\x08\x00\x02\x00\x00\x08\x1f\x19\x01\x01\x01\x01\x01\x01\x16\x1f\x08\x00\x02\x00\x00\x08\x1f\x17\x0f\x0b\x0e\r\x0c\x0b\x17\x1f\x08\x00\x02\x00\x00\x08\x1f\x1d\x1f\x1e\x1e\x1e\x1e\x1e\x1f\x1f\x08\x00\x00\x00\x00\x02\x02\x02\x02\x02\x02\x02\x02\x02\x02\x02\x02\x02\x00\x00\x03\x12\x12\x12\x12\x12\x12\x12\x12\x12\x12\x12\x12\x03\x00\x00\n\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\x1f\n\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00(\x00\x00\x00 \x00\x00\x00@\x00\x00\x00\x01\x00 \x00\x00\x00\x00\x00\x00\x10\x00\x00\x13\x0b\x00\x00\x13\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x11w\x00J\x11w\x00\xe7\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xe4\x11w\x00J\x11w\x00\xe6\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xe7\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x91\xc0\x89\xff\xff\xff\xff\xff\xff\xff\xff\xff\xf8\xfb\xf8\xff\x1a|\n\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x1a|\n\xff\xf8\xfb\xf8\xff\xff\xff\xff\xff\xff\xff\xff\xff\x91\xc0\x89\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd5\xe7\xd2\xff\xff\xff\xff\xff\xff\xff\xff\xff\xbe\xda\xb9\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xbe\xda\xb9\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd5\xe7\xd2\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff+\x86\x1c\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xffz\xb3p\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xffz\xb3p\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff+\x86\x1c\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xffn\xacd\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff5\x8c\'\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff5\x8c\'\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xffn\xacd\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xbd\xd9\xb8\xff\xff\xff\xff\xff\xff\xff\xff\xff\xe4\xf0\xe2\xff/\x88 \xff/\x88 \xff/\x88 \xff/\x88 \xff/\x88 \xff/\x88 \xff/\x88 \xff/\x88 \xff\xe4\xf0\xe2\xff\xff\xff\xff\xff\xff\xff\xff\xff\xbd\xd9\xb8\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xdb\xea\xd8\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\xdb\xea\xd8\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xb5\xd5\xb0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xb5\xd5\xb0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xb5\xd5\xb0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xb5\xd5\xb0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xb5\xd5\xb0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xb5\xd5\xb0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xb5\xd5\xb0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xb5\xd5\xb0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xdb\xea\xd8\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\x88\xbb\x80\xff\xdb\xea\xd8\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xd3\xe6\xd0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xd3\xe6\xd0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xe2\xee\xe0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xe2\xee\xe0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xe2\xee\xe0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xe2\xee\xe0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\xe2\xee\xe0\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xe2\xee\xe0\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xe6\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xe6\x11w\x00I\x11w\x00\xe6\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xff\x11w\x00\xe6\x11w\x00I\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
```

这里打印了response的两个属性，一个是**text**，一个是**content**。

###### open方法

前者出现乱码，后者带有一个**b**，这代表是**bytes类型**的数据，由于图片是二进制数据，所以前者在打印时转化为str类型，也就是图片直接转化为字符串，所以会出现乱码。我们现在尝试把它保存下来。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
import re

headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
r = requests.get('https://movie.douban.com/favicon.ico',headers=headers)
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```

成功地保存了。这里我们调用了**open方法**，第一个参数是文件名称，第二个参数，我们以**二进制**的形式保存，这里使用到的是**'wb'（writable binary，二进制写入）**，可以向文件写入二进制数据。运行结束后就出现下面的图片。音频、视频类也可以同样根据这个方式进行获取。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210348.png" alt="image-20230629173207567" style="zoom: 33%;" />

###### 请求头设置

在发起一个HTTP请求时，会有一个请求头**Request Headers**，我们看看如何设置。

**使用headers参数**。上面写了一个，不再赘述。它的作用就是伪装成别的，不被网站识别成反爬。

#### Post请求

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests

headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
data = {
    'name' : 'germey',
    'age' : 25
}
r = requests.post('http://httpbin.org/post', headers=headers, data=data)
print(r.text)
```

成功返回结果

```json
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "age": "25", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "18", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36", 
    "X-Amzn-Trace-Id": "Root=1-649d8616-5060659b305e96a25453095b"
  }, 
  "json": null, 
  "origin": "42.3.79.113", 
  "url": "http://httpbin.org/post"
}
```

其中**form就是我们提交的表单数据**，这就说明我们成功发送了。

#### 响应（Response）

发送请求后，自然就会得到响应。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests

headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
r = requests.get('https://movie.douban.com/top250',headers=headers)
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```

返回的响应如下

```css
<class 'int'> 200 # 状态码
<class 'requests.structures.CaseInsensitiveDict'> {'Date': 'Thu, 29 Jun 2023 13:30:17 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Keep-Alive': 'timeout=30', 'X-Xss-Protection': '1; mode=block', 'X-Douban-Mobileapp': '0', 'Expires': 'Sun, 1 Jan 2006 01:00:00 GMT', 'Pragma': 'no-cache', 'Cache-Control': 'must-revalidate, no-cache, private', 'Set-Cookie': 'ck="deleted"; max-age=0; domain=.douban.com; expires=Thu, 01-Jan-1970 00:00:00 GMT; path=/, dbcl2="deleted"; max-age=0; domain=.douban.com; expires=Thu, 01-Jan-1970 00:00:00 GMT; path=/, bid=6P0GEKRQnfk; Expires=Fri, 28-Jun-24 13:30:17 GMT; Domain=.douban.com; Path=/', 'X-DAE-App': 'movie', 'X-DAE-Instance': 'default', 'X-DOUBAN-NEWBID': '6P0GEKRQnfk', 'Server': 'dae', 'Strict-Transport-Security': 'max-age=15552000', 'X-Content-Type-Options': 'nosniff', 'Content-Encoding': 'gzip'} # 头部字典
<class 'requests.cookies.RequestsCookieJar'> <RequestsCookieJar[<Cookie bid=6P0GEKRQnfk for .douban.com/>]> # cookies
<class 'str'> https://movie.douban.com/top250 # url
<class 'list'> []
```

requests还提供了一个内置的状态码查询对象**requests.codes**

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests

headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
r = requests.get('https://movie.douban.com/top250',headers=headers)
exit() if not r.status_code == requests.codes.ok else print('Request Successfully')
```

返回结果如下，这说明没问题，哈哈。

`Request Successfully`

##### 状态码

```json
# 信息性状态码
100: ('continue',),
101: ('switching_protocols',),
102: ('processing',),
103: ('checkpoint',),
122: ('uri_too_long', 'request_uri_too_long'),

# 成功状态码
200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✔'),
201: ('created',),
202: ('accepted',),
203: ('non_authoritative_info', 'non_authoritative_information'),
204: ('no_content',),
205: ('reset_content', 'reset'),
206: ('partial_content', 'partial'),
207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),
208: ('already_reported',),
226: ('im_used',),

# 重定向状态码
300: ('multiple_choices',),
301: ('moved_permanently', 'moved', '\\o-'),
302: ('found',),
303: ('see_other', 'other'),
304: ('not_modified',),
305: ('use_proxy',),
306: ('switch_proxy',),
307: ('temporary_redirect', 'temporary_moved', 'temporary'),
308: ('permanent_redirect',
      'resume_incomplete', 'resume',), # These 2 to removed in 3.0

# 客户端错误状态码
400: ('bad_request', 'bad'),
401: ('unauthorized',),
402: ('payment_required', 'payment'),
403: ('forbidden',),
404: ('not_found', '-o-'),
405: ('method_not_allowed', 'not_allowed'),
406: ('not_acceptable',),
407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),
408: ('request_timeout', 'timeout'),
409: ('conflict',),
410: ('gone',),
411: ('length_required',),
412: ('precondition_failed', 'precondition'),
413: ('request_entity_too_large',),
414: ('request_uri_too_large',),
415: ('unsupported_media_type', 'unsupported_media', 'media_type'),
416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),
417: ('expectation_failed',),
418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'), # 1998茶壶冷笑话，反爬虫机制
421: ('misdirected_request',),
422: ('unprocessable_entity', 'unprocessable'),
423: ('locked',),
424: ('failed_dependency', 'dependency'),
425: ('unordered_collection', 'unordered'),
426: ('upgrade_required', 'upgrade'),
428: ('precondition_required', 'precondition'),
429: ('too_many_requests', 'too_many'),
431: ('header_fields_too_large', 'fields_too_large'),
444: ('no_response', 'none'),
449: ('retry_with', 'retry'),
450: ('blocked_by_windows_parental_controls', 'parental_controls'),
451: ('unavailable_for_reasons', 'legal_reasons'),
499: ('client_closed_request',),

# 服务端错误状态码
500: ('internal_server_error', 'server_error', '/o\\', 'X'),
501: ('not_implemented',),
502: ('bad_gateway',),
503: ('service_unvailable', 'unvailable'),
504: ('gateway_timeout',),
505: ('http_version_not_supported', 'http_version'),
506: ('variant_also_negotiates',),
507: ('insufficient_storage',),
509: ('bandwidth_limit_exceeded', 'bandwidth'),
510: ('not_extended',),
511: ('network_authentication_required', 'network_auth', 'network_authentication')
```

#### 高级用法（文件上传，cookies设置，代理设置...etc）

##### 文件上传 

我们知道request可以模拟提交一些数据，例如有的网站它需要上传一些文件，我们也可以用它来实现。在上面我们抓取了豆瓣的ico，那么我们现在来模拟上传的过程。

```python
# Author: Rainsblue.chan
# Create: 2023/6/30
# FileName: requests.post
import requests

headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
files = {'file' : open('favicon.ico', 'rb')}
r = requests.post('http://httpbin.org/post',headers=headers, files=files)
print(r.text)
```

结果如下

```json
{
  "args": {}, 
  "data": "", 
  "files": {
    "file": "data:application/octet-stream;base64,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"
  }, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "5833", 
    "Content-Type": "multipart/form-data; boundary=bf2280ed84a32fc7579faaebc1a503ba", 
    "Host": "httpbin.org", 
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36", 
    "X-Amzn-Trace-Id": "Root=1-649e4cb4-01ffeef01bfed6f802bc2102"
  }, 
  "json": null, 
  "origin": "120.204.152.210", 
  "url": "http://httpbin.org/post"
}
```

里面包含files这个字段，而form字段是空的，证明文件上传部分会单独有一个files字段来标识。

##### Cookies

还是看一个示例，这个只要一个**r.cookies**就能反回来。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests

headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

r = requests.get('http://www.baidu.com', headers=headers)
print(r.cookies)
for key, value in r.cookies.items():
    print(key + '=' + value)
```

我们看结果

```json
<RequestsCookieJar[<Cookie H_PS_PSSID=36551_38942_38857_38798_38959_38831_38915_38973_38805_38986_38638_38867_26350 for .baidu.com/>, <Cookie BDSVRTM=38 for www.baidu.com/>, <Cookie BD_HOME=1 for www.baidu.com/>]>
H_PS_PSSID=36551_38942_38857_38798_38959_38831_38915_38973_38805_38986_38638_38867_26350
BDSVRTM=38
BD_HOME=1
```

这里我们首先调用cookies这个属性就可以成功得到它了。可以发现它是**<RequestCookieJar>**这样的类型。然后我们用items方法就可以把它转换成元组组成的列表，遍历输出每一个cookie，可以看到下面的名称和值。接下来是网站示例，首先我们先登录自己的github。

![image-20230630114554203](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210508.png)

将这段cookie复制下来，放入自己的cookie

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests

# 建议挂个代理，我是挂代理才有的回显
# proxies={http:xxxx}类似，然后r里加参数
headers={
    'Cookie': 'your-cookie',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

r = requests.get('https://www.github.com/', headers=headers)
print(r.text)
```

返回特别长，我们就看当中一些具体信息即可。只要包含自己的github用户名即可。

##### RequestsCookieJar（个人感觉比上面好，独立处理）

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests

# 设置代理
proxies = {
    "http": "http://127.0.0.1:4780",
    "https": "http://127.0.0.1:4780",
}

# RequestCookieJar
cookies =  'your-cookie'
jar = requests.cookies.RequestsCookieJar()

# 请求头
headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
for cookie in cookies.split(';'):
    key, value = cookie.split('=', 1)
    jar.set(key, value)
r = requests.get('https://www.github.com/', headers=headers, proxies=proxies, cookies=jar)
print(r.text)
```

我们这里新建了一个**RequestsCookieJar对象**，然后将复制下来的cookie用split方法进行分割，**（使用分号，因为截下来有分号）**，这里key和value只在第一个 = 分割一次，随后用set方法生成好每个cookie的key和value。

![image-20230630162034468](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102210166.png)

有上述回返则正确。

##### Session维持

在requests中，如果直接利用get或post等方法的确可以做到模拟网页的请求，但是**这实际是相当于不同的Session**，相当于你用两个浏览器打开了不同的界面。

设想这样一个场景，我们先用post方法打开了一个网站，第二次，我们想要获取登录成功后的个人信息，我们可能采用一次get方法得到个人信息的页面，这就是两个不相关的session，但是这样当然不能一次成功。

方式是在两次请求时设置一样的cookie，但是这样会比较繁琐，有什么方式可以解决它呢？

**维持同样的session，也就是打开新的浏览器选项卡，而不是新开一个浏览器。**又不想要每次都设置cookies，这时候就有新方法，**session对象**。利用session对象，我们可以维护一个session，而且不用担心cookies的问题，它会自动处理好。示例如下。先是一个不成功的。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests

# 设置代理
proxies = {
    "http": "http://127.0.0.1:4780",
    "https": "http://127.0.0.1:4780",
}

# 请求头
headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
requests.get('http://httpbin.org/cookies/set/number/123456789', headers=headers, proxies=proxies)
r = requests.get('http://httpbin.org/cookies')
print(r.text)
```

结果就是下面这个

```css
{
  "cookies": {}
}
```

下面用session对象的方法

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests

# 设置代理
proxies = {
    "http": "http://127.0.0.1:4780",
    "https": "http://127.0.0.1:4780",
}

# 请求头
headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789', headers=headers, proxies=proxies)
r = s.get('http://httpbin.org/cookies')
print(r.text)
```

那么就会有回显。

```css
{
  "cookies": {
    "number": "123456789"
  }
}
```

这样更能体会到**同一个session和不同session之间的区别**了。可以模拟同一个session，然后再进行下一步操作。

##### SSL证书验证

现在很多网站都要求使用HTTPS协议，但是有些网站可能并没有设置好HTTPS证书，或者不被CA机构认可（如**中铁**），这时这些**网站可能就会出现SSL证书错误的提示**。

我们可以通过一些方法爬取这些无证明的网站。可以使用**verify参数**控制是否验证证书，如果将其设置为False，在请求时就不会再验证证书是否有效，如果不加verify参数的话，默认值是True，会自动验证。我找一个这样的网站嗷。对不起找不到（懒得）

```python
import requests
from requests.packages import urllib3
# 方式1：设置忽略警告方式，屏蔽警告
urllib3.disable_warnings() 
# 方式2：可以通过捕获日志的方式来忽略它
import logging
logging.captureWarnings(True)
response = requests.get('https://某个没ca证书的url', verify=False) # 不检验有效
print(response.status_code) # 检验是否可通
# 方式3：可以指定一个本地证书用作客户端证书，可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组
response = requests.get('https://某个没ca证书的url', cert=('/path/server.crt', '/path/server.key'))
print(response.status_code)
```

##### 超时设置

本机网络状态不好或者服务器网络响应延迟甚至无响应时，会等待很久才能收到响应，甚至到最后收不到响应而报错，为防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应，就报错，这需要用到**timeout参数**。

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
# 请求头
headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
r = requests.get('http://httpbin.org/get', headers=headers, timeout=1)
print(r.status_code)
```

**如果1秒内没有响应的话，那么就会爆出异常**，这里基本是异常，因为太快了。

实际上，请求分为两个阶段，即**连接阶段**和**读取阶段**。这里用的timeout是两者时间的总和，如果要分别设置，可以传入一个元组类似于下面这一行。

`r = requests.get('https://httpbin.org/get', timeout=(5, 30))`

如果想**永久等待**，可以直接将timeout设置为**None**，或者不设置直接留空，因为默认是None。

##### 身份认证

访问某些设置了身份认证的网站时，网站由于启用了基本身份认证，用户登录需要输入用户名和密码作为登录凭证。

如果遇到这种形式，怎么用requests来爬取呢？

我们可以使用requests自带的身份认证功能，通过**auth参数**即可设置。

```python
import requests
from requests.auth import HTTPBasicAuth

r = requests.get('url', auth=HTTPBasicAuth('admin, 'admin')) # 用户和密码都是admin
print(r.status_code)
```

如果用户名和密码正确的话，那么会认证成功，返回200，失败，则返回401.

简写就是直接这一条

`r = requests.get('url', auth=('admin','admin))`

还提供了其他认证方式，如**OAuth认证**，不过此时需要安装oauth包。

##### 代理设置

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
import requests
# 代理
proxies={
    'http' : 'http://127.0.0.1:4780', # 一定要注意逗号
    'https' : 'http://127.0.0.1:4780',
}
# 请求头
headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
r = requests.get('http://bilibili.com', headers=headers, proxies=proxies)
print(r.status_code)
```

如果代理需要使用到身份认证，就按下列。

`proxies = {'https':'http://user:password@10.10.10.10:1080/',}`

requests还支持SOCKS协议的代理，需要安装socks库。

##### Prepared Request（独立对象）

不用get方法，直接构造一个Prepared Request对象

```python
# Author: Rainsblue.chan
# Create: 2023/6/27
# FileName: requests.get
from requests import Request, Session
# 代理
proxies={
    'http' : 'http://127.0.0.1:4780',
    'https' : 'http://127.0.0.1:4780',
}
# 请求头
headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
url = 'http://httpbin.org/post'
data = {'name':'germey'}
s = Session()
req = Request('POST', url, data=data, headers=headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
```

这样就可以灵活调度，一次好几个页面。

### 08.解析无所不能的正则表达式

正则表达式是为了从HTML中**获取真正想要的数据**。

它是处理字符串的有力工具，有自己独特的语法结构，有了就可以做到检索、匹配验证。

#### 实例引入

打开开源中国提供的正则表达式测试工具，输入待匹配的字符串，选择常用的正则表达式，就可以得到结果。

`http://tool.oschina.net/regex/`

输入下面这段待匹配的文本，我们可以看结果

```
Hello,my phone number is 010-86432100 and email is cqc@cuiqingcai.com,and my website is https://cuiqingcai.com
```

- 匹配email地址

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209009.png" alt="image-20230701210620082" style="zoom: 33%;" />

- 匹配URL

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209398.png" alt="image-20230701210739794" style="zoom:50%;" />

这里就是使用了正则表达式的**匹配功能**。也就是用一定的规则将特定的文本提取出来。

比如说电子邮件组成格式：一段字符串+@+某个域名

URL的组成格式：协议类型（http还是https）+  冒号加双斜线  +  域名和路径

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209907.png" alt="image-20230701211028837" style="zoom:50%;" />

URL的匹配如上图所示：`[a-zA-z]+://[^\s]*`

```css
a到z 可以匹配任意小写字母
反斜线s  \s 可以匹配任意空白字符
* 匹配前面多个任意字符
这一长串就是一个组合
```

 比如我们如果想得到一个HTML源码里有多少URL，那么用这个就行了。

##### 常用匹配规则

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209972.png" alt="image-20230701212313845" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209771.png" alt="image-20230701220155641" style="zoom:50%;" />

```
模式及描述
\w			匹配字母、数字及下划线
\W			匹配不是字母、数字及下划线的字符
\s(space)    匹配任意空白字符，等价于[\t\n\r\f]
\S           匹配任意非空字符
\d			匹配任意数字，等价于[0~9]
\D			匹配任意非数字的字符
\A			匹配字符串开头
\Z			匹配字符串结尾，如果存在换行，只匹配到换行前的结束字符串
\z			匹配字符串结尾，如果存在换行，同时还会匹配换行符
\G			匹配最后匹配完成的位置
\n			匹配一个换行符
\t			匹配一个制表符
^			匹配一行字符串的开头
$			匹配一行字符串的结尾
.			匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符
[...]        用来表示一组字符，单独列出，比如[amk]匹配a、m或k
[^...]       不在[]中的字符，比如匹配除了a、b、c之外的字符
* 			匹配0个或多个表达式
+			匹配一个或多个表达式，比如 \d+的意思就是匹配至少一个数字，并没有指定具体多少个数字
?			匹配0个或1个前面的正则表达式定义的片段，非贪婪方式
{n}			精确匹配n个前面的表达式
{n,m}		匹配n到m次由前面正则表达式定义的片段，贪婪方式
a|b			匹配a或b
()			匹配括号内的表达式，也表示一个组
etc....
```

python的re库提供了整个正则表达式的运用，我们来介绍一些常用方法。

##### re.match

向match传入要匹配的字符串，以及正则表达式，就可以检测这个正则表达式是否匹配字符串。

match方法会尝试**从字符串的起始位置匹配正则表达式**，如果匹配--**就返回匹配成功的结果**，如果不匹配--**就返回None**。

🥵🥵🥵**注意！注意！**这里一定要注意一个限制啊友友们，re.match**它只能从字符串的起始位置来匹配！**（也就是说你在字符串当中整一个^和$规则是没用的！）

示例如下

```Python
# Author: Rainsblue.chan
# Create: 2023/7/1
# FileName: match
import re

content = 'Hello 123 4567 World_This is a Regex Demo'
print(len(content))
result = re.match('^Hello\s\d\d\d\s\d{4}\s\w{10}', content)
print(result)
print(result.group())
print(result.span())
```

结果如下

```python
41
<re.Match object; span=(0, 25), match='Hello 123 4567 World_This'>
Hello 123 4567 World_This
(0, 25)
```

我们看看这里match做了什么。**re.match('^Hello\s\d{3}\s\d{4}\s\w{10}')**,^用于匹配一个字符串的开头，开头是Hello，然后选择匹配1个空格，匹配3个数字，匹配1个空格，匹配4个数字，匹配1个空格，再匹配10个字母、数字或下划线。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209865.png" alt="image-20230702001605189" style="zoom:50%;" />

在match方法中，**第一个参数传入正则表达式，第二个参数传入要匹配的字符串**。

输出结果是SRE_Match对象，证明成功匹配。这里**result即为SRE_Match对象。**

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209234.png" alt="image-20230702001859768" style="zoom:50%;" />

- result.group()方法输出匹配的内容
- result.span()方法可以输出匹配范围

我们用match匹配，如何提取他们呢？从一段文本中提取出邮件或电话号码等内容，可以使用**（）括号**将想要提取的子字符串括起来。

（）实际标记了一个子表达式的开始和结束位置，被标记的每个子表达式会依次对应每一个分组，调用group方法传入分组的索引即可获取提取的结果。

```python
# Author: Rainsblue.chan
# Create: 2023/7/2
# FileName: match1
import re

content = 'Hello 1234567 World_This is a Regex Demo'
result = re.match('^Hello\s(\d+)\sWorld', content)
print(result)
print(result.group())
print(result.group(1))
print(result.span())
```

结果如下

```python
<re.Match object; span=(0, 19), match='Hello 1234567 World'>
Hello 1234567 World
1234567
(0, 19)
```

我们这里将数字的部分括起来，然后用**group(1)**，**他会输出第一个用小括号括起来的内容。**诸如此类我可以group(2),group(3)，都行。

##### .*（万能匹配）

出现空白字符就写\s匹配，出现数字就用\d匹配，**工作量非常大**。

可以用**.***这个万能匹配来减少这些工作，**.**可以匹配任意字符（除换行符），*****代表匹配前面的字符无限次

`.*意为匹配任意字符无限次（除换行符）`

```python
import re

content = 'Hello 1234567 World_This is a Regex Demo'
result = re.match('^Hello.*Demo$', content)
print(result)
print(result.group())
print(result.span())
```

这里match的正则就是**以Hello开头，以Demo结尾，中间匹配任意字符**。运行结果如下。

```
<re.Match object; span=(0, 40), match='Hello 1234567 World_This is a Regex Demo'>
Hello 1234567 World_This is a Regex Demo
(0, 40)
```

但是.*有时候并不能让我们获取想要的结果，我们看下面的例子。

```python
import re

content = 'Hello 1234567 World_This is a Regex Demo'
result = re.match('^He.*(\d+).*Demo$', content)
print(result)
print(result.group(1))
```

我们需要获取数字，所以将它括起来，周围杂乱的使用.*来过滤。但是最后得到的如下。

```
<re.Match object; span=(0, 40), match='Hello 1234567 World_This is a Regex Demo'>
7
```

这里就涉及到一个**贪婪匹配和非贪婪匹配**的问题了。

##### 贪婪匹配和非贪婪匹配的运用

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209160.png" alt="image-20230702114549527" style="zoom:50%;" />

这里我们需要回忆常用匹配规则，+的意思就是匹配至少一个表达式，而\d+的意思就是匹配至少一个数字，并没有指定具体多少个数字，因此.*就尽可能匹配多的字符，**这里就把123456匹配了，而只给\d+留下一个可满足的数字7，所以最后得到的就只有数字7**。

也就是说，当非贪婪匹配遇上贪婪匹配，**会被抢占**。所以这会为我们带来不便，有的时候匹配莫名其妙就少了一部分内容。所以我们这里只要使用非贪婪匹配就可以了。

```python
import re

content = 'Hello 1234567 World_This is a Regex Demo'
result = re.match('^He.*?(\d+).*Demo$', content)
print(result)
print(result.group(1))
```

在这里，**?匹配0个或1个前面的正则表达式定义的片段，在这里是.*，用的是非贪婪方式**。

- 贪婪匹配就是尽可能匹配多的字符
- 非贪婪匹配就是匹配尽可能少的字符

当**.*?匹配到Hello后面的内容**时，再往后的内容就是数字了，而**反斜线\d+**恰好可以匹配数字，那么**.*?就不再进行匹配了**，而是交给反斜线d+匹配后面的数字。这样.*?就匹配了尽可能少的字符，反斜线d+就是1234567了。这就是两者配合的一个实现。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209382.png" alt="image-20230702164351816" style="zoom: 50%;" />

所以，在做匹配的时候，字符串中间尽量使用非贪婪匹配，也就是用**.*?**来代替**.***，以免出现匹配结果缺失的情况。

我们再看一个例子，如果匹配的结果出现在字符串结尾，那么可能就匹配不到任何内容，因为**.*?**会匹配尽可能少的内容。

```python
import re

content = 'http://weibo.com/comment/kEraCN'
result1 = re.match('http.*?comment/(.*?)', content)
result2 = re.match('http.*?comment/(.*)', content)
print('result1', result1.group(1))
print('result2', result2.group(1))
```

结果如下

```
result1 
result2 kEraCN
```

这里第一个正则最后用了**.*?**，所以非贪婪被前面的非贪婪抢占了，后面自然抢不到字符，后面是贪婪抢占非贪婪，所以成功了。

##### 修饰符

正则表达式可以包含一些可选标志修饰符来控制匹配的模式，**修饰符**被指定为一个可选的标志。

```python
import re

content = '''Hello 1234567 World_This
is a Regex Demo
'''
result = re.match('^He.*?(\d+).*?Demo$', content)
print(result.group(1))
```

`AttributeError: 'NoneType' object has no attribute 'group'`，直接报错

为什么加了一个换行符就无法匹配呢？

这是因为匹配的是**除换行符之外的任意字符**，当遇到换行符时**.*?**，就不能匹配了，导致匹配失败。所以这里SRE_Match对象也没有真正建立起来，才会告诉你这个group用不了。

这里我们只需要用到修饰符**re.S**，作用是**匹配包括换行符在内的所有字符**。

```python
import re

content = '''Hello 1234567 World_This
is a Regex Demo
'''
result = re.match('^He.*?(\d+).*?Demo$', content, re.S)
print(result.group(1))
```

运行结果为1234567。

**re.S**在网页匹配中会经常用到，因为**HTML节点会经常有换行**，加上它就可以匹配节点与节点之间的换行了。

###### 其他修饰符

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209663.png" alt="image-20230702181008099" style="zoom:50%;" />

简而言之，这些修饰符都是为了**匹配字符串格式**而用上的。

| 修饰符 | 描述                                                         |
| ------ | ------------------------------------------------------------ |
| re.I   | 使匹配对大小写不敏感**（忽略大小写）**                       |
| re.L   | 做本地化识别（local-aware）匹配                              |
| re.M   | 多行匹配，影响^和$                                           |
| re.S   | 使匹配包括换行符在内的所有字符**（忽略换行符）**             |
| re.U   | 根据Unicode字符集解析字符。这个标志影响\w、\W、\b、\B        |
| re.X   | 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解 |

##### 转义匹配

我们知道正则表达式定义了许多匹配模式，如匹配除换行符以外的任意字符**.**，但如果目标字符串里也包含了**.**该怎么办？

这里就要用到**转义匹配**。加一个反斜线 \ 即可。

```python
import re

content = '(百度) www.baidu.com'
result = re.match('\(百度\)www\.baidu\.com', content)
print(result)
```

结果如下

```
<re.Match object; span=(0, 17), match='(百度)www.baidu.com'>
```

不过这里我插一句，如果有人按照我这个一模一样复制下来去运行，那是别想看见运行结果了，不信可以试试。

##### re.search

然后，re.match它只能从一个字符串的开头开始匹配。这个问题我在**re.match**那里已经强调了，我们看看运行。

```python
import re

content = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'
result = re.match('^Hello.*?(\d+).*?Demo$', content) # 我这里已经把^和$规则都放上去了
print(result)
```

出来就是None，说明失败了。因为match在用的时候要考虑开头，在匹配的时候并不方便。所以match它更适合**用于检测一个字符串是否满足某个正则表达式的要求**。这里引入另外一个方法，**re.search**.

**re.search**在匹配时会**扫描整个字符串，然后返回第一个成功匹配的结果**。（注意到区别没有？re.match是从字符串开头开始扫描，而re.search是扫描整个字符串）也就是说，正则表达式可以是字符串的一部分，匹配时search方法会依次扫描字符串，直到找到第一个符合规则的字符串，然后返回匹配内容。如果还是没找到就是none。

```python
import re

content = 'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'
result = re.search('Hello.*?(\d+).*?Demo', content)
print(result)
print(result.group(1))
```

结果如下

```
<re.Match object; span=(13, 53), match='Hello 1234567 World_This is a Regex Demo'>
1234567
```

因此，为了匹配方便，我们尽量使用search方法。接下来再看一个实例，我会将代码贴在下面，是一段亟待处理的html。

```html
import re

html = '''
<div id="songs-list">
    <h2 class="title">
        经典老歌
    </h2>
    <p class="introduction">
        经典老歌列表
    </p>
    <ul id="list" class="list-group">
        <li data-view="2">一路上有你</li>
        <li data-view="7">
        <a href="/2.mp3" singer="任贤齐">沧海一声笑</a>
        </li>
        <li data-view="4" class="active">
        <a href="/3.mp3" singer="齐秦">往事随风</a>
        </li>
        <li data-view="6">
        <a href="/4.mp3" singer="beyond">光辉岁月</a>
        </li>
        <li data-view="5">
        <a href="/5.mp3" singer="陈慧琳">记事本</a>
        </li>
        <li data-view="5">
        <a href="/6.mp3" singer="邓丽君">但愿人长久</a>
        </li>
    </ul>
</div>'''

result = re.search('<li.*?active.*?singer="(.*?)">(.*?)</a>', html, re.S)
if result:
    print(result.group(1), result.group(2))
```

这里的结果是齐秦 往事随风，我们来看上面一段正则表达式做了什么。

- 首先提取class为active的li节点内部超链接包含的歌手名和歌名
- 此时需要提取第三个li节点下a节点的singer属性和文本
- 接下来提取singer这个属性值，所以还需要写入singer="(.*?)"
- 这里需要提取的部分用小括号括起来，以便用group方法提取两侧边界是双引号

找到符合正则表达式的第一个内容后就会返回。re.S是针对节点间换行的。

如果将active去掉，那么返回的结果就是 任贤齐 沧海一声笑

##### re.findall(i.love.it)

反正去年我python大作业一直用它。😊😊😊

re.findall会搜索整个字符串并且返回匹配正则表达式的**所有内容**（相较于re.search应该就是一次）

代码如下

```python
import re

html = 'xxx'
results = re.findall('<li.*?href="(.*?)".*?singer="(.*?)">(.*?)</a>', html, re.S)
print(results)
print(type(results))
for result in results:
    print(result)
    print(result[0],result[1],result[2])
```

结果如下

```
[('/2.mp3', '任贤齐', '沧海一声笑'), ('/3.mp3', '齐秦', '往事随风'), ('/4.mp3', 'beyond', '光辉岁月'), ('/5.mp3', '陈慧琳', '记事本'), ('/6.mp3', '邓丽君', '但愿人长久')]
<class 'list'>
('/2.mp3', '任贤齐', '沧海一声笑')
/2.mp3 任贤齐 沧海一声笑
('/3.mp3', '齐秦', '往事随风')
/3.mp3 齐秦 往事随风
('/4.mp3', 'beyond', '光辉岁月')
/4.mp3 beyond 光辉岁月
('/5.mp3', '陈慧琳', '记事本')
/5.mp3 陈慧琳 记事本
('/6.mp3', '邓丽君', '但愿人长久')
/6.mp3 邓丽君 但愿人长久
```

它的返回结果不是**SRE对象**，而是**列表**。所以说需要遍历一下才能得到结果。

##### re.sub(格式化删除)

如果想要把一串文本中的所有数字都去掉，只用字符串的replace方法太繁琐，这是可以借助**sub方法**。

```python
import re

content = '54aK54yr5oiR54ix5L2g'
content = re.sub('\d+', '', content)
print(content)
```

结果就是一条`aKyroiRixLg`

这里只需要写第一条参数\d+匹配所有数字，第二个参数就是替换的字符串，赋值为空，第三个参数就是源。

###### 技巧：re.sub处理,re.findall提取

回到上面有一个html的提取问题，纯用re.findall来做正则写得会有些繁琐，有种好方法，就是**先用sub方法将a节点去掉，只留下文本，再利用findall提取**。比较的代码如下

```python
# 纯findall
results = re.findall('<li.*?>\s*?(<a.*?>)?(\w+)(</a>)?\s*?</li>')
for result in results:
    print(result[1])

# re.sub + re.findall
html = re.sub('<a.*?>|</a>', '', html)
# print(html)
results = re.findall('<li.*?>(.*?)</li>', html, re.S)
for result in results:
    print(result.strip())
```

在关键时候，sub方法可以起到事半功倍的作用。

##### re.compile（正则表达式的封装）

前面讲的都是处理字符串的方法。而**re.compile**可以将正则字符串编译成正则表达式对象，以便在后面的匹配中复用。

示例代码如下

```python
# Author: Rainsblue.chan
# Create: 2023/7/2
# FileName: compile
import re

content1 = '2019-12-15 12:00'
content2 = '2019-12-17 12:55'
content3 = '2019-12-22 13:21'
pattern = re.compile('\d{2}:\d{2}')
result1 = re.sub(pattern, '', content1)
result2 = re.sub(pattern, '', content2)
result3 = re.sub(pattern, '', content3)
print(result1, result2, result3)
```

结果就是`2019-12-15  2019-12-17  2019-12-22 `

就是把相应格式减掉，如此而已。好处就是可以复用，另外compile方法还可以传入**修饰符**，如re.S，**这样在search和findall当中就不需要额外传入了。**

所以可以说，re.compile的作用就是**给正则表达式做了一次封装，以便我们更好的复用**。

到此概念部分结束，我们之后会通过更多的实例来运用它们。

### 09.爬虫解析利器PyQuery的使用

#### 前言，为什么需要PyQuery

摆了一天，非常非常慌，赶紧爬回来接着搞，昨天忙着找小程序资源，发现这玩意还是得从前端底层学。

今天是2023年7月4号，上回书说到，这个正则表达式可以配合我们的爬虫找出我们想要的东西，用的是re库，用了match、search、findall、compile封装、sub格式化删除，等等知识点，非常的丰富，但是因为网页结构复杂的原因，当中很容易写错，所以今天要讲的**PyQuery**就是一个好方法。

每个网页都有一定的特殊结构和层级关系，而且很多节点都有id或class作为区分，我们可以借助它们的结构和属性来提取信息吗？**这的确是可行的。**

从这里开始，我们就会用到这个强大的HTML解析库——**PyQuery**。有了它，我们就可以解析DOM节点结构（DOM，文档对象模型，详见02），对于DOM节点属性快速提取。

#### 初始化

解析HTML文本时，首先需要将其初始化为一个pyquery对象。

##### **字符串初始化**（最常用，可与requests库配合使用）

直接把HTML的内容当作参数来初始化pyquery对象，下为实例

```python
# Author: Rainsblue.chan
# Create: 2023/7/4
# FileName: PyQuery
from pyquery import PyQuery as pq

html = '''
<div>
    <ul>
        <li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html">second item</a></li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
    </ul>
</div>
'''

# 创建 PyQuery 对象
doc = pq(html)

# 打印所有 li 标签
print(doc('li'))
```

这里我遇到一个问题，是lxml库里没有etree，应该是版本太高了，[这个链接有方法解决](https://www.detayun.cn/blog/blogArticlePage/?blog_id=f83baefa-0341-11ee-bfda-88b111be8e07&source=baidu)

```
解决方法：
（1）先卸载原本的lxml
pip uninstall lxml

（2）再安装下面这个版本
高版本lxml没有etree模块。有网友确定lxml4.2.5版本带有etree模块，且该版本lxml支持python3.7.16版本。安装命令：
pip install lxml==4.2.5
```

这样就能正确返回结果

```
<li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html">second item</a></li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
```

这里首先引入pyquery这个库取别名为pq，然后声明了一个长的html字符串，并把它赋给py这个类，**成功完成了初始化。**

接下来，我们将初始化完毕的对象**传入css选择器**（定位节点作用，具体详见02），我们选择li节点，这样我就可以选择所有的li节点内容来进行处理。

##### **URL初始化**

初始化的参数不仅可以以字符串的形式传递，还可以传入网页的URL，只需要指定参数为url即可。

我们来举个例子。

```python
# Author: Rainsblue.chan
# Create: 2023/7/4
# FileName: PyQuery
from pyquery import PyQuery as pq

# 代理
proxies={
    'http':'127.0.0.1:4780',
    'https':'127.0.0.1:4780',
}
# request-headers
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

# 选择著名爬虫靶站豆瓣top250
doc = pq(url='https://movie.douban.com/top250?start',proxies=proxies,headers=headers)

# 打印所有 title 标签
print(doc('title'))
```

这个库的基本使用还有参数什么的，**和requests基本差不多**。得到结果如下。

```
<title>
豆瓣电影 Top 250
</title>
```

###### **CSS选择器的巩固**

返回就是一个标题头，如果想要返回电影名称的话，我们只要用`doc(span.title)`来作为我们的css选择器即可。

![image-20230704184732096](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209823.png)

```
在这个选择器中，"." 符号表示 class 选择器，用于选择具有指定 class 的元素，而 "title" 是 class 的名称。因此，使用 '.title' 选择器可以选择具有 class 属性为 "title" 的元素。如果你的代码中需要选择具有特定 class 的元素，可以尝试使用 class 选择器。
```

这个是对02的一个复习巩固了，非常关键的内容。**.class**选择器的使用就是用  .来接属性值。好牛逼。

###### 与requests.get

`doc = pq(url='https://movie.douban.com/top250?start',proxies=proxies,headers=headers)`与`doc = pq(requests.get('https://movie.douban.com/top250?start').text,proxies=proxies,headers=headers)`的作用是相同的，就是负责做一个html请求得到源码，然后使用pyquery做一个初始化对象，只不过它原生自带的参数写法更为简单。

##### 文件初始化

除了传递一个URL，还可以传递本地的文件名，参数指定为filename即可。

我们曾经用过的一个网页文件练手。这里我又遇到一个小问题，就是你命名的文件不要和库重名，我这里py文件取了pyquery就报错了。还有就是关于pyquery对象设置编码的问题，我们知道就是在requests库里是可以通过这一行来解决的。`r.encoding = r.apparent_encoding`，而在pyquery里面需要这样。

```python
# Author: Rainsblue.chan
# Create: 2023/7/4
# FileName: pyquery
from pyquery import PyQuery as pq
with open('test.html','r+',encoding='utf-8') as f:
    text = f.read()
doc = pq(text)
print(doc)
```

就是先读取并重新用utf-8编码。选择li节点后就得到抽象之语。

```html
<li><a href="https://www.miyoushe.com/" target="_blank">米游社</a></li>
               <li><a href="https://jump2.bdimg.com/f?kw=%E5%8E%9F%E7%A5%9E&amp;ie=utf-8">原神吧</a></li>
               <li><a href="https://jump2.bdimg.com/f?kw=%E6%98%9F%E7%A9%B9%E9%93%81%E9%81%93" target="_blank">星穹铁道吧</a></li>
          <li><img src="./tupian/wx.png" alt="如果想知道" class="尾部" title="如果想知道"/></li>
          <li>---</li>
          <li><img src="./tupian/qt.png" alt="qt为什么那么强" class="尾部" title="qt为什么那么强"/></li>
          <li>---</li>
          <li><img src="./tupian/hhc.png" alt="请打开1.php" class="尾部" title="请打开1.php"/></li>
```

以上三种都可以用，当然最常见还是字符串形式，我猜**是因为requests功能和pyquery分开做，功能独立开更加高效的缘故**。对吧，requests抓源代码下来，下一步要做解析了就交给pyquery，非常清楚的思路。

#### 基本CSS选择器

示例如下

```python
# Author: Rainsblue.chan
# Create: 2023/7/4
# FileName: PyQuery
from pyquery import PyQuery as pq

html = '''
<div id="container">
    <ul class="list">
        <li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html">second item</a></li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
    </ul>
</div>
'''

doc = pq(html)
print(doc('#container .list li'))
print(type(doc('#container .list li')))
```

运行结果如下

```
<li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html">second item</a></li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
    
<class 'pyquery.pyquery.PyQuery'>
```

在上面的例子当中，我们初始化pyquery对象之后，传入css选择器（**#container  .list li** ），它的意思是**“先选取id为container的节点，再选取其内部class为list的所有li节点，最后打印输出”**。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209513.png" alt="image-20230705000735389" style="zoom:50%;" />

直接遍历这些节点，然后调用text方法，就可以获取节点的文本内容。

```python
for item in doc('#container .list li').items():
    print(item.text())
```

结果为

```
first item
second item
third item
fourth item
fifth item
```

我们没有使用正则表达式，而是用css选择器就得到了文本信息。是不是方便很多？

接下来介绍一些能够高效运作的方法。

#### 子、父、兄弟节点（find，children，.active，parent(s)，siblings）

##### find/children

查找子节点需要用到**find方法**，传入的参数是**CSS选择器**。

```python
# Author: Rainsblue.chan
# Create: 2023/7/5
# FileName: zijiedian
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f:
    html = f.read()
doc = pq(html)
items = doc('.list')
print(type(items))
print(items)
lis = items.find('li')
print(type(lis))
print(lis)
```

我这里为了简介，所以用文件读写的方式，将html文件与代码分隔开来。结果如下

```css
<class 'pyquery.pyquery.PyQuery'>
<ul class="list">
        <li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html">second item</a></li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
    </ul>

<class 'pyquery.pyquery.PyQuery'>
<li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html">second item</a></li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
```

这里先用了一个.list，找所有class属性为list的节点。然后我们调用**find方法**，传入css选择器，寻找内部为li的节点。find的查找范围是所有子孙，如果只想找子节点，那么就用**children方法**。

##### children.active

如果要筛选所有子节点中符合条件的节点，比如想要筛选出子节点中class为active的节点，可以向children方法传入CSS选择器.active。这里结果就只会有一条

```
<class 'pyquery.pyquery.PyQuery'>
<li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
```

##### parent(s)

用parent方法来获取某个节点的**父节点**。

```python
# Author: Rainsblue.chan
# Create: 2023/7/5
# FileName: zijiedian
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f: # 自动关闭
    html = f.read()
doc = pq(html)
items = doc('.list')
container = items.parent()
print(type(container))
print(container)
```

结果如下

```css
<class 'pyquery.pyquery.PyQuery'>
<div id="container">
    <ul class="list">
        <li class="item-0">first item</li>
        <li class="item-1"><a href="link2.html">second item</a></li>
        <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
        <li class="item-1 active"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a></li>
    </ul>
</div>
```

**parents方法**为寻找祖先节点，会返回所有祖先节点和父节点。这里不作展示。

如果需要筛选某个祖先节点，那么就向parents方法中传入CSS选择器，比如.wrap即可。

##### siblings

```python
# Author: Rainsblue.chan
# Create: 2023/7/5
# FileName: zijiedian
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f: # 自动关闭
    html = f.read()
doc = pq(html)
li = doc('.list .item-0.active')
print(li.siblings())
```

结果为

```css
<li class="item-1"><a href="link2.html">second item</a></li>
            <li class="item-0">first item</li>
            <li class="item-1 active"><a href="link4.html">fourth item</a></li>
            <li class="item-0"><a href="link5.html">fifth item</a></li>
```

如果要筛选，依然可以用css选择器，只要传入siblings方法内就可以了。

#### 遍历

根据刚刚的几个例子，我们发现**pyquery的选择结果**：

- 可能是多个节点
- 可能是单个节点

都是**pyquery类型**，并没有返回列表。

##### 对于单个节点

可以直接打印输出，也可以直接转成字符串

```python
from pyquery import PyQuery as pq
doc = pq(html)
li = doc('.item-0.active')
print(li)
print(str(li))
```

运行结果如下

```css
<li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
            
<li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
```

##### 对于多个节点

需要用遍历来获取

```python
from pyquery import PyQuery as pq
doc = pq(html)
lis = doc('li').items()
print(type(lis))
for li in lis:
	print(li, type(li))
```

结果如下

```css
<class 'generator'>
<li class="item-0">first item</li>
             <class 'pyquery.pyquery.PyQuery'>
<li class="item-1"><a href="link2.html">second item</a></li>
             <class 'pyquery.pyquery.PyQuery'>
<li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
             <class 'pyquery.pyquery.PyQuery'>
<li class="item-1 active"><a href="link4.html">fourth item</a></li>
             <class 'pyquery.pyquery.PyQuery'>
<li class="item-0"><a href="link5.html">fifth item</a></li>
         <class 'pyquery.pyquery.PyQuery'>
```

需要调用**items()方法**，他会有一个生成器，非常灵活。

#### 获取信息

我们最终的目的是提取**节点所包含的信息**，这里包含两个东西：

- 获取**属性**
- 获取**文本**

##### 获取属性（attr方法）

调用**attr方法**来获取属性

```python
# Author: Rainsblue.chan
# Create: 2023/7/5
# FileName: get
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f: # 自动关闭
    html = f.read()
doc = pq(html)
a = doc('.item-0.active a')
print(a, type(a))
print(a.attr('href'))
```

结果如下

```
<a href="link3.html"><span class="bold">third item</span></a> <class 'pyquery.pyquery.PyQuery'>
link3.html
```

我们调用attr方法，传入属性名称就可以得到属性值，这里href的属性值是link3.html，也可以用a.attr.href来得到。

如果attr遇到多节点，它只会返回第一个节点的属性。

```python
a = doc('a')
print(a, type(a))
print(a.attr('href'))
print(a.attr.href)

结果为
<a href="link2.html">second item</a><a href="link3.html"><span class="bold">third item</span></a><a href="link4.html">fourth item</a><a href="link5.html">fifth item</a> <class 'pyquery.pyquery.PyQuery'>
link2.html
link2.html
```

遇到这种情况，如果想要获取所有a节点的属性，就要使用遍历，

```python
from pyquery import PyQuery as pq
doc = pq(html)
a = doc('a')
for item in a.items():
	print(item.attr('href'))

结果正常返回如下
link2.html
link3.html
link4.html
link5.html
```

##### 获取文本（text、html方法）

获取其内部文本可以调用**text方法**来实现

```python
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f: # 自动关闭
    html = f.read()
doc = pq(html)
a = doc('.item-0.active a')
print(a)
print(a.text())

返回如下：
<a href="link3.html"><span class="bold">third item</span></a>
third item
```

我们这里首先选用a节点，忽略节点内部，返回纯文字。

如果想要获取这个节点内部的html文本，就要用**html方法**。

```python
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f: # 自动关闭
    html = f.read()
doc = pq(html)
a = doc('.item-0.active')
print(a)
print(a.html())

返回如下：
<li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
            
<a href="link3.html"><span class="bold">third item</span></a>
```

上面还是针对单节点来的，如果多节点的话，我们来看一下示例，这里**text方法和html方法会有一些不同**。

```python
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f: # 自动关闭
    html = f.read()
doc = pq(html)
li = doc('li')
print(li.html())
print(li.text())
print(type(li.text()))

返回如下：
first item
first item second item third item fourth item fifth item
<class 'str'>
```

html内容还是需要遍历的方法来得到，但是text会把整个以字符串形式返回。

#### 节点操作

pyquery提供了一系列方法来对节点进行**动态修改**，比如：

- 为某个节点添加一个class
- 移除某个节点等

接下来举几个典型的例子

##### addClass和removeClass

```python
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f: # 自动关闭
    html = f.read()
doc = pq(html)
li = doc('.item-0.active')
print(li)
li.removeClass('active')
print(li)
li.addClass('active')
print(li)

结果如下：
<li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
            
<li class="item-0"><a href="link3.html"><span class="bold">third item</span></a></li>
            
<li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
```

可以动态改变class属性

##### attr、text、html的赋值

除了操作class这个属性外，也可以用attr方法来对属性进行操作，还可以用text和html方法来改变节点内部的内容。

```python
from pyquery import PyQuery as pq
with open('html.html','r+',encoding='utf-8') as f: # 自动关闭
    html = f.read()
doc = pq(html)
li = doc('.item-0.active')
print(li)
li.attr('name', 'link')
print(li)
li.text('changed item')
print(li)
li.html('<span>changed item</span>')
print(li)

返回如下：
<li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
            
<li class="item-0 active" name="link"><a href="link3.html"><span class="bold">third item</span></a></li>
            
<li class="item-0 active" name="link">changed item</li>
            
<li class="item-0 active" name="link"><span>changed item</span></li>
```

attr传二参会动态赋一个属性，text，html传参会赋值，不传参是得值。

##### remove

remove方法就是移除。

```python
html = '''
<div class="wrap">
	Hello,World
	<p>This is a paragraph.</p>
</div>
'''
from pyquery import PyQuery as pq
doc = pq(html)
wrap = doc('.wrap')
print(wrap.text()+'\n')

# 接下来是移除操作
wrap.find('p').remove()
print(wrap.text())

结果如下：
Hello,World
This is a paragraph.

Hello,World
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102209098.png" alt="image-20230705154309971" style="zoom:50%;" />

还有很多其他节点操作的方法，比如append、empty和prepend等方法，详细的用法可以参考下面链接

[链接](http://pyquery.readthedocs.io/en/latest/api.html)

#### 伪类选择器

CSS选择器强大很重要的原因，就是**它支持多种多样的伪类选择器**

这里不在多介绍，我今天也学累了，[更多用法可参考这个](http://www.w3school.com.cn/css/index.asp)

### 10.高效存储MongoDB的用法

#### 前言

我们上一讲提到了pyquery，我们可以使用它快速提取html当中节点信息。

那么提取完的信息该如何保存呢？当然用文本文件是可以的，**但是它不方便检索**。本课时就是引入**MongoDB**数据库，它是**文档型数据库**。

#### 导入

**MongoDB**

是由C++语言编写的**非关系型**数据库（一般是关系型，像MySQL）

是一个基于分布式文件存储的**开源数据库系统**。

其内容存储形式类似JSON对象，它的字段值可以包含其他文档、**数组**及**文档数组**。

python3有其对应的相关操作。

#### 准备工作

![image-20230705172935224](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102208178.png)

https://juejin.cn/post/6844903912000978952 这个不错

<img src="scrawer/image-20230705182114950.png" alt="image-20230705182114950" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102214302.png" alt="image-20230705182252935" style="zoom:50%;" />

常用命令，需在管理员模式运行。

https://www.jianshu.com/p/d6e009c1b836 还有这个链接非常重要，因为6以上的mongodb是没有mongo指令的。

千万别下6，一坨浓屑，卸了换4了。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215084.png" alt="image-20230705193316104" style="zoom:50%;" />

爽的一，6一坨浓屑，4只要输一个mongo就能进数据库操作。端口为**27017**，如果启动服务成功，输入127.0.0.1:27017即可检验。

#### 连接MongoDB

需要使用PyMongo库里面的**MongoClient**

```python
# Author: Rainsblue.chan
# Create: 2023/7/5
# FileName: mongo
import pymongo
client = pymongo.MongoClient(host='localhost', port=27017) # localhost | 127.0.0.1
```

一般来说，我们只需传入mongodb的**ip和端口**即可。如果不传端口就默认为27017。

还可以直接传入MongoDB的连接字符串。

```python
client = MongoClient('mongodb://localhost:27017/')
```

#### 指定数据库

MongoDB中可以建立多个数据库，需要指定操作其中一个数据库。

```python
db = client.test # client['test']也可以
```

这里我们以test数据库为下一步程序需要使用的例子。

这里用client.test，即可返回test数据库。

#### 指定集合

MongoDB的每个数据库又包含许多集合(collection)，类似于关系型数据库中的表

```python
collection = db.students # db['students']
```

指定名为“students”的集合。这样我们便声明了一个collection对象。接下来我们就可以插入数据了。

#### 插入数据（insert_one，insert_many）

对students这个集合新建一条学生数据，再调用collection的insert_one方法**(4以前版本应该是可以直接一个insert)**即可插入数据

```python
student = {
	'id': '20170101',
	'name': 'Jordan',
	'age': 20,
	'gender': 'male'
}

result = collection.insert_one(student)
print(result)
```

<img src="scrawer/image-20230705201754353.png" alt="image-20230705201754353" style="zoom:50%;" />

显示是没问题。insert方法会在执行完之后返回id值。

还有一个彻底的方法，跟flask库有关系或者和pymonge库有关系

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215032.png" alt="image-20230705201953279" style="zoom:50%;" />

将其中的**MongoDBSessionInterface函数**改为下面的即可

```python
class MongoDBSessionInterface(SessionInterface):
    """A Session interface that uses mongodb as backend.
 
    .. versionadded:: 0.2
        The `use_signer` parameter was added.
 
    :param client: A ``pymongo.MongoClient`` instance.
    :param db: The database you want to use.
    :param collection: The collection you want to use.
    :param key_prefix: A prefix that is added to all MongoDB store keys.
    :param use_signer: Whether to sign the session id cookie or not.
    :param permanent: Whether to use permanent session or not.
    """
 
    serializer = pickle
    session_class = MongoDBSession
 
    def __init__(self, client, db, collection, key_prefix, use_signer=False,
                 permanent=True):
        if client is None:
            from pymongo import MongoClient
            client = MongoClient()
        self.client = client
        self.store = client[db][collection]
        self.key_prefix = key_prefix
        self.use_signer = use_signer
        self.permanent = permanent
        self.has_same_site_capability = hasattr(self, "get_cookie_samesite")
 
    def open_session(self, app, request):
        sid = request.cookies.get(app.session_cookie_name)
        if not sid:
            sid = self._generate_sid()
            return self.session_class(sid=sid, permanent=self.permanent)
        if self.use_signer:
            signer = self._get_signer(app)
            if signer is None:
                return None
            try:
                sid_as_bytes = signer.unsign(sid)
                sid = sid_as_bytes.decode()
            except BadSignature:
                sid = self._generate_sid()
                return self.session_class(sid=sid, permanent=self.permanent)
 
        store_id = self.key_prefix + sid
        document = self.store.find_one({'id': store_id})
        if document and document.get('expiration') <= datetime.utcnow():
            # Delete expired session
            self.store.delete_many({'id': store_id})
            document = None
        if document is not None:
            try:
                val = document['val']
                data = self.serializer.loads(want_bytes(val))
                return self.session_class(data, sid=sid)
            except:
                return self.session_class(sid=sid, permanent=self.permanent)
        return self.session_class(sid=sid, permanent=self.permanent)
 
    def save_session(self, app, session, response):
        domain = self.get_cookie_domain(app)
        path = self.get_cookie_path(app)
        store_id = self.key_prefix + session.sid
        if not session:
            if session.modified:
                self.store.delete_many({'id': store_id})
                response.delete_cookie(app.session_cookie_name,
                                       domain=domain, path=path)
            return
 
        conditional_cookie_kwargs = {}
        httponly = self.get_cookie_httponly(app)
        secure = self.get_cookie_secure(app)
        if self.has_same_site_capability:
            conditional_cookie_kwargs["samesite"] = self.get_cookie_samesite(app)
        expires = self.get_expiration_time(app, session)
        val = self.serializer.dumps(dict(session))
 
        whe     = {'id': store_id}
        data_   = {'id': store_id,'val': val,'expiration': expires }
        if  self.store.count_documents(whe)>0:
            self.store.update_many(whe, {"$set":data_})
        else:
             self.store.insert_one(data_)
 
        if self.use_signer:
            session_id = self._get_signer(app).sign(want_bytes(session.sid))
        else:
            session_id = session.sid
        response.set_cookie(app.session_cookie_name, session_id,
                            expires=expires, httponly=httponly,
                            domain=domain, path=path, secure=secure,
                            **conditional_cookie_kwargs)
```

```bash
降级
pip3 install pymongo==3.9
```

后来我在3.9运的时候，虽然有报错但还是成功插入了，它的报错是insert命令被弃用了，请使用insert_one或者insert_many方法。

##### 插入多条数据

```python
# Author: Rainsblue.chan
# Create: 2023/7/5
# FileName: mongo
import pymongo

# 连接数据库
client = pymongo.MongoClient(host='localhost', port=27017) # localhost | 127.0.0.1

# 指定数据库
db = client.test

# 指定集合为students
collection = db.students

# 插入一个学生数据
student1 = {
	'id': '20170101',
	'name': 'Jordan',
	'age': 20,
	'gender': 'male'
}

student2 = {
	'id': '20170202',
	'name': 'Mike',
	'age': 21,
	'gender': 'male'
}
result = collection.insert_many([student1,student2])
print(result)

返回为一个pymongo对象
<pymongo.results.InsertManyResult object at 0x00000126799AC708>
```

<img src="scrawer/image-20230705203253923.png" alt="image-20230705203253923" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215733.png" alt="image-20230705203359575" style="zoom:67%;" />

```
返回结果
<pymongo.results.InsertOneResult object at 0x0000021C68FA3B08>
64a5636d19a7ee40e1cd8b69
```

如果要看many的id，就是inserted_ids

#### 查询（find、find_one、ObjectId、比较符号、正则）

利用find_one或者find方法进行查询，**find_one**查询得到的是单个结果，**find**则返回一个生成器对象。

```python
result = collection.find_one({'name': 'Mike'})
print(type(result))
print(result)

结果如下
<class 'dict'>
{'_id': ObjectId('64a565b2e04969e0b46c41c7'), 'id': '20170202', 'name': 'Mike', 'age': 21, 'gender': 'male'}
```

找到是一个字典类型的数据，这也与**MongoDB本身的非关系存储**是挂钩的，然后返回这一个集合的数据。

可以根据ObjectId来查询，此时需要调用bson库里面的objectid

```python
from bson.objectid import ObjectId

result = collection.find_one({'_id': ObjectId('64a565b2e04969e0b46c41c7')})
print(result)
结果是一样的
```

![image-20230705205633081](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215013.png)

[https://www.idcnote.com/php/biji/107844.html](https://www.idcnote.com/php/biji/107844.html)这个是图片来源，里面详细介绍了bson是干嘛的。

如果没查到，返回为None。

多条数据的查询可以使用**find方法**。

```python
results = collection.find({'age': 20})
print(results)
for result in results:
    print(result)
    
结果为：
<pymongo.cursor.Cursor object at 0x0000021575A1D808>
{'_id': ObjectId('64a5636d19a7ee40e1cd8b69'), 'id': '20170101', 'name': 'Jordan', 'age': 20, 'gender': 'male'}
{'_id': ObjectId('64a56adf8ae47d31b6dc9afb'), 'id': '20170303', 'name': 'Kevin', 'age': 20, 'gender': 'male'}
{'_id': ObjectId('64a56adf8ae47d31b6dc9afc'), 'id': '20170404', 'name': 'Harden', 'age': 20, 'gender': 'male'}
```

将键值对当中含有**'age':20**的字典数据全部找出来。

如果要查询年龄大于20的数据，写法如下

`results = collection.find({'age': {'$gt': 20}})`

##### 比较符号

比较符号$gt，这里用的是字典形式，类似的可见下图

![image-20230705220220456](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215044.png)

```
常用大于等于小于的特殊标记LT、LE、EQ、NE、GE、GT

lt：less than 小于
le：less than or equal to 小于等于
eq：equal to 等于
ne：not equal to 不等于
ge：greater than or equal to 大于等于
gt：greater than 大于
GT=Great Than >
EQ=Equal =
GE=Great and Equal >=
NE=Not Equal <>
```

##### 正则匹配查询

`results = collection.find({'name': {'$regex': '^M.*'}})`

这里$regex来指定正则匹配，^M.*代表以M开头的正则表达式。

![image-20230705220857752](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215092.png)

我们试一下。首先第一个用正则来找名字开头为M的字典数据。

![image-20230705221429023](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215861.png)

接下来，找字典中存在name属性值的字典。于是返回4个。

![image-20230705221514706](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215344.png)

其他我不试了，我累了。

#### 统计（count）

统计查询结果有多少条数据，可以调用count方法

```python
# 统计所有数据
count = collection.find().count()
print(count)

# 统计符合某个条件的数据
count = collection.find({'age': 20}).count()
print(count)
```

#### 排序（sort）

排序时可以直接调用sort方法，并在其中传入排序的字段及升降序标志。

```python
# 排序
results = collection.find().sort('name', pymongo.ASCENDING) # 降序选择pymongo.DESCENDING
print([result['name'] for result in results]) # 简单的列表推导式

结果如下：
['Harden', 'Jordan', 'Kevin', 'Mike']
```

#### 偏移（skip，limit）

某些情况下，可能只需要取某几个元素，这时可以利用skip方法偏移几个位置。limit为限制。

```python
# 偏移
results = collection.find().sort('name', pymongo.ASCENDING).skip(2) # 与limit不同，这里是跳过
print([result['name'] for result in results])

结果只有两个：['Kevin', 'Mike']，忽略前两个元素

# skip+limit
results = collection.find().sort('name', pymongo.ASCENDING).skip(1).limit(2)
print([result['name'] for result in results])

结果为：['Jordan', 'Kevin']
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215752.png" alt="image-20230706001744112" style="zoom: 67%;" />

这里用了一个greater than这个id号的方式来做到更快的查询。

#### 更新（update）

数据更新可以使用update方法，指定更新的条件和更新后的数据即可。

```python
# Author: Rainsblue.chan
# Create: 2023/7/6
# FileName: update
import pymongo
client = pymongo.MongoClient(host='localhost')
# 指定数据库test
db = client.test
# 指定集合为students
collection = db.students
condition = {'name': 'Kevin'}
student = collection.find_one(condition)
# 修改字典内元素
student['age'] = 25
# 更新
result = collection.update_one(condition, {'$set':student})
print(result)
# 结果返回了一个生成器，然后下面要这么搞
# 读取对象
res = collection.find_one({'name': 'Kevin'})
print(res)

返回如下：
<pymongo.results.UpdateResult object at 0x000001AD870E21C8>
{'_id': ObjectId('64a56adf8ae47d31b6dc9afb'), 'id': '20170303', 'name': 'Kevin', 'age': 25, 'gender': 'male'}
```

都是在4.4标准之下进行的。就是说，他把所有的功能全拆开来了。update管update，生成器隶属严格，然后find管find，是这样的一套模式。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215002.png" style="zoom:67%;" />

就是说更加严格，下面也为示例。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215933.png" alt="image-20230706010609988" style="zoom: 67%;" />

如果是many方法，那么就会有3条更新

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215708.png" alt="image-20230706010738702" style="zoom:67%;" />

#### 删除（delete_one/many）

调用delete_one/many方法指定删除的条件即可，一删就删一个字典吧，应该

```python
result = collection.delete_one({'name': 'Kevin'})
print(result)
print(result.deleted_count)
# 删除所有小于25岁的
result = collection.delete_many({'age': {$lt: 25}})
print(result.deleted_count)
```

#### 其他操作（组合）

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215951.png" alt="image-20230706011347335" style="zoom:80%;" />

![image-20230706011413013](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215476.png)

下一讲就开始将这三个课时，requests，PyQuery，MongoDB结合起来做基本案例实战。

### 11.Requests + PyQuery + PyMongo 基本案例实战（2023.7.6）

这一次就用这三个知识点来爬取我们著名靶场豆瓣250，将所有知识点串联起来。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215939.png" alt="image-20230706154905219" style="zoom:80%;" />

#### 爬取目标

爬取链接：https://movie.douban.com/top250?start

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215807.png" alt="image-20230706155349372" style="zoom: 50%;" />

每一页都是一个新的列表，每一个列表里面有对应的元素，每一个元素就是一部电影，一个电影里有相应的信息。

如果我们点开其中一部电影，还会进入详情页面。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215621.png" alt="image-20230706155650220" style="zoom:50%;" />

我们的目标如下：

- 用**requests**爬取这个站点每一页的电影列表，再顺着列表爬取每个电影的详情页
- 用**pyquery**和**正则表达式**提取每部电影的名称、封面、类别、上映时间、评分、剧情简介等
- 把以上爬取的内容存入**MongoDB数据库**
- 使用**多进程**实现爬取的加速

#### 爬取列表页

打开网站，打开开发者工具，我们观察网页当中区块的html代码部分。

![image-20230706160854075](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215304.png)

这里看开发者工具这一栏左上角，有一个小的箭头，点击它就能快速选择网页元素。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215399.png" alt="image-20230706161240048" style="zoom:50%;" />

这里是肖申克的救赎这部电影的节点。我们不难发现，这里所有电影的节点都是以**div**开头的，而且当中含有电影详情页的链接。（**div就是division，在css当中用于分类，划分**）

它的**class属性**对应**pic**这个值，我们回忆一下css选择器当中，这应该写作为**.pic**。这里应该指的就是图片，图片也有超链接。每一个列表页有这样**25个div节点**，也就是对应25部电影的信息。

我们再来深入地看这一个节点

![image-20230706162450860](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215556.png)

##### 为何可以进一步爬取详情页？

我们选择电影标题，发现它是一个**span节点**（*Special Parametric Analysis of Control Nodes*，特别参数节点），包在外面的是一个*a href*节点，它也是超链接，指向电影详情页。

这样我们只需要提取这个*a href*属性就可以构造出详情页的url并且进行爬取。

##### 翻页逻辑

我们拉到网页最下端可以看见网页的页码。

![image-20230706175501292](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215531.png)

页面显示有250条数据，所以页码为10，连接分别如上面所示，接下来我们点击第二页，它会变成这样。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215975.png" alt="image-20230706175631601" style="zoom:50%;" />

可以看见网页的url变成了这个链接。相比根url，多了  *=25&filter=*  这么一串，而网页的结构还是和原来一模一样，可以和第一页一样进行处理，之后的每一页都变成了加start+25。同时filter里面已经表明了1，2，3，4这样的值，不过我的测试如下。可以看见我的传参是start=147，直接跳到148而不是整数，这说明序号是从0开始排的，0，1，..，24就是第一页。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102215230.png" alt="image-20230706180247996" style="zoom:50%;" />

##### 目标

- 遍历页码构造10页的索引页URL
- 从每个索引页分析提取出每个电影的详情页URL

##### 代码

```python
# Author: Rainsblue.chan
# Create: 2023/7/6
# FileName: 11requests+pyquery+pymongo
import requests # 获取源码
import logging # 用于输出信息
import re # 正则处理
import pymongo # 存储爬虫数据
from pyquery import PyQuery as pq # 直接解析网页
from urllib.parse import urljoin # 用于url拼接

# 定义日志输出级别，输出格式
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s: %(message)s')

# 定义当前站点的根URL
BASE_URL = 'https://movie.douban.com/top250?'

# 爬取总的页码数
TOTAL_PAGE = 10
```

定义一些基础变量，引入一些必要的库。

###### **页面爬取**

```python
# 页面爬取
def scrape_page(url):
    logging.info('scraping %s...', url)
    try:
        response = requests.get(url)
        # 若状态码为200，是则返回页面html源码
        if response.status_code == 200:
            return response.text
        # 不是，输出错误日志信息
        logging.error('get invalid status code %s while scraping %s', response.status_code, url)
    # 异常错误处理，exc_info报true添加异常信息于日志中
    except requests.RequestException:
        logging.error('error occured while scraping %s', url, exc_info=True)
```

因为既要爬取列表页，又要爬取详情信息，所以这里就需要定义一个**通用爬虫函数**。

[logging浅看](https://blog.csdn.net/cainiao_python/article/details/121279514)

在这个基础上，我们来实现列表页爬取

###### 列表页爬取

```python
# 列表页爬取,25为2，50为3
def scrape_index(index):
    index_url = f'{BASE_URL}start={index}'
    return scrape_page(index_url)
```

###### 自己的一个总结性示例

```python
# Author: Rainsblue.chan
# Create: 2023/7/6
# FileName: 11requests+pyquery+pymongo
import requests # 获取源码
import logging # 用于输出信息
import re # 正则处理
import pymongo # 存储爬虫数据
from pyquery import PyQuery as pq # 直接解析网页
from urllib.parse import urljoin # 用于url拼接

global proxies
global headers
# 代理
proxies = {
    'http': 'http://127.0.0.1:4780',
    'https': 'http://127.0.0.1:4780',
}
# 请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090551) XWEB/8259 Flue'
}

# 定义日志输出级别，输出格式
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s: %(message)s')

# 定义当前站点的根URL
BASE_URL = 'https://movie.douban.com/top250?'

# 爬取总的页码数
TOTAL_PAGE = 10

# 代理是否开启
print('主机是否已开启代理？')
proxies_off = input('直接enter键选择否，输入任意值使用代理\n')

# 页面爬取
def scrape_page(url):
    logging.info('scraping %s...', url)
    try:
        if proxies_off == '':
            response = requests.get(url,headers=headers)
        response = requests.get(url,headers=headers,proxies=proxies)
        # 若状态码为200，是则返回页面html源码
        if response.status_code == 200:
            return response.text
        # 不是，输出错误日志信息
        logging.error('get invalid status code %s while scraping %s', response.status_code, url)
    # 异常错误处理，exc_info报true添加异常信息于日志中
    except requests.RequestException:
        logging.error('error occured while scraping %s', url, exc_info=True)

# 列表页爬取
# 列表页爬取,25为2，50为3
def scrape_index(index):
    index_url = f'{BASE_URL}start={index}'
    return scrape_page(index_url)

for i in range(10):
    # print(scrape_index(i*5))
    html = scrape_index(i*5)
    doc = pq(html)
    for item in doc('.title').items():
        print(item.text())
```

###### 解析列表页

```python
# 解析列表页
def parse_index(html):
    doc = pq(html)
    links = doc('#content .hd a')
    for link in links.items():
        href = link.attr('href')
        detail_url = urljoin(BASE_URL, href)
        logging.info('get detail url %s', detail_url)
        yield detail_url
```

这一段我们需要借助urljoin将详情页的url给拼接起来，用yield返回。

###### 串联调用

```python
# 串联
def main():
    for page in range(0, TOTAL_PAGE):
        index = page * 25
        index_html = scrape_index(index)
        detail_urls = parse_index(index_html)
        logging.info('detail urls %s', list(detail_urls))
        
结果如下：
2023-07-07 13:25:56,627 - INFO: scraping https://movie.douban.com/top250?start=0...
2023-07-07 13:25:56,866 - INFO: get detail url https://movie.douban.com/subject/1292052/
2023-07-07 13:25:56,866 - INFO: get detail url https://movie.douban.com/subject/1291546/
2023-07-07 13:25:56,866 - INFO: get detail url https://movie.douban.com/subject/1292720/
2023-07-07 13:25:56,866 - INFO: get detail url https://movie.douban.com/subject/1292722/
2023-07-07 13:25:56,866 - INFO: get detail url https://movie.douban.com/subject/1295644/
2023-07-07 13:25:56,866 - INFO: get detail url https://movie.douban.com/subject/1291561/
......
```

程序首先解析列表页，得到了详情页的每一个url，接着再爬取第2页，第3页，这样就可以获取所有的详情页的url。

#### 爬取详情页

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216378.png" alt="image-20230707133339475" style="zoom:67%;" />

![image-20230707133446484](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216958.png)

需要爬取的信息如下：

- 封面：是一个img节点，#wrapper .nbgnbg a，然后老办法用attr剥离src
- 名称：这里是一个h1节点，内容就是名称
- 类别：span节点，div id为info

![image-20230707135052451](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216549.png)

- 上映时间：是span节点，其内容包含了上映时间，外侧包含了class为info的div节点，和上面是同一个。可以利用正则表达式将“上映”和日期给提取出来。可以使用nth-of-type来做区分

![image-20230707142707355](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216082.png)

- 评分：是一个strong节点，class属性内容就是9.7

![image-20230707142941554](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216657.png)

- 剧情简介：一个span节点，属性为property，隶属于class为indent的div节点

##### 代码

###### 详情页爬取（调用页面爬取即可）

```python
def scrape_detail(url):
	return scrape_page(url)
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216228.png" alt="image-20230707143354164" style="zoom:67%;" />

###### 详情页解析

```python
# 详情页解析
def parse_detail(html):
    doc = pq(html)
    # 封面、影名、缩略信息、剧情简介、评分
    cover = doc('a.sub-cover img').attr('src')
    name = doc('.sub-title').text()
    inf = doc('.sub-meta').text()
    drama = doc('.bd p').text()
    # 这里解释一下，因为doc对象里面的meta不能读标签，原因大概是这个
    #  <!-- Schema.org markup for Google+ -->
    # 所以我就改成html里面用正则了，\d.\d，就是数字点数字
    score = result = re.search('content="(\d.\d)"', html, re.S).group(1)
    return {
        'cover': cover,
        'name': name,
        'inf': inf,
        'drama': drama,
        'score': score
    }

# 串联
def main():
    for page in range(0, TOTAL_PAGE):
        index = page * 25
        index_html = scrape_index(index)
        detail_urls = parse_index(index_html)
        for detail_url in detail_urls:
            detail_html = scrape_detail(detail_url)
            data = parse_detail(detail_html)
            logging.info('get detail data %s', data)
```

一点一点调的，难受的很，不过我们还是坚信耐心出细活。

调试的方法是这样，开一个test一个个试试，然后从pyquery对象中确认css选择器的用法，这样比较准确。

运行结果大概就是这样。

![image-20230707203801044](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216448.png)

后面都是每一条对应的信息。

#### 保存到MongoDB

```python
# 保存到MongoDB
MONGO_CONNECTION_STRING = 'mongodb://localhost:27017'
MONGO_DB_NAME = 'movies' # MongoDB数据库名称
MONGO_COLLECTION_NAME = 'movies' # MongoDB的集合名称

client = pymongo.MongoClient(MONGO_CONNECTION_STRING)
db = client['movies']
collections = db['movies']
```

请确保MongoDB打开，然后我们进行基本连接，不清楚的可以看上面一节课。

##### 保存方法函数

```python
def save_data(data):
    collections.update_one({
        'name': data.get('name')
    },{
        '$set': data # set操作符表明更新操作
    }, upsert=True)  # 关键，当upsert为True，则可以做到存在即更新，不存在即插入 
```

将数据进行一个保存，这里提取的名字就是电影名。

##### upsert=True

非常关键，当它为True，则可以做到存在即更新，不存在即插入。**它的根据name来对数据进行操作**，name相当于关系型数据库中的主键了，防止同名字段，实现了MongoDB的去重操作。

#### 完整代码

```python
# Author: Rainsblue.chan
# Create: 2023/7/7
# FileName: doubantop250
import requests # 获取源码
import logging # 用于输出信息
import re # 正则处理
import pymongo # 存储爬虫数据
from pyquery import PyQuery as pq # 直接解析网页
from urllib.parse import urljoin # 用于url拼接

global proxies
global headers
# 代理
proxies = {
    'http': 'http://127.0.0.1:4780',
    'https': 'http://127.0.0.1:4780',
}
# 请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090551) XWEB/8259 Flue'
}
# 定义日志输出级别，输出格式
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s: %(message)s')

# 定义当前站点的根URL
BASE_URL = 'https://movie.douban.com/top250?'

# 爬取总的页码数
TOTAL_PAGE = 10

# 页面爬取
def scrape_page(url):
    logging.info('scraping %s...', url)
    try:
        response = requests.get(url,headers=headers,proxies=proxies)
        # 若状态码为200，是则返回页面html源码
        if response.status_code == 200:
            return response.text
        # 不是，输出错误日志信息
        logging.error('get invalid status code %s while scraping %s', response.status_code, url)
    # 异常错误处理，exc_info报true添加异常信息于日志中
    except requests.RequestException:
        logging.error('error occured while scraping %s', url, exc_info=True)

# 列表页爬取
# 列表页爬取,25为2，50为3
def scrape_index(page):
    index_url = f'{BASE_URL}start={page}'
    return scrape_page(index_url)

# 解析列表页
def parse_index(html):
    doc = pq(html)
    links = doc('#content .hd a')
    for link in links.items():
        href = link.attr('href')
        detail_url = urljoin(BASE_URL, href)
        logging.info('get detail url %s', detail_url)
        yield detail_url

# 详情页爬取
def scrape_detail(url):
    return scrape_page(url)

# 详情页解析
def parse_detail(html):
    doc = pq(html)
    # 封面、影名、缩略信息、剧情简介、评分
    cover = doc('a.sub-cover img').attr('src')
    name = doc('.sub-title').text()
    inf = doc('.sub-meta').text()
    drama = doc('.bd p').text()
    score = result = re.search('content="(\d.\d)"', html, re.S).group(1)
    return {
        'cover': cover,
        'name': name,
        'inf': inf,
        'drama': drama,
        'score': score
    }

# 保存到MongoDB
MONGO_CONNECTION_STRING = 'mongodb://localhost:27017'
MONGO_DB_NAME = 'movies'
MONGO_COLLECTION_NAME = 'movies'

client = pymongo.MongoClient(MONGO_CONNECTION_STRING)
db = client['movies']
collections = db['movies']

def save_data(data):
    collections.update_one({
        'name': data.get('name')
    },{
        '$set': data
    }, upsert=True)

# 串联
def main():
    for page in range(0, TOTAL_PAGE):
        index = page * 25
        index_html = scrape_index(index)
        detail_urls = parse_index(index_html)
        for detail_url in detail_urls:
            detail_html = scrape_detail(detail_url)
            data = parse_detail(detail_html)
            logging.info('get detail data %s', data)
            logging.info('saving data to mongodb')
            save_data(data)
            logging.info('data saved successfully')

if __name__ == '__main__':
    main()
```

没有代理的别挂。这样就没问题。

![image-20230707225039409](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216215.png)

#### ROBO 3T（可视化MongoDB）

[使用robo 3t连接mongodb的方法](https://blog.csdn.net/LittleGiantWang/article/details/122258922)

![image-20230707230856140](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216511.png)

#### 多进程加速

![image-20230707235957202](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216231.png)

```python
# 多进程，小子
import multiprocessing

# 串联
def main(page):
    index_html = scrape_index(page)
    detail_urls = parse_index(index_html)
    for detail_url in detail_urls:
        detail_html = scrape_detail(detail_url)
        data = parse_detail(detail_html)
        logging.info('get detail data %s', data)
        logging.info('saving data to mongodb')
        save_data(data)
        logging.info('data saved successfully')

if __name__ == '__main__':
    pool = multiprocessing.Pool()
    # 使用 map 函数将数字序列按照每25个元素提取一个
    pages = list(map(lambda x: x * 25, range(0, 10)))
    pool.map(main, pages)
    pool.close()
    pool.join()
```

使用进程池，map是用的方法，pages是放入进程池中间的网页。

进程数量依据电脑cpu，你可以发现比单单之前那个快很多。

#### 403问题

买代理，没别的方法，你换头不换ip就被ban。

还有一个实战问题吧，就是有些网站会针对你的代理ip地址进行计数，像豆瓣，爬10页就ban，那个时候你只能换一个代理ip地址，应该是这样没错，所以好的方式应该是一个随机代理ip池，大于20个那种，每爬一页就换，这样就能行。

## 模块三	多种形式的爬取方法（2023.7.8	1：30）

### 12.Ajax的原理和解析

#### 前言

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216924.png" alt="image-20230708103547612" style="zoom: 67%;" />

这些数据来源多种，可能是通过**Ajax**来加载的，有可能是包含在HTML文档中的，也有可能是js算法，对于第一种情况，数据的加载就是一种**异步加载方式**，原始的页面它不会再包含这些数据，**只有在这个页面加载完成，才会向服务器发送请求某个接口来获取数据**。然后数据才会被处理，从而呈现到网页上面，这个过程实际上就是向服务器上接口**发送了一个Ajax请求**。

按照web的趋势来说，这样的页面会越来越多。网页的原始HTML文档不会包含任何数据，数据都是通过Ajax统一加载后再呈现出来的，这样在web开发上可以做到**前后端分离**，并且降低服务器直接渲染页面带来的压力。

**如果requests能够模拟这些Ajax请求，那么就可以成功抓取渲染后的数据。**

本课时就是了解并学会如何抓取它。

#### 什么是Ajax

**Ajax（Asynchronous JavaScript and XML)**，即**异步的JavaScript和XML**。

它不是一门编程语言，而是利用**JavaScript在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。**

传统的网页如果想要更新新内容，必须要刷新页面，而有了Ajax便可以在页面不被全部刷新的情况下更新其内容。在这个过程中，页面实际上与后台服务器进行了数据交互，获取到真实数据之后，我们可以利用**javascript**来改变这个网页，这样网页内容就会被更新了。

我们可以试下这个链接。[DEMO](http://www.w3school.com.cn/ajax/ajax_xmlhttprequest_send.asp)

实例就是微博点击评论，开始加载中，这就是**典型通过ajax来获取信息**。

#### 基本原理

发送Ajax请求到网页更新的过程可以简单分为以下3步：

- 发送请求
- 解析内容
- 渲染网页

##### 发送请求

我们知道javascript可以实现与网页的交互功能，Ajax当然也不例外，执行方式就是如下的代码

```js
var xmlhttp;
if (window.XMLHttpRequest){
    //code for IE7+, Firefox, Chrome, Opera, Safari
    # 新建一个对象
    xmlhttp = new XMLHttpRequest();}else{//code for IE6, IE5
    xmlhttp = new ActiveXObject("Microsoft.XMLHTTP");}
}
# 设置监听，当服务器返回结果，触发方法
xmlhttp.onreadystatechange=function(){if (xmlhttp.readyState==4 && xmlhttp.status==200)
    									
# document.getElementById().innerHTML对某个元素内的源代码进行更改，responseText获取响应内容
{document.getElementById("myDiv").innerHTML=xmlhttp.responseText;
    }
}
xmlhttp.open("POST", "/ajax/", true);
xmlhttp.send();
```

这是**JavaScript对Ajax最底层的实现**。这个过程实际上是新建了一个XMLHttpRequest对象，然后设置一个onreadystatechange，这是一个**监听**。最后用open和send方法，来向某个链接发送请求。前面所用到的都是python来发送请求，而这里只有js来发送。

由于设置了监听，所以当服务器返回结果时，onreadystatechange所对应的方法就会触发。我们在这个触发的方法里**解析内容**就好了。

##### 解析内容

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216722.png" alt="image-20230708110958809" style="zoom: 80%;" />

比如，如果返回的内容是JSON的话，我们便可以对它进行解析和转化。

**JavaScript有改变网页内容的能力**。解析完成这个响应之后呢，就可以调用jiavascript来针对解析完的这个内容对网页进行下一步的处理。

##### 渲染网页（DOM操作）

比如通过`document.getElementById().innerHTML`这样的操作对某个元素内的源代码进行更改，这样网页显示的内容就改变了。这种对Document网页文档进行如更改、删除等操作也被称作DOM操作。

`document.getElementById("myDiv").innerHTML = xmlhttp.responseText`将id为myDiv的节点内部的HTML代码更改为服务器返回的内容，这样myDiv元素内部便会呈现出服务器返回的新数据，网页的部分内容看上去就更新了。

##### 小结

![image-20230708112128879](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216747.png)

所以每一次网页局部更新就是JavaScript在一次次做Ajax请求。那么哪里来找Ajax请求的位置，我们移步Chrome浏览器的开发者工具里面去一个看。

#### Ajax分析

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216239.png" alt="image-20230708113316681" style="zoom:80%;" />

[链接点击](https://m.weibo.cn/u/2830678474)

![image-20230708114749531](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216799.png)

好好好。

##### XHR

![image-20230708115235815](scrawer/image-20230708115235815.png)

Network--->ctrl R之后，选择Fetch XHR，这就是**Ajax特殊的请求类型**。在图中我们看见这是以  *getIndex*  开头的请求。其Type为XHR。

![image-20230708115619858](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216707.png)

这里打开详情，**X-Requested-With：XMLHttpRequest**，说明这就是一个Ajax请求。

随后我们点击Preview，就可以看见响应的内容了。

![image-20230708115820608](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216019.png)

它是由json格式的内容组成的。返回结果就是个人信息，javascript接收到这些信息就开始渲染

Response就是真实内容，如图所示

![image-20230708122901154](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216738.png)

我们看见的真实数据并不是由原始页面返回，而是执行了JavaScript之后再次向后台发送Ajax请求，浏览器拿到数据之后在渲染出来的。

##### 捕获

![image-20230708123209074](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216182.png)

当出现新的Ajax请求，那么就会出现在这里。

然后我们想要模拟这些数据来提交就非常简单了（因为有现成模板）。只需要模拟它，程序便能得到真实信息。

### 13.Ajax爬取案例实战（可跳过）

由于这哥们给的靶场链接总是犯病，我当然就直接自己上，我们去微博练手，那里好多Ajax。

目标如下：

- 分析页面数据的加载逻辑
- 用requests实现Ajax数据的爬取
- 将每部电影的数据保存成一个JSON数据文件

#### 登录首页（初步探索）

```python
# Author: Rainsblue.chan
# Create: 2023/7/8
# FileName: ajax实战
import requests

url = 'https://weibo.com/newlogin?tabtype=weibo&gid=102803&openLoginLayer=0&url=https%3A%2F%2Fweibo.com%2F'

headers = {
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090551) XWEB/6945 Flue'
}

proxies = {
    "http": "http://127.0.0.1:4780",
    "https": "http://127.0.0.1:4780",
}

html = requests.get(url,headers=headers,proxies=proxies)
html.encoding = html.apparent_encoding
print(html.text)
```

返回结果如下

```html
<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-type" content="text/html; charset=gb2312"/>
    <title>Sina Visitor System</title>
</head>
<body>
<span id="message"></span>
<script type="text/javascript" src="/js/visitor/mini_original.js?v=20161116"></script>
<script type="text/javascript">
    window.use_fp = "1" == "1"; // 是否采集设备指纹。
    var url = url || {};
    (function () {
        this.l = function (u, c) {
            try {
                var s = document.createElement("script");
                s.type = "text/javascript";
                s[document.all ? "onreadystatechange" : "onload"] = function () {

                    if (document.all && this.readyState != "loaded" && this.readyState != "complete") {
                        return
                    }
                    this[document.all ? "onreadystatechange" : "onload"] = null;
                    this.parentNode.removeChild(this);
                    if (c) {
                        c()
                    }
                };
                s.src = u;
                document.getElementsByTagName("head")[0].appendChild(s)
            } catch (e) {
            }
        };
    }).call(url);

    // 流程入口。
    wload(function () {

        try {

            var need_restore = "1" == "1"; // 是否走恢复身份流程。

            // 如果需要走恢复身份流程，尝试从 cookie 获取用户身份。
            if (!need_restore || !Store.CookieHelper.get("SRF")) {

                // 若获取失败走创建访客流程。
                // 流程执行时间过长（超过 3s），则认为出错。
                var error_timeout = window.setTimeout("error_back()", 5000);

                tid.get(function (tid, where, confidence) {
                    // 取指纹顺利完成，清除出错 timeout 。
                    window.clearTimeout(error_timeout);
                    incarnate(tid, where, confidence);
                });
            } else {
                // 用户身份存在，尝试恢复用户身份。
                restore();
            }
        } catch (e) {
            // 出错。
            error_back();
        }
    });

    // “返回” 回调函数。
    var return_back = function (response) {

        if (response["retcode"] == 20000000) {
            back();
        } else {
            // 出错。
            error_back(response["msg"]);
        }
    };

    // 跳转回初始地址。
    var back = function() {

        var url = "https://weibo.com/newlogin?tabtype=weibo&gid=102803&openLoginLayer=0&url=https%3A%2F%2Fweibo.com%2F";
        if (url != "none") {
            window.location.href = url;
        }
    };

    // 跨域广播。
    var cross_domain = function (response) {

        var from = "weibo";
        var entry = "miniblog";
        if (response["retcode"] == 20000000) {

            var crossdomain_host = "login.sina.com.cn";
            if (crossdomain_host != "none") {

                var cross_domain_intr = window.location.protocol + "//" + crossdomain_host + "/visitor/visitor?a=crossdomain&cb=return_back&s=" +
                        encodeURIComponent(response["data"]["sub"]) + "&sp=" + encodeURIComponent(response["data"]["subp"]) + "&from=" + from + "&_rand=" + Math.random() + "&entry=" + entry;
                url.l(cross_domain_intr);
            } else {

                back();
            }
        } else {

            // 出错。
            error_back(response["msg"]);
        }
    };

    // 为用户赋予访客身份 。
    var incarnate = function (tid, where, conficence) {

        var gen_conf = "";
        var from = "weibo";
        var incarnate_intr = "https://" + window.location.host + "/visitor/visitor?a=incarnate&t=" +
                encodeURIComponent(tid) + "&w=" + encodeURIComponent(where) + "&c=" + encodeURIComponent(conficence) +
                "&gc=" + encodeURIComponent(gen_conf) + "&cb=cross_domain&from=" + from + "&_rand=" + Math.random();
        url.l(incarnate_intr);
    };

    // 恢复用户丢失的身份。
    var restore = function () {

        var from = "weibo";
        var restore_intr = "https://" + window.location.host +
                "/visitor/visitor?a=restore&cb=restore_back&from=" + from + "&_rand=" + Math.random();

        url.l(restore_intr);
    };

    // 跨域恢复丢失的身份。
    var restore_back = function (response) {

        // 身份恢复成功走广播流程，否则走创建访客流程。
        if (response["retcode"] == 20000000) {

            var url = "https://weibo.com/newlogin?tabtype=weibo&gid=102803&openLoginLayer=0&url=https%3A%2F%2Fweibo.com%2F";
            var alt = response["data"]["alt"];
            var savestate = response["data"]["savestate"];
            if (alt != "") {
                requrl = (url == "none") ? "" : "&url=" + encodeURIComponent(url);
                var params = "entry=sso&alt=" + encodeURIComponent(alt) + "&returntype=META" +
                    "&gateway=1&savestate=" + encodeURIComponent(savestate) + requrl;
                window.location.href = "https://login.sina.com.cn/sso/login.php?" + params;
            } else {

                cross_domain(response);
            }
        } else if(response['retcode'] == 50111261 && isInIframe()) {
            //do nothing
        } else {

            tid.get(function (tid, where, confidence) {
                incarnate(tid, where, confidence);
            });
        }
    };

    // 出错情况返回登录页。
    var error_back = function (msg) {

        var url = "https://weibo.com/newlogin?tabtype=weibo&gid=102803&openLoginLayer=0&url=https%3A%2F%2Fweibo.com%2F";
        var clientType = "pc";
        if (url != "none") {

            if (url.indexOf("ssovie4c55=0") === -1) {
                url += (((url.indexOf("?") === -1) ? "?" : "&") + "ssovie4c55=0");
            }
            if (clientType == "mobile") {
            	window.location.href = "https://passport.weibo.cn/signin/login?r="+url;
            } else{
            	window.location.href = "https://weibo.com/login.php";
            }
        } else {

            if(document.getElementById("message")) {
                document.getElementById("message").innerHTML = "Error occurred" + (msg ? (": " + msg) : ".");
            }
        }
    };

    var isInIframe = function () {
        try {
            return window.self !== window.top;
        } catch (e) {
            return true;
        }
    };

</script>
</body>
</html>
```

而页面是这样子

![image-20230708180649325](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216143.png)

在源码中我们只能看见引用一些JavaScript和css文件，没有观察到页面上这些数据。

如果遇到这样的情况，说明现在看到的**整个页面是通过JavaScript渲染得到的**。

#### 微博热点（接口逻辑）

![image-20230708184838377](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216826.png)

我突然想到好玩的，我们来试试如何将微博热点爬下来。但是我失败了。

```python
# 日志配置
import logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s-%(levelname)s:%(message)s')
```

#### 寻找样板（实例）

因为自己实在没什么基础，课里的例子又显得非常靶机类，所以去找了个例子。

[【微博爬虫教程&实例】基于requests、mysql爬取大数据量博主关键字下博文及评论](https://blog.csdn.net/otonashi_ayana/article/details/127273082)

#### 核心操作（寻找接口）

我发现有一篇博客（博客湾的）就在那里直接给答案，没有说到Ajax接口在哪里查看，真纯的。

![image-20230709093442115](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102216073.png)

我这边记一下，**F12的Network里面，检索XHR，在Headers里找General，这里Request URL就是我们要的**。麻烦说话要说就说最清楚，细节教不明白他妈的别教了。

```python
# Author: Rainsblue.chan
# Create: 2023/7/9
# FileName: blogger
import requests, json
url = 'https://weibo.com/u/id'

headers = {
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63090551) XWEB/6945 Flue'
}

proxies = {
    "http": "http://127.0.0.1:4780",
    "https": "http://127.0.0.1:4780",
}

global user_id
global cookie
user_id = 'id'
cookie = 'SINAGLOBAL=5997781523862.422.1687845706419; UOR=,,www.baidu.com; PC_TOKEN=db40cb6edc; XSRF-TOKEN=cNgTFQVKgr74B7rPrIP-Lsx2; _s_tentry=www.weibo.com; Apache=7936047385721.463.1688863334027; ULV=1688863334070:9:5:1:7936047385721.463.1688863334027:1688809772160; SUB=_2A25JrnM_DeRhGeNG7VEX-SbLyDuIHXVq2uP3rDV8PUNbmtAbLVrHkW9NSyywTyfjx8ojdOo-1XrHTIGGgECf5MjL; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFAkPlQ7UMrAmGdNarL5NjU5JpX5KzhUgL.Fo-RSoec1KnNe0M2dJLoIEBLxK-LB-BLBKBLxK.L1-zLB-2LxKBLBo.L12zLxK.L1hML12qt; ALF=1720399598; SSOLoginState=1688863599; WBPSESS=W-RWYWssIB9nAIzCm_tGsnn-s4Ua-Dra4YsDW0a72vKE46Vs-dkYFAWJlmX9x-3plrPmhyp2_x6c_5pDsm2boYXwRf-pD3IFZSEczPyAdklXhelohjyX5G1NnUGEoAYFSu-LeS22ywM8QNsbsbIySg=='

def user_info(user_id):
    # 模拟传参,详见Network中的 Request URL
    # 在 https://weibo.com/ajax/profile/info 里传递用户参数为 ?uid=
    params = {
        'uid': user_id,
    }
    headers = {
        'Referer': 'https://weibo.com/u/' + str(user_id),
        'Host': 'weibo.com',
        'Cookie': cookie,
        'Connection': 'close'
    }
    try:
        res = requests.get('https://weibo.com/ajax/profile/info', headers=headers, params=params, proxies=proxies).content.decode("utf-8")
        json_data = json.loads(res)
        print('已取得博主%s的user_info包' % (str(user_id)))
        return json_data
    except:
        attempts += 1
        print('【' + str(user_id) + '】''user_info() request error, trying again...', attempts)
        if attempts < 5:
            return user_info(user_id, attempts)
        else:
            print("连接似乎出现问题，数据采集终止")
            return None

json_data = user_info(user_id)
print('博主id:', json_data['data']['user']['id'])  # 博主id
print('ip归属地:', json_data['data']['user']['location'])  # ip归属地
print('博主名:', json_data['data']['user']['screen_name'])  # 博主名
print('粉丝数:', json_data['data']['user']['followers_count'])  # 粉丝数
print('关注数:', json_data['data']['user']['friends_count'])  # 关注数
print('性别:', json_data['data']['user']['gender'])  # 性别（f & m）
print('总微博数量:', json_data['data']['user']['statuses_count'])  # 总微博数量
print('认证类型:', json_data['data']['user']['verified_type'])  # 认证类型
if json_data['data']['user']['verified_type'] != -1:
    print('认证信息:', json_data['data']['user']['verified_reason'])  # 认证信息（注意缺省）
else:
    print("Not verified")
```

这一篇我感觉到这就先差不多，等基础上来再看后面的，接口大概懂点了（汗）。

Ajax接口，找到对应参数请求，就能够被爬取到，具体看选项卡中payload。

### 14.Selenium的基本使用（利器，重要）

#### 导入

Ajax请求的接口通常会包含加密的参数，如token，sign等。由于数据接口带上参数，这个时候需要搞token。

有两个方法：

- 深挖逻辑，将token构造逻辑找出来，再用Python复现，构造Ajax请求。
- **直接通过模拟浏览器的方式，绕过这个过程**

因为在浏览器里面能够直接看见数据。由于第一种难度比较高，所以就先搞第二种。

##### Selenium

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217512.png" alt="image-20230709123936143" style="zoom:80%;" />

#### 基本使用

```python
# Author: Rainsblue.chan
# Create: 2023/7/9
# FileName: selenium基本使用
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait

browser = webdriver.Chrome()
try:
    browser.get('https://www.baidu.com')
    input = browser.find_element(By.ID,'kw')
    input.send_keys('Python')
    input.send_keys(Keys.ENTER)
    wait = WebDriverWait(browser,10)
    wait.until(EC.presence_of_element_located((By.ID,'content_left')))
    print(browser.current_url)
    print(browser.get_cookies())
    print(browser.page_source)
finally:
    browser.close()
```

在运行完代码后，会跳出一个Chrome浏览器，在搜索框中百度搜索Python，接着跳转到搜索的结果页。

返回源代码包含了js渲染后的结果，不用在担心加密逻辑。

#### 声明浏览器对象

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217783.png" alt="image-20230709125533486" style="zoom:67%;" />

调用它，模拟操作。

#### 访问页面

用**get**方法请求网页

```python
from selenium import webdriver

browser = webdriver.Chrome()
# 打开淘宝网页
browser.get('https://www.taobao.com')

# 输出淘宝网页源代码
print(browser.page_source)
browser.close()
```

#### 查找节点

**Selenium**可以驱动浏览器完成各种操作，比如**填充表单、模拟点击**等。

当想要完成向某个输入框输入文字的操作时，首先需要知道这个输入框在哪，而**Selenium**提供了一系列查找节点的方法，可以用这些方法来获取想要的节点，以便执行下一步动作或者提取信息。

##### 单个节点

接下来我以考试宝为例。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217959.png" alt="image-20230709130948478" style="zoom:50%;" />

```html
# 背题模式未点击
<div role="switch" class="el-switch"><input type="checkbox" name="" true-value="true" class="el-switch__input"><!----><span class="el-switch__core" style="width: 40px;"></span><!----></div>

# 背题模式点击
<div role="switch" class="el-switch is-checked" aria-checked="true"><input type="checkbox" name="" true-value="true" class="el-switch__input"><!----><span class="el-switch__core" style="width: 40px;"></span><!----></div>
```

我们发现这个按钮的role是"switch"，打开时会有一个检查输入的盒子，当点击后，**aria-checked**这个值会变为"True"。

它还有多种属性，我们用很多方式来获取它，比如**FindElementByName**，代表根据name的值来获取，**FindElementById**，则表示根据id来获取。另外还有根据**XPath**和**CSS**选择器来获取的一些方式，我们可以用代码来实现一下。

![image-20230709131918941](scrawer/image-20230709131918941.png)

```python
# Author: Rainsblue.chan
# Create: 2023/7/9
# FileName: kaoshibao
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
browser = webdriver.Chrome()

url = 'https://www.zaixiankaoshi.com/online/?paperId=10976328&practice=&modal=1&is_recite=&qtype=&text=%E9%A1%BA%E5%BA%8F%E7%BB%83%E4%B9%A0&sequence=0&is_collect=0&is_vip_paper=0'

browser.get(url)
# 找寻答题模式节点
# XPath,最爽，没有之一
button_first = browser.find_element(By.XPATH, '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[2]/div[1]/p[2]/span[2]/div')
print(button_first)
# Css_selector
button_second = browser.find_element(By.CSS_SELECTOR, '.el-switch')
print(button_second)
browser.close()

返回结果如下：
<selenium.webdriver.remote.webelement.WebElement (session="45d65bb312eec26c021c10090a818f21", element="CBBCDFEC021EA2A09CFF000410572130_element_5")>
<selenium.webdriver.remote.webelement.WebElement (session="45d65bb312eec26c021c10090a818f21", element="CBBCDFEC021EA2A09CFF000410572130_element_6")>
```

可以看见返回结果类型都是一致的，都是**WebElement**。这里我个人推荐的话，就用**XPath**，它是绝对路径，而且用起来非常方便。[python3 selenium 自动化测试 强大的xpath定位](https://blog.csdn.net/xiezhiming1234/article/details/82904248)，下列是一些常用的方法，但是基本上，by都改在括号内了，要注意下，除非你用的是老版本编译器。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217291.png" alt="image-20230709133650546" style="zoom: 67%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217545.png" alt="image-20230709133723893" style="zoom:67%;" />

##### 多个节点

如果有多个节点需要查找，用find_element方法就只能得到第一个节点，如果要查找**所有满足条件的节点**，需要用**find_elements**方法。多了一个s。

假如想要找到所有的条目，就用这个方式，得到列表类型的数据。方法如上，这里一般用CSS比较方便。

#### 节点交互

**Selenium**可以驱动浏览器来执行一些操作，或者说可以让浏览器模拟执行一些动作。

```python
from selenium import webdriver
import time

browser = webdriver.Chrome()
browser.get('https://www.taobao.com')
# 找到输入框
input = browser.find_element(By.Id,'q')
# 在输入框里输入iPhone
input.send_keys('iPhone')
time.sleep(1)
input.clear()
# 在输入框里输入iPad
input.send_keys('iPad')
# 寻找搜索按钮，然后模拟按
button = browser.find_element(By.CLASS_NAME,'btn-search')
button.click()
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217803.png" alt="image-20230709134514536" style="zoom:50%;" />

运行结果如上。

这里首先驱动浏览器打开淘宝，然后**find_element(By.ID,)**获取输入框，然后用**send_keys**方法输入iphone和ipad，当中的间隔用input.clear做清除，从结果上看只会搜ipad，完成了搜索的动作。

#### 动作链

有一些操作没有特定的执行对象，比如鼠标拖拽、键盘按键等，这些动作用另一种方式来执行，那就是**动作链**。

```python
from selenium import webdriver
from selenium.webdriver import ActionChains

browser = webdriver.Chrome()
url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'
browser.get(url)
browser.switch_to.frame('iframeResult')
source = browser.find_element(By.CSS_SELECTOR('#draggable'))
target = browser.find_element(By.CSS_SELECTOR('#droppable'))
actions = ActionChains(browser)
actions.drag_and_drop(source,target)
actions.perform()
```

拖拽的实例，感觉用的机会不是很多，所以就是记一下。

我错了。

#### 执行JavaScript（个人感觉重要）

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('https://www.zhihu.com/explore')
browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')
browser.execute_script('alert("To Bottom")')
```

![image-20230709144024940](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217701.png)

自动到达底部。

#### 获取节点信息

Selenium已经提供了选择节点方法，并返回的是WebElement类型那么它也有相关的方法和属性来**直接提取**节点信息，如属性、文本等。可以不用通过解析源代码来提取信息，非常方便。

##### 获取属性（get_attribute）

使用**get_attribute**方法来获取节点的属性，但是前提是**先选中这个节点**。

![image-20230709144551605](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217696.png)

如果我打开的话就会不一样。

```python
# Author: Rainsblue.chan
# Create: 2023/7/9
# FileName: kaoshibao
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
browser = webdriver.Chrome()

url = 'https://www.zaixiankaoshi.com/online/?paperId=10976328&practice=&modal=1&is_recite=&qtype=&text=%E9%A1%BA%E5%BA%8F%E7%BB%83%E4%B9%A0&sequence=0&is_collect=0&is_vip_paper=0'

browser.get(url)

# 找寻答题模式节点
# XPath,最爽，没有之一
el_switch = browser.find_element(By.XPATH, '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[2]/div[1]/p[2]/span[2]/div')
print(el_switch)
print(el_switch.get_attribute('class'))
# 寻找按钮，然后模拟按
button = browser.find_element(By.XPATH,'//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[2]/div[1]/p[2]/span[2]/div/span')
button.click()
el_switch = browser.find_element(By.XPATH, '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[2]/div[1]/p[2]/span[2]/div')
print(el_switch)
print(el_switch.get_attribute('class'))
browser.close()
```

![image-20230709144931672](scrawer/image-20230709144931672.png)

哈哈，考试宝终于完蛋了（目标达成：1/3）

使用了**get_attribute**帮助我确定了这个按钮找的非常正确，是可行的。

##### 获取文本值（input.text）

接下来我要得到文本信息，也就是题目的获取。

![image-20230709145908598](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217879.png)

 比较恶心的一点是在第1页之后，如果你打开第2页，它就会弹出一个框说你上次练到第一页是否继续，我开了一个time.sleep()才发现，这也是得关闭的，自动化一下。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217238.png" alt="image-20230709152355324" style="zoom:50%;" />

关于这个问题的详细解决过程，我放在末尾的一个实例来交代。我们继续往下走。

##### 获取ID、位置、标签名、大小

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217241.png" alt="image-20230711134457719" style="zoom:67%;" />

**input.id/location/tag_name/size(宽、高)**

#### 切换Frame

Selenium打开页面后，默认是在父级Frame里面操作，而此时如果页面中还有子Frame，Selenium是不能获取到子Frame里面的节点的，这是就需要用**switch_to.frame**方法来切换Frame。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217754.png" alt="image-20230711165851171" style="zoom:80%;" />

输出结果如下

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217125.png" alt="image-20230711165941035" style="zoom: 67%;" />

首先我们通过switch的方法，我们切到子frame的节点，但是找不到，于是抛出no logo。当页面中包含子frame时，需要先调用switch方法，到达子frame，才能获取子节点。

#### 延时等待（重要）

在Selenium中，**get方法会在网页框架加载结束后结束执行**。此时如果获取**page_source**，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到，所以，这里需要**延时等待一定时间**，确保节点已经加载出来。这里有两种方式，一种叫**隐式等待**，一种叫**显式等待**。

##### 隐式等待(implicitly_wait)

如果Selenium没有在DOM中找到节点将继续等待，超出设定时间后，则抛出找不到节点的异常。这里采用了**implicitly_wait()方法**。

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.implicitly_wait(10)
browser.get('https://dynamic2.scrape.cuiqingcai.com/')
input = browser.find_element_by_class_name('logo-image')
print(input)
```

值得说的一点是，它这个给的url没用。例子我懒得找了。

##### 显式等待（WebDriverWait(browser, *kwarg)，更为合适)

指定要查找的节点，然后指定一个最长等待时间。如果说在规定时间内找到，就插入，如果没有，就报错。

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

browser = webdriver.Chrome()
browser.get('https://www.taobao.com/')
wait = WebDriverWait(browser,10)
input = wait.until(EC.presence_of_element_located((By.ID,'q')))
button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,'.btn-search')))
print(input,button)
```

正常运行，返回一个结果如下，控制台成功输出两个节点

```
<selenium.webdriver.remote.webelement.WebElement (session="42a8c85448be4771b2ab4e186a7e81d0", element="60BAB41F96372D86470BD120E87B26AA_element_2")> <selenium.webdriver.remote.webelement.WebElement (session="42a8c85448be4771b2ab4e186a7e81d0", element="60BAB41F96372D86470BD120E87B26AA_element_16")>
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217838.png" alt="image-20230711182905375" style="zoom:67%;" />

#### 前进后退(forward,back)

Selenium使用**back**方法后退，使用**forward**方法前进。

```python
import time
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('https://www.baidu.com/')
browser.get('https://www.taobao.com/')
browser.get('https://www.python.org/')
browser.back()
time.sleep(1)
browser.forward()
browser.close()
```

这里先打开baidu，然后进入淘宝，再进入python，随后回退，前进，关闭浏览器。

#### Cookies(get_cookies,add_cookie,delete_cookies)

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('https://www.zhihu.com/explore')
print(browser.get_cookies())
# 增加cookie
browser.add_cookie({'name':'name','domain':'www.zhihu.com','value':'germey'})
print(browser.get_cookies())
# 删除所有cookies
browser.delete_all_cookies()
print(browser.get_cookies())

返回三条结果，在第二条中出现了add_cookie中的cookie值
```

#### 选项卡管理(window_handles)

```python
import time
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('https://www.baidu.com')
# 开启一个新的选项卡
browser.execute_script('window.open()')
print(browser.window_handles)
# 切换到新选项卡并打开淘宝网页
browser.switch_to.window(browser.window_handles[1])
browser.get('https://www.taobao.com')
time.sleep(1)
# 切换到第一张选项卡，登录python官网
browser.switch_to.window(browser.window_handles[0])
browser.get('https://python.org')

返回如下：
['5CDD4E303F2F3024800BF294387D816B', '2EE23EBBDCDCC1C22F8C97C225A86603']
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217649.png" alt="image-20230711185232917" style="zoom: 33%;" />

返回为选项卡参数。

#### 异常处理(try except)

在使用Selenium的时候经常会碰到很多异常状况。

我们这里尝试打开百度页面，寻找一个并不存在的节点名称。

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

browser = webdriver.Chrome()
browser.get('https://www.baidu.com')
browser.find_element(By.ID,'hello')
```

使用try except结构来捕获异常

```python
# Author: Rainsblue.chan
# Create: 2023/7/11
# FileName: 异常处理
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException,NoSuchElementException

browser = webdriver.Chrome()
try:
    browser.get('https://www.baidu.com')
except TimeoutException:
    print('Time Out')
try:
    browser.find_element(By.ID,'hello')
except NoSuchElementException:
    print('No Element')
finally:
    browser.close()

结果返回：No Element
```

#### 反屏蔽(面对网站针对Selenium的屏蔽，重要)

##### 检测基本原理

检测当前浏览器窗口下的**window.navigator**对象是否包含webdriver这个属性，**因为在正常使用浏览器的情况下这个属性是undefined**，然而一旦我们使用了Selenium，**Selenium就会给window.navigator设置webdriver属性**，很多网站就通过JavaScript判断，如果webdriver属性存在，那就直接屏蔽。

https://antispider1.scrape.center/，这个是下一课的靶机站。如果你针对它使用Selenium爬取，那么就会出现下面的图。

先贴个代码

```python
from selenium import webdriver
import time

browser = webdriver.Chrome()
url = 'https://antispider1.scrape.center/'
browser.get(url)
time.sleep(5)
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217872.png" alt="image-20230711221350440" style="zoom: 33%;" />

##### 躲避审查（CDP，Chrome开发工具协议的运用）

一般思路是，使用JavaScript直接把这个webdriver属性置空。

`Object.defineProperty(navigator,"webdriver",{get:()=>undefined})`

但是，这一句代码执行的位置在网站渲染之后，**而网站对于webdriver属性的检测在页面渲染之前**。所以做不成效果。

这就需要**CDP**。

`CDP，即Chrome Devtools-Protocol，Chrome开发工具协议`

通过CDP我们可以实现在每个页面刚加载的时候执行JavaScript代码，执行的CDP方法叫做**Page.addScriptToEvaluateOnNewDocument**，然后传入上文的JavaScript代码即可，这样我们就可以在每次加载页面之前将webdriver属性置空了。代码实现如下。

```python
# Author: Rainsblue.chan
# Create: 2023/7/11
# FileName: anti-shielding
from selenium import webdriver
from selenium.webdriver import  ChromeOptions
import time

option = ChromeOptions()
option.add_experimental_option('excludeSwitches',['enable-automation'])
option.add_experimental_option('useAutomationExtension',False)
browser = webdriver.Chrome(options=option)
browser.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument',{
    'source':'Object.defineProperty(navigator,"webdriver",{get:()=>undefined})'
})
url = 'https://antispider1.scrape.center/'
browser.get(url)
time.sleep(5)
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217163.png" alt="image-20230711233903293" style="zoom: 33%;" />

这样就正常显示了。但是如果对于一些特殊的网站，可能还会有更加严格的排查，需要针对webdriver细化。

#### 无头模式（节能）

在运行的时候不会再弹出浏览器窗口，减少了干扰，也减少了一些资源加载，如图片等资源。所以在**一定程度上节省了资源加载时间和网络带宽**。Chrome浏览器从60版本支持无头模式，即**headless**。

借助于**ChromeOptions**来开启**Chrome Headless模式**。

```python
from selenium import webdriver
from selenium.webdriver import ChromeOptions

option = ChromeOptions()
# 开启无头模式
option.add_argument('--headless')
browser = webdriver.Chrome(options=option)
browser.set_window_size(1366,768)
# 登录baidu页面
browser.get('https://www.baidu.com')
# 截图
browser.get_screenshot_as_file('preview.png')
```

开启无头模式方法**option.add_argument('--headless')**

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217537.png" alt="image-20230711235833156" style="zoom: 33%;" />

这样我们就可以在无头模式下进行抓取和截图了。

#### 实例：考试宝爬虫问题

##### 元素的Style属性

![image-20230710222542811](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217042.png)

接下来是弹窗问题，问了老师，给了一个csdn，[selenium爬虫笔记——csdn博客页面登录弹窗问题解决](https://blog.csdn.net/weixin_42182448/article/details/113656120)

大致问题就是，弹窗的属性值为display：block时就会有弹窗，我这个放firefox看的，因为chrome会有一个user agent的问题，那是用户自己定义的，然后就很迷，我不知道怎么解决，所以就放firefox一把梭。改为display：none之后确实弹窗消失了。

后来chrome尝试了，直接在这里改。

![image-20230710225253550](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217270.png)

可以直接在这里手动改

网页标签的style属性控制的不只是弹窗，还有灰色，找到v-modal。(7.10)

![image-20230710230116101](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217546.png)

##### (7.11)代码成功实现（延时问题的重要性）

[[Why does jQuery or a DOM method such as getElementById not find the element?]](https://stackoverflow.com/questions/14028959/why-does-jquery-or-a-dom-method-such-as-getelementbyid-not-find-the-element)

这篇stackflow很有用，我来简单总结一下。

在这个节点还没有出现的时候执行一个寻找该节点的script，返回的就是null。**所以解决方案就是等到该节点出现或者在这个节点后面执行script。**

```python
# 弹窗及灰色解决写法
# 但是我是分开写的，if(elem)用于解决延时问题
js = '''
var elem = document.getElementById("el-message-box");
if(elem){
	elem.style.display = "none";
}
'''
browser.execute_script(js)
# 然后接着按上面写就行
```

![image-20230711132617325](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102217456.png)

感谢老师的一个小帮忙，然后感谢chatgpt，后面这个if等待是神之一笔，因为我没学过java，所以不知道怎么下手，最后能成功还是挺妙的。具体的完整代码实现如下。

```python
# Author: Rainsblue.chan
# Create: 2023/7/9
# FileName: kaoshibao
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
import time

browser = webdriver.Chrome()
question_repository = []
options_repository = []
option_right_repository = []
for num in range(0,100):
    url = f'https://www.zaixiankaoshi.com/online/?paperId=10976328&practice=&modal=1&is_recite=&qtype=&text=%E9%A1%BA%E5%BA%8F%E7%BB%83%E4%B9%A0&sequence='+str(num)+'&is_collect=0&is_vip_paper=0'
    browser.get(url)
    # 进行页面数据处理，等待1秒后换至下一个界面
    time.sleep(0.5)
    # 使用js代码，原理，定位id节点，等到找到节点后赋予style值（防止渲染不到位）
    # 取消灰色显示
    # 关闭登录弹窗
    js = '''
    var elem = document.getElementById("el-message-box");
    if (elem) {
        elem.style.display = "none";
    }
    '''
    browser.execute_script(js)
    js = '''
    var elem = document.getElementById("v-modal");
    if (elem) {
        elem.style.display = "none";
    }
    '''
    browser.execute_script(js)
    time.sleep(1)

    wait = WebDriverWait(browser, 10)
    def get_question():
        # 这里查看节点，是程序员写错了，不是我写错了（指question，蛮逆天）
        question = browser.find_element(By.CSS_SELECTOR,'.qusetion-box').text
        print(question)
        question_repository.append(question)
    # 选项卡
    def get_options():
        options = browser.find_elements(By.CSS_SELECTOR,'.option')
        for option in options:
            print(option.text)
        options_repository.append(options)
    get_question()
    get_options()
browser.close()
```

### 15.Selenium爬取实战(可跳过)

#### 回忆——适用场景

当网页出现许多Ajax请求和js渲染的时候，没有办法构造token、sign的时候，可以使用**Selenium**驱动浏览器渲染的方式来**模拟Ajax请求和JavaScript渲染**所得到的最终结果。

##### 缺点

模拟Ajax请求效率较低。某些情况比较复杂。

##### 优点

绝对有效，所见即所得。

#### 完整实例，关于考试宝爬虫

##### 初版

小tip：class属性中的空格代表有多个属性

所需安装包，配置如下

- Python3.7版本
- selenium库，没有的话使用pip install selenium

```python
# Author: Rainsblue.chan
# Create: 2023/7/9
# FileName: kaoshibao
from selenium import webdriver
from selenium.webdriver import ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from selenium.common.exceptions import ElementClickInterceptedException,UnexpectedAlertPresentException
import time

# Chrome选项卡
option = ChromeOptions()
option.add_experimental_option('excludeSwitches',['enable-automation'])
option.add_experimental_option('useAutomationExtension',False)
# 开启无头模式
option.add_argument('--headless')
browser = webdriver.Chrome(options=option)
browser.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument',{
    'source':'Object.defineProperty(navigator,"webdriver",{get:()=>undefined})'
})
browser = webdriver.Chrome(options=option)
question_repository = []
options_repository = []
# 单选题仓库
option_right_repository = []
# 多选题仓库
option_rights_repository = []

#  根网页
url = 'https://www.zaixiankaoshi.com/online/?paperId=10976328&practice=&modal=1&is_recite=&qtype=&text=%E9%A1%BA%E5%BA%8F%E7%BB%83%E4%B9%A0&sequence=0&is_collect=0&is_vip_paper=0'
browser.get(url)
# 打开背题模式
bt = browser.find_element(By.XPATH, '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[2]/div[1]/p[2]/span[2]/div')
bt.click()

# 单选题
for i in range(1,71):
    try:
        xp = '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[1]/div[1]/span['+str(i)+']'
        bt = browser.find_element(By.XPATH, xp)
        # 调试
        # print(bt)
        bt.click()
    except ElementClickInterceptedException:
        bt = browser.find_element(By.XPATH, '/html/body/div[4]/div/div[3]/button[1]')
        bt.click()
        xp = '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[1]/div[1]/span[' + str(i) + ']'
        bt = browser.find_element(By.XPATH, xp)
        bt.click()
    # 题目
    def get_question():
        question = browser.find_element(By.CSS_SELECTOR, '.qusetion-box').text
        print(question)
        question_repository.append(question)
    # 选项
    def get_options():
        options = browser.find_elements(By.CSS_SELECTOR, '.option')
        for option in options:
            print(option.text)
        options_repository.append(options)
    # 正确选项
    def get_right_option():
        option  = browser.find_element(By.CSS_SELECTOR, '.right').text
        print('【答案】' + option)
        option_right_repository.append(option)
    # 留足渲染时间
    time.sleep(0.1)
    get_question()
    get_options()
    get_right_option()
    time.sleep(0.1)

# 多选题
for i in range(71,111):
    try:
        xp = '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[1]/div[1]/span['+str(i)+']'
        bt = browser.find_element(By.XPATH, xp)
        # 调试
        # print(bt)
        bt.click()
    except ElementClickInterceptedException:
        bt = browser.find_element(By.XPATH, '/html/body/div[4]/div/div[3]/button[1]')
        bt.click()
        xp = '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[1]/div[1]/span[' + str(i) + ']'
        bt = browser.find_element(By.XPATH, xp)
        bt.click()
    # 题目
    def get_question():
        question = browser.find_element(By.CSS_SELECTOR, '.qusetion-box').text
        print(question)
        question_repository.append(question)
    # 选项
    def get_options():
        options = browser.find_elements(By.CSS_SELECTOR, '.option')
        for option in options:
            print(option.text)
        options_repository.append(options)
    # 正确选项
    def get_right_options():
        options = browser.find_elements(By.CSS_SELECTOR, '.right3')
        print('【答案】')
        for option in options:
            print(option.text)
        option_rights_repository.append(options)
    # 留足渲染时间
    time.sleep(0.1)
    get_question()
    get_options()
    get_right_options()
    time.sleep(0.1)

# 判断题
# 多选题
for i in range(71,111):
    try:
        xp = '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[1]/div[1]/span['+str(i)+']'
        bt = browser.find_element(By.XPATH, xp)
        # 调试
        # print(bt)
        bt.click()
    except ElementClickInterceptedException:
        bt = browser.find_element(By.XPATH, '/html/body/div[4]/div/div[3]/button[1]')
        bt.click()
        xp = '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[1]/div[1]/span[' + str(i) + ']'
        bt = browser.find_element(By.XPATH, xp)
        bt.click()
    # 题目
    def get_question():
        question = browser.find_element(By.CSS_SELECTOR, '.qusetion-box').text
        print(question)
        question_repository.append(question)
    # 选项
    def get_options():
        options = browser.find_elements(By.CSS_SELECTOR, '.option')
        for option in options:
            print(option.text)
        options_repository.append(options)
    # 正确选项
    def get_right_option():
        option = browser.find_element(By.CSS_SELECTOR, '.right').text
        print('【答案】' + option)
        option_right_repository.append(option)
    # 留足渲染时间
    time.sleep(0.1)
    get_question()
    get_options()
    get_right_option()
    time.sleep(0.1)

browser.close()
```

### 16.异步爬虫的原理和解析

#### 引入，请求时间计算

我们知道，爬虫是**IO密集型任务**，我们使用requests访问一个网站，返回响应时我们必须要等待，而在整个等待过程中，爬虫程序也一直在等待，**实际上没有做任何事情**。对于这种情况我们有没有优化的方案呢？比如一个网站，他设计的就是**等5秒**再给响应，内容就是电影数据，遍历之后，如果有100部电影，那么就要500多秒。

这边记录时间有一个方式，用**time.time()**记录前后时间，相减得到所花费时间。我这里就随便找个网站做个展示。

```python
# Author: Rainsblue.chan
# Create: 2023/7/12
# FileName: time运用
import requests
import logging
import time

proxies = {
    'http': '127.0.0.1:4780',
    'https': '127.0.0.1:4780'
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s-%(levelname)s:%(message)s')
TOTAL_NUMBER = 100
BASE_URL = 'https://www.baidu.com'

start_time = time.time()
logging.info('scraping %s',BASE_URL)
res = requests.get(BASE_URL, proxies=proxies, headers=headers)
end_time = time.time()
logging.info('total time %s seconds',end_time-start_time)
```

![image-20230714131429708](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218730.png)

我们知道，一个网站的访问时间可能长可能短，有的1、2秒，有的则更长，这个时候通过requests单线程的去爬取会浪费时间。**如果我们使用多线程或者多进程去爬取，那么爬取效率确实会成倍地提升**。但是有没有更好的解决方案，本课时就采用**异步执行**的方式来进行加速方法。这种方法**对于IO密集型任务**来说非常有效，如果运用到网络爬虫之中，甚至会有**上百倍提升**。我们要先了解一些基本概念。

#### 基本了解

##### 阻塞（block）

指程序未得到所需计算资源时被挂起的状态。程序在等待某个操作完成期间，自身无法继续处理其他的事情，则称**该程序在操作上是阻塞的**。常见的阻塞有**网络I/O阻塞**、**磁盘I/O阻塞**、**用户输入阻塞**等等。它无处不在，包括CPU切换上下文时。

##### 非阻塞

程序在等待某个操作过程时，自身不被阻塞。**可以继续处理其他的事情**，则称该程序在该操作上是非阻塞的，非阻塞并不是在任何程序级别、任何情况下都可以存在的。**仅当程序封装的级别可以囊括独立的子程序单元时，才可能存在非阻塞状态**。

**非阻塞的存在正是因为阻塞的存在**，正是因为某个操作阻塞导致耗时与效率低下，我们才要把它变成非阻塞。

##### 同步（有序、保持通信）

不同程序单元为了完成某个任务，在执行过程中需要靠某种通信方式以协调一致，称这些程序单元是同步执行的。比如购物系统中更新商品库存，我们需要用横锁作为通信信号，**让不同的程序强制排队顺序执行**。

**同步意味着有序**。

##### 异步（无序、无需保持通信，Asynchronous）

为完成某个任务，不同程序单元之间过程中无需通信协调，也能完成任务的方式，不相关的程序单元之间可以是异步的。

比如爬虫下载网页，调度下载程序之后，即可调度其他的任务，**即无需与该下载任务保持通信以协调行为**。

不同的网页，它的下载、保存等行为**都是无关的**，也**无需相互通知协调**。

简而言之，异步意味着**无序**。

我们前面讲Ajax，就是指异步的JS和XML。

##### 多进程

多进程就是利用CPU的多核优势，在同一时间并行地执行多个任务，在同一时间并行地执行多个任务，可以大大提高执行效率。

##### 协程（Coroutine，本质是单进程）

又称**微线程、纤程**，协程是一种**用户态**的**轻量级线程**。（我想就是目态😂😂😂）

**拥有自己的寄存器上下文和栈**，当它调度切换时，它会把寄存器上下文和栈保存到其他地方，等再切换回来的时候，恢复保存到寄存器、上下文和栈。因此**协程是可以保存上一次调用的状态的**，即所有局部状态的一个Coroutine组合，每次过程重入时，就相当于进入上一次的调用状态。

**本质上是个单进程。**相对于于多进程，无需线程上下文切换的开销，无需原子操作锁定及同步的开销。

**编程模型非常简单**。我们可以使用协程来完成异步的操作，比如说在网络爬虫场景下，我们发出一个请求之后，需要等待一定的时间之后才能得到响应，但是实际在等待过程之中，程序是可以干很多其他的事情的。然后等到这个响应得到之后呢，我们在切换回来继续处理就好，这样的话就可以充分利用CPU及其他资源，这就是**协程的优势**。

#### 协程用法

从Python3.4开始，加入了**协程**的概念。但这个版本的协程还是以生成器对象为基础的。

在Python3.5则增加了**async/await**，使得协程的实现更加方便。我们需要了解下面几个概念。

##### event_loop

事件循环，相当于一个无限循环，可以把一些函数注册到这个事件循环上，当满足条件发生的时候，就会调用对应的处理方法。

##### coroutine（协程）

在Python中常指代为**协程对象类型**，可以将协程对象注册到时间循环中，它会被事件循环调用，可以使用async关键字来定义一个方法，这个方法在调用时不会被立即执行，而是返回一个协程对象。

##### task&future

task，任务，它是对协程对象的进一步封装，包含了任务的各个状态。

future，代表**将来执行或没有执行的任务的结果**，实际上和task没有本质区别。

##### aysnc/await关键字

它是从python3.5中才出现的，专门用于定义协程。其中**async**用于定义一个协程，**await**用来挂起阻塞方法的执行。

#### 实例引用

##### run_until_complete

```python
# Author: Rainsblue.chan
# Create: 2023/7/14
# FileName: 协程coroutine
import asyncio

async def execute(x):
    print('Number:',x)

coroutine = execute(1)
print('Coroutine:',coroutine)
print('After calling execute')

loop = asyncio.get_event_loop()		 #创建循环事件loop
loop.run_until_complete(coroutine)    #将协程注册到loop之中然后启动
print('After calling loop')

返回结果如下：
Coroutine: <coroutine object execute at 0x00000213DA0024C8>
After calling execute
Number: 1
After calling loop
```

首先我们引入asyncio这个包，这样我们才能使用async还有wait。然后我们使用async定义了一个execute方法，方法接收一个数字参数，方法执行之后会打印这个数字。随后我们直接调用了这个方法，然而这个方法并没有执行，而是返回了一个Coroutine，即**协程对象**。我们接着使用**get_event_loop**来创建事件循环loop，并调用了loop对象run_until_complete方法，将协程注册到循环loop中然后启动。最后我们才看到execute打印了输出结果。

可见async定义的方法会变成一个无法执行的Coroutine对象，**必须将其注册到这个事件循环中**才可以把它执行。

上面我们还提到task，它是对于Coroutine对象做进一步封装。它里面相比Coroutine对象多了**运行的状态**，比如running、finished等等。我们可以利用这些对象得知协程的运行情况。

##### create_task

```python
# Author: Rainsblue.chan
# Create: 2023/7/14
# FileName: 协程coroutine
import asyncio

async def execute(x):
    print('Number:',x)
    return x

coroutine = execute(1)
print('Coroutine:',coroutine)
print('After calling execute')

loop = asyncio.get_event_loop()
# 调用create_task方法
task = loop.create_task(coroutine)
print('Task:',task)
# 将task对象注册到loop之中后启动
loop.run_until_complete(task)
# 打印task查看其状态
print('Task:',task)
print('After calling loop')

结果如下：
Coroutine: <coroutine object execute at 0x0000020D96A905C8>
After calling execute
Task: <Task pending coro=<execute() running at D:\系统化爬虫专攻\class\17异步\协程coroutine.py:6>>
Number: 1
Task: <Task finished coro=<execute() done, defined at D:\系统化爬虫专攻\class\17异步\协程coroutine.py:6> result=1>
After calling loop
```

这里使用**create_task**方法，将**Coroutine对象**转化为task对象，随后我们将它打印输出一下，发现它是**pending状态**，随后我们将这个task对象添加到事件循环中，然后执行。随后我们在打印输出一下这个task对象，发现它是**finished**，同时还发现这个result变成了1，这就是我们**定义的execut的返回结果**。

使用方式为，先新建一个async.loop后再新建这个task。

另外定义task对象还有一种方法，就是ensure_future方法。它是放在最前面使用的。

##### ensure_future

```python
# Author: Rainsblue.chan
# Create: 2023/7/14
# FileName: 协程coroutine
import asyncio

async def execute(x):
    print('Number:',x)
    return x

coroutine = execute(1)
print('Coroutine:',coroutine)
print('After calling execute')

task = asyncio.ensure_future(coroutine)
print('Task:',task)
loop = asyncio.get_event_loop()
loop.run_until_complete(task)
print('Task:',task)
print('After calling loop')

结果如下：
Coroutine: <coroutine object execute at 0x000001D2343A15C8>
After calling execute
Task: <Task pending coro=<execute() running at D:\系统化爬虫专攻\class\17异步\协程coroutine.py:6>>
Number: 1
Task: <Task finished coro=<execute() done, defined at D:\系统化爬虫专攻\class\17异步\协程coroutine.py:6> result=1>
After calling loop
```

效果是一样的，但是就是使用方式不一样。

##### aysncio&requests

```python
# Author: Rainsblue.chan
# Create: 2023/7/14
# FileName: asyncio&requests
import asyncio
import requests

proxies = {
    'http':'127.0.0.1:4780',
    'https':'127.0.0.1:4780'
}
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}
async def request():
    url = 'https://www.baidu.com'
    status = requests.get(url, proxies=proxies, headers=headers)
    return status

def callback(task):
    print('Status', task.result())

coroutine = request()
task = asyncio.ensure_future(coroutine)
task.add_done_callback(callback)
print('Task:',task)

loop = asyncio.get_event_loop()
loop.run_until_complete(task)
print('Task:',task)

返回如下：
Task: <Task pending coro=<request() running at D:\系统化爬虫专攻\class\17异步\asyncio&requests.py:14> cb=[callback() at D:\系统化爬虫专攻\class\17异步\asyncio&requests.py:19]>
Status <Response [200]>
Task: <Task finished coro=<request() done, defined at D:\系统化爬虫专攻\class\17异步\asyncio&requests.py:14> result=<Response [200]>>
```

我们使用callback函数，利用print方法打印task里的内容。这样我们就定义了一个coroutine对象和一个回调方法，现在我们希望的是当调用完coroutine对象之后，就去执行声明的callback方法，那么二者怎样关联起来呢？很简单，只需调用**add_done_callback**方法即可。我们将callback方法传给这个**封装好的task对象**，这样当这个task执行完毕之后就可以调用这个callback方法。同时task还会**作为参数**传递给这个callback方法。

其实不用callback方法，直接调用result是一样的。直接task.result()

##### 多任务协程

如果想要多个任务来进行请求，就定义一个task列表。

```python
# Author: Rainsblue.chan
# Create: 2023/7/14
# FileName: asyncio&requests
import asyncio
import requests

proxies = {
    'http':'127.0.0.1:4780',
    'https':'127.0.0.1:4780'
}
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

async def request():
    url = 'https://www.baidu.com'
    status = requests.get(url, proxies=proxies, headers=headers)
    return status

# 连做5次requests请求
tasks = [asyncio.ensure_future(request()) for _ in range(5)]
print('Tasks',tasks)

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

for task in tasks:
    print('Task Result',task.result())
    
    
结果如下：
Tasks [<Task pending coro=<request() running at D:\系统化爬虫专攻\class\17异步\多任务协程.py:15>>, <Task pending coro=<request() running at D:\系统化爬虫专攻\class\17异步\多任务协程.py:15>>, <Task pending coro=<request() running at D:\系统化爬虫专攻\class\17异步\多任务协程.py:15>>, <Task pending coro=<request() running at D:\系统化爬虫专攻\class\17异步\多任务协程.py:15>>, <Task pending coro=<request() running at D:\系统化爬虫专攻\class\17异步\多任务协程.py:15>>]
Task Result <Response [200]>
Task Result <Response [200]>
Task Result <Response [200]>
Task Result <Response [200]>
Task Result <Response [200]>
```

发起5个任务，用列表推导式，下划线代表everything，直接运5次。用wait做等待，然后执行。

前面这些东西看上去非常怪是不是？你先别急，我们往下看。

#### 协程实现（7.14）

我们请求一个网页，这就是**耗时等待**的过程。耗时等待的操作一般都是**IO操作**，如**文件读取、网络请求**等，协程对于处理这种操作是有很大优势的。当遇到需要等待的情况的时候，程序可以**暂时挂起**，转而去执行其他的操作，从而避免一直等待一个程序而耗费过多的时间，充分利用资源。

我们找一个响应速度较慢的网站作为演示，来更好地理解异步的使用方法。

我把代理关了，然后登录github，这个响应就慢了，但是能登上。

![image-20230716193458141](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218859.png)

慢的一。我们需要**挂起**。当一个操作需要等待IO结果的时候，我们可以挂起当前的任务，转而去执行其他的任务，这样我们才能充分利用好资源。上面就是串行，连挂起都没有，更别说异步了。要实现异步，得先了解await用法。

##### 挂起（await）

使用await可以将耗时等待的操作挂起，让出控制权。

```python
async def request():
    url = 'https://github.com/'
    print('Waiting for',url)
    response = await requests.get(url)
    print('Get response from',url,'response',response)
```

当协程执行时遇到这个await，它会挂起，执行别的协程，**直到其他协程执行完毕或者挂起**。

await后面的对象必须是如下格式之一：

- 一个原生coroutine对象
- 一个由types.coroutine修饰的生成器，这个生成器可以返回coroutine对象
- 一个包含_await_方法的对象返回的一个迭代器

只需要改写成得到coroutine对象即可。

```python
# Author: Rainsblue.chan
# Create: 2023/7/16
# FileName: 协程实现
import asyncio
import requests
import time

start = time.time()

async def get(url):
    return requests.get(url)

async def request():
    url = 'https://github.com/'
    print('Waiting for',url)
    response = await get(url)
    print('Get response from',url,'response',response)

tasks = [asyncio.ensure_future(request()) for _ in range(10)]
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

end = time.time()
print('Cost time:',end - start)
```

但这有一个致命的问题，挺逗的，**request库不支持异步操作**，所以针对io的优化就失效了。我们需要使用**aiohttp**这个库，利用它和asycnio配合可以非常方便地实现异步请求操作。

##### 使用aiohttp

下载的库分为两部分，一部分是Client，一部分是Server，详细的内容可以参考官方文档。

![image-20230716195224229](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218005.png)

快的一。只要两秒钟。

```python
# Author: Rainsblue.chan
# Create: 2023/7/16
# FileName: 协程实现
import asyncio
import aiohttp
import time

start = time.time()

async def get(url):
    session = aiohttp.ClientSession()
    response = await session.get(url)
    await response.text()
    await session.close()
    return response

async def request():
    url = 'https://github.com/'
    print('Waiting for',url)
    response = await get(url)
    print('Get response from',url,'response',response)

tasks = [asyncio.ensure_future(request()) for _ in range(10)]
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

end = time.time()
print('Cost time:',end - start)
```

通过Client Session和get即可完成请求。如果遇到协程，就会挂起，转而执行其他的协程，直到其他协程挂起或者执行完毕。挂起之后挂下一个，全挂起之后等响应唤醒。当遇到阻塞之后会挂起，不必把时间用在等待io上。

通过aiohttp之后，我们可以实现成百上千次的网络请求。

### 17.aiohttp异步爬虫实战

#### aiohttp

是一个**基于asyncio的异步HTTP网络模块**，既提供服务端，又提供客户端，用服务端可以搭建一个支持异步处理的服务器，用于处理请求并返回响应，类似于Django、Flask、Tornado等一些Web服务器。

#### 基本使用

有别于requests的同步，我们来看看aiohttp的使用方式。

```python
# Author: Rainsblue.chan
# Create: 2023/7/16
# FileName: aiohttpexercise
import aiohttp
import asyncio

async def fetch(session,url):
    async with session.get(url) as response:
        return await response.text(),response.status

async def main():
    async with aiohttp.ClientSession() as session:
        html,status = await fetch(session, 'https://cuiqingcai.com')
        print(f'html:{html[:100]}...')
        print(f'status:{status}')

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
    
返回结果如下：
html:<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content...
status:200
```

- 导入库的时候，除了必须要引入aiohttp库，还需要**asyncio**这个库，因为**要实现异步爬取需要启动协程**，而协程则需要借助于asyncio里面的事件循环来执行，除了事件循环，asyncio里面也提供了很多基础的异步操作
- 异步爬取的方法的定义和之前有所不同，在每个异步方法前面统一要加**aysnc**来修饰
- with as语句前面同样需要加async来修饰，在**Python中**，with as语句用于**声明一个上下文管理器，能够帮我们自动分配和释放资源**，在异步方法中，with as前面加上async代表**声明一个支持异步的上下文管理器**。
- 对于一些返回coroutine的操作，前面需要加**await**来修饰，如response调用text方法，查询API可以发现其返回的是coroutine对象，那么前面就要加await，对于状态码来说，就不需要加await。
- 定义完爬取方法之后，实际上是main方法调用了fetch方法，要运行必须要启用事件循环，事件循环就需要使用asyncio库，然后使用**run_until_complete**方法来运行。**这里注意，在Python3.7以后，我们可以直接使用aysncio.run_main来直接代替最后的启动操作，不需要再显式的执行。**

对于url参数，我们可以使用**params参数**来完成，传入一个字典即可，以下是一个示例

```python
# Author: Rainsblue.chan
# Create: 2023/7/16
# FileName: aiohttp1
import aiohttp
import asyncio

async def main():
    params = {'name':'germey','age':25}
    async with aiohttp.ClientSession() as session:
        async with session.get('https://httpbin.org/get',params=params) as response:
            print(await response.text())

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
    
返回如下：
{
  "args": {
    "age": "25", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "Python/3.7 aiohttp/3.8.4", 
    "X-Amzn-Trace-Id": "Root=1-64b3e776-387982585cb1bb4923a2aa2c"
  }, 
  "origin": "114.84.212.100", 
   # 这里返回对应了传输的params
  "url": "https://httpbin.org/get?name=germey&age=25"
}
```

其他支持类型：

```python
session.post('http:httpbin.org/post',data=b'data')
session.put('http:httpbin.org/put',data=b'data')
session.delete('http:httpbin.org/delete')
session.head('http:httpbin.org/get')
session.options('http:httpbin.org/get')
session.patch('http:httpbin.org/patch',data=b'data')
```

也能传json数据，但是我这里真懒了，有点看不进去，但是总感觉很重要，我打算慢慢来。7.16

##### post表单提交

其对应的请求头 "Content-Type"为"application/x-www-form-urlencoded",

我们可以用如下的方式来实现

```python
# Author: Rainsblue.chan
# Create: 2023/7/17
# FileName: post表单提交
import aiohttp
import asyncio

async def main():
    data = {'name':'germey','age':25}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://httpbin.org/post',data=data) as response:
            print(await response.text())

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

我们首先引入两个包，async放在前面修饰，说明这个函数支持异步（应该如此），随后我们传入要post的data，然后就是，别忘了，在**Python中**，with as语句用于**声明一个上下文管理器，能够帮我们自动分配和释放资源**，在异步方法中，with as前面加上async代表**声明一个支持异步的上下文管理器**。

关于with as，我想经常会用到的是文件读写，有了它不用再写一个f.close()。await挂起。最后末尾一句连用。

运行结果如下

```Python
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "age": "25", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "18", 
    # 对应的请求头
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Python/3.7 aiohttp/3.8.4", 
    "X-Amzn-Trace-Id": "Root=1-64b49de0-7e3725696a5c0d6329711d40"
  }, 
  "json": null, 
  "origin": "39.144.44.131", 
  "url": "https://httpbin.org/post"
}
```

如果传入的是json参数，那么只需要这么改

##### json参数

```python
async def main():
	data = {'name':'germey','age':25}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://httpbin.org/post',json=data) as response:
            print(await response.text())
            
返回如下：
{
  "args": {}, 
  "data": "{\"name\": \"germey\", \"age\": 25}", 
  "files": {}, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "29", 
    "Content-Type": "application/json", 
    "Host": "httpbin.org", 
    "User-Agent": "Python/3.7 aiohttp/3.8.4", 
    "X-Amzn-Trace-Id": "Root=1-64b4a289-251b593b23c2303c353e2b1c"
  }, 
  #	变为json数据
  "json": {
    "age": 25, 
    "name": "germey"
  }, 
  "origin": "39.144.44.131", 
  "url": "https://httpbin.org/post"
}
```

##### 获取响应基本信息

```python
# Author: Rainsblue.chan
# Create: 2023/7/17
# FileName: 获取响应基本信息
import aiohttp
import asyncio

async def main():
    data = {'name':'germey','age':25}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://httpbin.org/post',data=data) as response:
            print('status:',response.status)
            print('headers:',response.headers)
            print('body:',await response.text())
            print('bytes:',await response.read())
            print('json:',await response.json())

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
    
return：
status: 200
headers: <CIMultiDictProxy('Date': 'Mon, 17 Jul 2023 02:15:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '502', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true')>
body: {
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "age": "25", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "18", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Python/3.7 aiohttp/3.8.4", 
    "X-Amzn-Trace-Id": "Root=1-64b4a455-55be5cee26013fa66d37c2c8"
  }, 
  "json": null, 
  "origin": "39.144.44.131", 
  "url": "https://httpbin.org/post"
}

bytes: b'{\n  "args": {}, \n  "data": "", \n  "files": {}, \n  "form": {\n    "age": "25", \n    "name": "germey"\n  }, \n  "headers": {\n    "Accept": "*/*", \n    "Accept-Encoding": "gzip, deflate", \n    "Content-Length": "18", \n    "Content-Type": "application/x-www-form-urlencoded", \n    "Host": "httpbin.org", \n    "User-Agent": "Python/3.7 aiohttp/3.8.4", \n    "X-Amzn-Trace-Id": "Root=1-64b4a455-55be5cee26013fa66d37c2c8"\n  }, \n  "json": null, \n  "origin": "39.144.44.131", \n  "url": "https://httpbin.org/post"\n}\n'
json: {'args': {}, 'data': '', 'files': {}, 'form': {'age': '25', 'name': 'germey'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '18', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'Python/3.7 aiohttp/3.8.4', 'X-Amzn-Trace-Id': 'Root=1-64b4a455-55be5cee26013fa66d37c2c8'}, 'json': None, 'origin': '39.144.44.131', 'url': 'https://httpbin.org/post'}
```

我们这里找的就是响应码，头部，响应体，响应的二进制内容，json内容。

这里**有些字段需要加await，有些则不需要挂起**。其原则是，**如果其返回的是一个coroutine对象，如async修饰的方法，那么前面我们就需要加一个await**。

##### 对于超时的设置（ClientTimeout）

```python
# Author: Rainsblue.chan
# Create: 2023/7/18
# FileName: 超时设置
import aiohttp
import asyncio

async def main():
    # 设置timeout为1秒
    timeout = aiohttp.ClientTimeout(total=1)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.get('https://httpbin.org/get') as response:
            print('status:',response.status)

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
    
如果在1秒之内成功获取响应，运行结果就是200
```

ClientTimeout对象声明时还有其他参数，如connect、socket_connect等

##### 并发（asyncio.Semaphore）

由于aiohttp可以支持非常大的并发量，上万百万都可以，但是目标网站可能无法接受，有可能爬挂掉，所以最好控制一下爬取的并发量。

在一般情况下，我们可以使用asyncio的**Semaphore**信号量来控制。

```python
# Author: Rainsblue.chan
# Create: 2023/7/18
# FileName: Semaphore
import aiohttp
import asyncio

CONCURRENCY = 5 # 并发量为5
URL = 'https://www.baidu.com'

semaphore = asyncio.Semaphore(CONCURRENCY)
session = None

async def scrape_api():
    async with semaphore:
        print('scraping',URL)
        async with session.get(URL) as response:
            await asyncio.sleep(1)
            return await response.text()

async def main():
    global session
    session = aiohttp.ClientSession()
    scrape_index_tasks = [asyncio.ensure_future(scrape_api()) for _ in range(10000)]
    await asyncio.gather(*scrape_index_tasks)

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

我们限制并发量为5，定义一个爬取方法，使用了**with...as...**来控制上下文。在main方法中，我们声明了10000个任务来交给**gather方法**执行，假若我们不加以限制，那么这10000个task**就会同时执行**，并发的数量特别大，但是有了信号量，同时执行的数量就是5个，很好的控制了速度。

### 18.爬虫神器Pyppeteer的使用

#### Pyppteer介绍

**Puppteer（不是Pyppteer）**是Google基于Node.js开发的一个工具，有了它我们可以通过JavaScript来控制Chrome浏览器的一些操作。当然也可以用作**网络爬虫**上，其API极其完善，功能非常强大，Selenium当然同样可以做到。

**Pyppeteer**实际上是**Puppeteer的Python版本的实现**。

是一位来自于日本的工程师依据Puppeteer的一些功能开发的非官方版本。背后所用的是**Chromium浏览器**，你可以将其看作是**Chrome的开发版**。内核是一样的，实现方式也是一样的。在安装这个包的时候如果没有下载Chromium，会自动下载一个。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218049.png" alt="image-20230720141651545" style="zoom:50%;" />

#### 安装

由于Pyppeteer采用Python的async机制，所以其运行要求的**Python版本为3.5及以上**。

安装方式就是pip install。

#### 另一种实现

```python
# Author: Rainsblue.chan
# Create: 2023/7/20
# FileName: pyppeteer
import asyncio
from pyppeteer import launch
from pyquery import PyQuery as pq

#  根网页
global url
url = 'https://movie.douban.com/top250?start'

async def main():
    browser = await launch()	# 新建一个browser对象，启动浏览器
    page = await browser.newPage()	# browser调用newPage方法，新建一个选项卡，启动空白页面
    await page.goto(url)	# 在浏览器中输入url
    await page.waitForSelector('.title')	# 传入选择器，等待节点选择器加载出来，不出来就一直等到超时
    doc = pq(await page.content())	# js渲染后的结果，通过content方法得到html源码
    names = [item.text() for item in doc('.title').items()]	# 使用pyquery提取电影名称
    print('Names:',names)
    await browser.close()

asyncio.get_event_loop().run_until_complete(main())

返回结果如下：
Names: ['肖申克的救赎', '/\xa0The Shawshank Redemption', '霸王别姬', '阿甘正传', '/\xa0Forrest Gump', '泰坦尼克号', '/\xa0Titanic', '这个杀手不太冷', '/\xa0Léon', '千与千寻', '/\xa0千と千尋の神隠し', '美丽人生', '/\xa0La vita è bella', '辛德勒的名单', "/\xa0Schindler's List", '星际穿越', '/\xa0Interstellar', '盗梦空间', '/\xa0Inception', '楚门的世界', '/\xa0The Truman Show', '忠犬八公的故事', "/\xa0Hachi: A Dog's Tale", '海上钢琴师', "/\xa0La leggenda del pianista sull'oceano", '三傻大闹宝莱坞', '/\xa03 Idiots', '放牛班的春天', '/\xa0Les choristes', '机器人总动员', '/\xa0WALL·E', '无间道', '/\xa0無間道', '疯狂动物城', '/\xa0Zootopia', '控方证人', '/\xa0Witness for the Prosecution', '大话西游之大圣娶亲', '/\xa0西遊記大結局之仙履奇緣', '熔炉', '/\xa0도가니', '教父', '/\xa0The Godfather', '触不可及', '/\xa0Intouchables', '当幸福来敲门', '/\xa0The Pursuit of Happyness', '龙猫', '/\xa0となりのトトロ']
```

我们在这里没有配置浏览器，却也能达到相同的效果，从实践来说它确实省了很多力气。

##### 设置窗口大小

```python
import asyncio
from pyppeteer import launch

width,height = 1366,768

async def main():
    browser = await launch()
    page = await browser.newPage()
    await page.setViewport({'width':width,'height':height})
    await page.goto('url')
    await page.waitForSelector('.item .name')
    await asyncio.sleep(2)
    await page.screenshot(path='xxx.png')	# 截图
    dimensions = await page.evaluate('''()=>{
    	  return{
    	  width:document.documentElement.clientWidth,
    	  height:document.documentElement.clientHeight,
    	  deviceScaleFactor:window.devicePixelRatio,
    	}
    }''')
    print(dimensions)
    await browser.close()
    
asyncio.get_event_loop().run_until_complete(main())
```

调用evaluate方法执行一些javascript，javascript传入的是一个函数，使用return方法返回了网页的宽高、像素大小比率三个值，最后得到的是一个JSON格式的对象。

#### 详细用法

Pyppeteer的几乎所有功能都能在其官方文档的API Reference里面找到，[点击链接](https://miyakogi.github.io/pyppeteer/reference.html)

使用Pyppeteer的第一步就是启动浏览器。

##### launch方法

pyppeteer.launcher.launch(options:dict = None, **kwargs) --> pyppteer.browser.Browser

由于源码中发现他是一个async对象，所以要用一个**await前缀**。

##### ignoreHTTPErrors(bool)

是否要忽略HTTPS的错误，默认是False

##### headless(bool)

是否启用Headless模式，即无界面模式，如果devtools这个参数是True的话，那么该参数就会被设置为False，否则就为True，**即默认开启无头模式**。

##### executablePath(str)

可执行文件的路径，如果指定之后就不需要使用默认的Chromium了，可以指定为已有的Chrome或Chromium了。

##### slowMo(int|float)

通过传入指定的时间，可以减缓Pyppeteer的一些模拟操作

##### args(List[str])

在执行过程中可以传入的额外参数

##### ignoreDefaultArgs(bool)

**相对比较危险，慎用**，如果使用这个参数最好通过args来设定一些参数（作用是不适用Pyppeteer的默认参数）

##### handleSIGINT(bool)

是否响应SIGINT信号，也就是可以使用Ctrl+C来终止浏览器程序，默认是True

##### handleSIGTERM(bool)

是否响应SIGTERM信号，一般是kill命令，默认是True

##### dumpio(bool)

是否将Pyppeteer的输出内容传给process.stdout和process.stderr对象，默认是false

##### userDataDir(str)

即用户数据文件夹，即保留一些个性化配置和操作记录

##### env(dict)

环境变量，可以通过字典形式传入

##### devtools(bool)（可用它关闭无头模式）

是否为每一个页面自动开启调试工具，默认是False，如果这个参数设置为True，那么headless参数就会无效，会被强制设置为False

##### logLevel(int|str)

日志级别，默认和root logger对象的级别相同

##### autoClose(bool)

当一些命令执行完之后，是否自动关闭浏览器，默认是True

##### loop(asyncio.AbstractEventLoop)

事件循环对象

#### 关闭headless模式

一般调试的时候都会关闭无头模式以查看爬取详情，在生产过程中关闭。

```python
# Author: Rainsblue.chan
# Create: 2023/7/23
# FileName: 关闭无头模式
import asyncio
from pyppeteer import launch

async def main():
    await launch(headless=False)
    await asyncio.sleep(100)

asyncio.get_event_loop().run_until_complete(main())
```

#### 调试（devtools）

调试还是很关键的，接下来我们使用一个devtools的方式，让每一个页面都能够进来。

```python
# Author: Rainsblue.chan
# Create: 2023/7/23
# FileName: 关闭无头模式
import asyncio
from pyppeteer import launch

async def main():
    browser = await launch(devtools=True)
    page = await browser.newPage()
    await page.goto('https://www.baidu.com')
    await asyncio.sleep(100)

asyncio.get_event_loop().run_until_complete(main())
```

![image-20230723203942846](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218838.png)

devtools设置true，headless就会被关闭，加上args参数可以关闭上面那条“Chrome正.....”

`browser = await launch(headless=False,args=['--disable-infobars'])`

#### webdriver隐藏命令（防反爬）

`await page.evaluateOnNewDocument('Object.defineProperty(navigator,"webdriver)",{get:()=>undefined}')`

#### userDatadir（cookies储藏，用于多次爬取同一目标网站）

用到本地chromium的data文件夹

```python
import asyncio
from pyppeteer import launch

async def main():
    browser = await launch(headless=False,userDataDir='./userdata',args=['--disable-infobars'])
    page = await browser.newPage()
    await page.goto('https://www.taobao.com')
    await asyncio.sleep(100)
    
asyncio.get_event_loop().run_until_complete(main())
```

运行完之后。

![image-20230724123826166](https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218063.png)

登陆一下，下次在登陆就直接跳转到登录之后。cookie可能会过期，但那个长效。

#### browser类的定义

```
class pyppeteer.browser.Browser(
	connection:pyppeteer.connection.Connection,
	contextIds:List[str],
	ignoreHTTPSErrors:bool,
	setDefaultViewport:bool,
	process:Optional[subprocess.Popen] = None,
	closeCallback:Callable[[],Awaitable[None]] = None,
	**kwargs
)
```

##### 开启无痕模式

好处是比较干净，不与其他浏览器共享cash和cookies等内容。示例如下。

```python
# Author: Rainsblue.chan
# Create: 2023/7/24
# FileName: 无痕模式
import asyncio
from pyppeteer import launch

width,height = 1200,768

async def main():
    browser = await launch(headless=False,
                           args=['--disable-infobars',f'--window-size={width},{height}'])
    context = await browser.createIncogniteBrowserContext() # 开启无痕模式
    page = await context.newPage()
    await page.setViewport({'width':width,'height':height})
    await page.goto('https://www.baidu.com')
    await asyncio.sleep(100)

asyncio.get_event_loop().run_until_complete(main())
```

##### 多页面切换

```python
# Author: Rainsblue.chan
# Create: 2023/7/24
# FileName: 多页面切换
import asyncio
from pyppeteer import launch

async def main():
    browser = await launch(headless=False)
    page = await browser.newPage()
    await page.goto('https://www.baidu.com')
    page = await browser.newPage()
    await page.goto('https://www.bing.com')
    pages = await browser.pages()
    print('Pages:',pages)
    page1 = pages[1]
    await page1.bringToFront()
    await asyncio.sleep(100)

asyncio.get_event_loop().run_until_complete(main())
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218776.png" alt="image-20230724130921900" style="zoom:50%;" />

##### 各项页面功能

```python
# 后退
await page.goBack()
# 前进
await page.goForward()
# 刷新
await page.reload()
# 保存PDF
await page.pdf()
# 截图
await page.screenshot()
# 设置页面HTML
await page.setContent('<h2>Hello World</h2>')
# 设置User-Agent
await page.setUserAgent('Python')
# 设置Headers
await page.setExtraHTTPHeaders(headers={})
# 关闭
await page.close()
await browser.close()
# 模拟点击
element = await page.waitForSelector/XPath()
await element.click(options={
    'button':'right',
    'clickCount':1, # 1 or 2次
    'delay' :3000,  # 毫秒
})
# content输出html源码，cookies直接输出cookies
await page.content()
await page.cookies()
# 支持js，使用evaluate
dimensions = await page.evaluate('''()=>{
	return{
		width:document.documentElement.clientWidth,
		height:document.documentElement.clientHeight,
		deviceScaleFactor:window.devicePixelRatio,
	}
}''')
```

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218444.png" alt="image-20230724140100313" style="zoom: 67%;" />

### 19.Pyppeteer实战

我就主打一个完善考试宝。

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218046.png" alt="image-20230725002505238" style="zoom: 33%;" />

多选题的选项与单选、判断的节点有所不同，它是'.right3'，为了做一个细节区分，我在这里找一个标签。

```python
# Author: Rainsblue.chan
# Create: 2023/7/20
# FileName: pyppeteer
import asyncio
import time
import logging
from pyppeteer import launch
from pyquery import PyQuery as pq

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s-%(levelname)s:%(message)s')

# 根网页
global url
url = input('请输入url:')
print('爬虫，启动。')
time.sleep(0.2)
width,height = 1200,768

async def main():
    browser = await launch(headless=True, args=['--disable-infobars',f'--window-size={width},{height}'])
    context = await browser.createIncogniteBrowserContext()  # 开启无痕模式
    page = await context.newPage()
    await page.setViewport({'width':width,'height':height})
    await page.goto(url)
    element = await page.waitForXPath('//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[2]/div[1]/p[2]/span[2]/div')
    await element.click()
    # 全题型
    num = input('题目从哪里到哪里？（比如第1题到第5题，就输1 5，用空格隔开）：')
    sb = num.split(' ')
    for i in range(int(sb[0]), int(sb[1])):
        try:
            xp = '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[1]/div[1]/span[' + str(i) + ']'
            element = await page.waitForXPath(xp)
            # 调试
            # print(bt)
            await element.click()
        except ElementClickInterceptedException:
            element = await page.waitForXPath('/html/body/div[4]/div/div[3]/button[1]')
            await element.click()
            xp = '//*[@id="body"]/div[2]/div[1]/div[2]/div[2]/div[1]/div[1]/span[' + str(i) + ']'
            element = await page.waitForXPath(xp)
            await element.click()
        # 题目
        async def get_question():
            question = await page.waitForSelector('.qusetion-box')
            doc = pq(await page.content())
            names = [item.text() for item in doc('.qusetion-box').items()]
            print('Names:', names)
        # 选项
        async def get_options():
            option = await page.waitForSelector('.option')
            doc = pq(await page.content())
            names = [item.text() for item in doc('.option').items()]
            print('Options:', names)
        # 正确选项
        async def get_right_option():
            option = await page.waitForSelector('.right')
            doc = pq(await page.content())
            names = [item.text() for item in doc('.right').items()][0]
            print('Right option:', names)
        async def get_right_options():
            option = await page.waitForSelector('.right3')
            doc = pq(await page.content())
            names = [item.text() for item in doc('.right3').items()]
            print('Right options:', names)
        # 留足渲染时间
        time.sleep(0.1)
        await get_question()
        await get_options()
        # 改成XPath就果然没问题...
        option = await page.waitForXPath('//*[@id="body"]/div[2]/div[1]/div[2]/div[1]/div/div[1]/div/div[1]/div/span[1]')
        doc = pq(await page.content())
        names = [item.text() for item in doc('.topic-type').items()][0]
        if names != '多选题':
            await get_right_option()
        else:
            await get_right_options()
        time.sleep(0.1)
    time.sleep(5)
    await browser.close()

asyncio.get_event_loop().run_until_complete(main())
```

所需库：

- import asyncio
  import time
  import logging
  from pyppeteer import launch
  from pyquery import PyQuery as pq

对应安装即可。

至此三个模块算是告一段落，mongodb倒是只用了一次，因为说实在，目前没有用武之处。后续也加油稍微搞一搞？

2023.7.25 凌晨2：11

## 模块四	反爬虫的应对方法（7.25	2：07）

未来可能更多时间会先放在更有用的事情上，这里更新速度会慢很多。

7.31早

### 20.代理的基本原理和用法

#### 前言

在爬虫的时候可能会出现403forbidden，“您的IP访问频率太高”这样的字样，这说明网站采取了**反爬虫的措施**。单位时间内限制访问次数，我们把这种操作叫做**封IP**。我们只要让服务器无法识别是**本机发送的请求**，其实就可以伪装我们的IP。这里就引入了**代理**。

#### 基本原理

代理实际上指的就是**代理服务器（proxy server）**

功能是**代理网络用户去获取网络信息**，是网络信息的中转站。在我们正常请求一个网站的时候，它是发送了请求给web服务器，web服务器会把响应传送给我们，如果设置了代理服务器，就是相当于在本机与web服务器之间搭了一个桥。此时本机不是直接向web服务器发送请求，而是**向代理服务器发送请求**。大概就是，发请求给代理服务器，代理服务器再发请求给web服务器，web服务器再发送响应给代理服务器，代理服务器再发送响应给本机。**这个时候web服务器识别的真实ip就不是本机的真实ip了。这就成功实现了ip的伪装。**

#### 代理的作用

- 突破自身IP访问限制，访问一些平时不能访问的站点
- 访问一些单位或团体内部资源，如使用教育网内地址段免费代理服务器就可以用于对教育网开放的各类FTP下载上传，以及各类资料查询共享等服务。
- 提高访问速度，通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，也将其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度
- **隐藏真实IP**，上网者也可以通过这种方法隐藏自己的IP，免受攻击，对于爬虫来说，用代理就是为了隐藏自身IP，防止自身的IP被封锁。

#### 爬虫代理

由于爬虫爬取速度过快，爬取过程中可能遇到同一个IP访问过于频繁的问题，此时网站就会让我们输入验证码登录或者直接封锁IP，这样会给爬取带来极大的不便。**使用代理隐藏真实的IP**，可以达到很好的效果。

#### 代理分类

##### 根据协议区分

- FTP代理服务器

主要用于访问FTP服务器，一般有上传、下载以及缓存功能，端口一般为21、2121等

- HTTP代理服务器

主要用于访问网页，一般有内容过滤和缓存功能，端口一般为80、8080、3128等

- SSL/TLS代理

主要用于访问加密网站，一般有SSL或TLS加密功能（最高支持128位加密强度），端口一般为443

- RTSP代理

主要用于Realplayer访问Real流媒体服务器，一般有缓存功能，端口一般为554

- Telnet代理

主要用于telnet远程控制（黑客入侵计算机时常用于隐藏身份），端口一般为23

- POP3/SMTP代理

主要用于POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25

- SOCKS代理

只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080，SOCKS代理协议又分为SOCKS4和SOCKS5，SOCKS4协议只支持TCP，SOCKS5支持TCP和UDP，还支持各种身份验证机制、服务器端域名解析等，SOCKS4能做到SOCKS5都可以做到，但SOCKS5能做到SOCKS4不一定能做到

##### 根据匿名程度区分

- 高度匿名代理

会将数据包原封不动的转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的IP是代理服务器的IP。

- 普通匿名代理

会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实IP，代理服务器通常会加入的HTTP头有HTTP_VIA和HTTP_X_FORWARDED_FOR	

- 透明代理

不但改动了数据包，还会告诉服务器客户端的真实IP，最常见的例子是内网中的硬件防火墙

- 间谍代理

指组织或个人创建的，用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器

#### 常见代理类型

- 使用网上的免费代理，最好使用高匿代理，使用前抓取下来筛选一下可用代理，也可以进一步维护一个代理池
- 使用**付费代理**服务，互联网上存在许多代理商，可以付费使用，**质量比免费代理好很多**。
- ADSL拨号，拨一下号换一次IP，稳定性高，也是一种比较有效的解决方案
- 蜂窝代理，即用4G或5G网卡等制作的代理，由于蜂窝网络用作代理的情形较少，因此整体被封锁的几率会较低，**搭建成本较高**。

#### 代理设置

##### requests设置代理

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218572.png" alt="image-20230731102833766" style="zoom:50%;" />

我的本机代理软件会在本地的4780和4781创建代理服务，一个是http(s)，还有一个socks端口，那就是127.0.0.1:4780和4781.、

```python
# Author: Rainsblue.chan
# Create: 2023/7/31
# FileName: requests设置代理
import requests

proxy = '127.0.0.1:4780'

proxies = {
    'http':  proxy,
    'https': proxy,
}

try:
    response = requests.get('https://httpbin.org/get', proxies=proxies)
    print(response.text)
except requests.exceptions.ConnectionError as e:
    print('Error', e.args)
```

看一下回显

```
{
  "args": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.31.0", 
    "X-Amzn-Trace-Id": "Root=1-64c72033-1ec8d57e7818a7de5d691371"
  }, 
  "origin": "172.104.174.22", 
  "url": "https://httpbin.org/get"
}
```

我们看一下socks代理

```python
# Author: Rainsblue.chan
# Create: 2023/7/31
# FileName: socks设置代理
import requests

proxy = '127.0.0.1:4781'

proxies = {
    'http': 'socks5://' + proxy,
    'https': 'socks5://' + proxy,
}

try:
    response = requests.get('https://httpbin.org/get', proxies=proxies)
    print(response.text)
except requests.exceptions.ConnectionError as e:
    print('Error', e.args)
```

需要额外安装一个包---**requests[socks]**

##### Selenium设置代理

对于无认证的代理的设置方法

```python
# Author: Rainsblue.chan
# Create: 2023/7/31
# FileName: selenium设置代理
from selenium import webdriver

proxy = '127.0.0.1:4780'
options = webdriver.ChromeOptions()
options.add_argument('--proxy-server=http://'+proxy)
browser = webdriver.Chrome(options=options)
browser.get('https://httpbin.org/get')
print(browser.page_source)
browser.close()
```

返回origin

```css
"args": {}, 
  "headers": {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7", 
    "Accept-Encoding": "gzip, deflate, br", 
    "Accept-Language": "zh-CN,zh;q=0.9", 
    "Host": "httpbin.org", 
    "Sec-Ch-Ua": "\"Not/A)Brand\";v=\"99\", \"Google Chrome\";v=\"115\", \"Chromium\";v=\"115\"", 
    "Sec-Ch-Ua-Mobile": "?0", 
    "Sec-Ch-Ua-Platform": "\"Windows\"", 
    "Sec-Fetch-Dest": "document", 
    "Sec-Fetch-Mode": "navigate", 
    "Sec-Fetch-Site": "none", 
    "Sec-Fetch-User": "?1", 
    "Upgrade-Insecure-Requests": "1", 
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36", 
    "X-Amzn-Trace-Id": "Root=1-64c72801-1d1de49c126a2ed53e88d9d1"
  }, 
  "origin": "172.104.174.22",  # 新加坡代理
  "url": "https://httpbin.org/get"
}
```

有认证的比较麻烦，就不做了。

aiohttp和requests的用法是一样的。

##### Pyppeteer设置代理

```python
# Author: Rainsblue.chan
# Create: 2023/7/31
# FileName: Pyppeteer设置代理
import asyncio
from pyppeteer import launch

proxy = '127.0.0.1:4780'

async def main():
    browser = await launch(args=['--proxy-server='+proxy],headless=True) # 设置代理
    page = await browser.newPage()
    await page.goto('https://httpsbin.org/get')
    print(await page.content())
    await browser.close()

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

### 21.提高利用效率，代理池的搭建和使用

#### 代理池

代理不论是免费的还是付费的，都不能保证是可用的，可能此IP已被其他人使用来爬取同样的目标站点而被封禁，或者代理服务器突然发生故障或网络繁忙，需要提前做筛选，将不可用的代理剔除掉，保留可用代理。

如何搭建一个可用的代理池。

#### 准备工作

安装并成功运行一个Redis数据库

#### 代理池的目标

基本模块分为4块：**存储模块、获取模块、检测模块、接口模块**

##### 存储模块

负责存储抓取下来的代理，首先要保证代理不重复，要标识代理的可用情况，还要动态实时处理每个代理，一种比较高效和方标的存储方式就是使用**Redis的Sorted Set**，即**有序集合**。

##### 获取模块

需要定时在各大代理网站抓取代理，代理可以是免费公开代理也可以是付费代理，代理的形式都是IP加端口，此模块尽量从不同来源获取，尽量抓取高匿代理，抓取成功之后将可用代理保存到数据库中。

##### 检测模块

需要定时检测数据库中的代理，这里需要设置一个检测链接，最好是爬取哪个网站就检测哪个网站，如果要做一个通用型的代理，那可以设置百度等链接来检测。

另外，需要标识每一个代理的状态，如设置分数标识，100分代表可用，分数越少代表越不可用

- 如果代理可用，可以将分数标识立即设置为100满分，也可以在原基础上加1分
- 如果代理不可用，可以将分数标识减一分，当分数减到一定阈值后，代理就直接从数据库移除

##### 接口模块

需要用API来提供对外服务的接口，可以直接连接数据库来获取对应的数据，但是这样就需要知道数据库的连接信息，并且要配置连接，比较安全和方便的方式就是提供一个**Web API接口**，通过访问接口即可拿到可用代理，由于可用代理可能有多个，可以设置一个随机返回某个可用代理的接口。

#### 代理池的架构

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218482.png" alt="image-20230804110615702" style="zoom: 67%;" />

<img src="https://cdn.jsdelivr.net/gh/rainsbluechan/blogimage@main/img/202310102218128.png" alt="image-20230804110725145" style="zoom: 67%;" />

#### 存储模块

使用**Redis的有序集合**，集合的每一个元素都是不重复的，对于代理池，集合的元素就变成了一个个代理，也就是IP加端口的形式
